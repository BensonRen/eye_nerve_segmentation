Making network now
ResNetUNet(
  (base_model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer0_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer1_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2_1x1): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3_1x1): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4_1x1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (upsample): Upsample(scale_factor=2.0, mode=bilinear)
  (conv_up3): Sequential(
    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up2): Sequential(
    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up1): Sequential(
    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up0): Sequential(
    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size2): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 512, 512]           1,792
              ReLU-2         [-1, 64, 512, 512]               0
            Conv2d-3         [-1, 64, 512, 512]          36,928
              ReLU-4         [-1, 64, 512, 512]               0
            Conv2d-5         [-1, 64, 256, 256]           9,408
            Conv2d-6         [-1, 64, 256, 256]           9,408
       BatchNorm2d-7         [-1, 64, 256, 256]             128
       BatchNorm2d-8         [-1, 64, 256, 256]             128
              ReLU-9         [-1, 64, 256, 256]               0
             ReLU-10         [-1, 64, 256, 256]               0
        MaxPool2d-11         [-1, 64, 128, 128]               0
        MaxPool2d-12         [-1, 64, 128, 128]               0
           Conv2d-13         [-1, 64, 128, 128]          36,864
           Conv2d-14         [-1, 64, 128, 128]          36,864
      BatchNorm2d-15         [-1, 64, 128, 128]             128
      BatchNorm2d-16         [-1, 64, 128, 128]             128
             ReLU-17         [-1, 64, 128, 128]               0
             ReLU-18         [-1, 64, 128, 128]               0
           Conv2d-19         [-1, 64, 128, 128]          36,864
           Conv2d-20         [-1, 64, 128, 128]          36,864
      BatchNorm2d-21         [-1, 64, 128, 128]             128
      BatchNorm2d-22         [-1, 64, 128, 128]             128
             ReLU-23         [-1, 64, 128, 128]               0
             ReLU-24         [-1, 64, 128, 128]               0
       BasicBlock-25         [-1, 64, 128, 128]               0
       BasicBlock-26         [-1, 64, 128, 128]               0
           Conv2d-27         [-1, 64, 128, 128]          36,864
           Conv2d-28         [-1, 64, 128, 128]          36,864
      BatchNorm2d-29         [-1, 64, 128, 128]             128
      BatchNorm2d-30         [-1, 64, 128, 128]             128
             ReLU-31         [-1, 64, 128, 128]               0
             ReLU-32         [-1, 64, 128, 128]               0
           Conv2d-33         [-1, 64, 128, 128]          36,864
           Conv2d-34         [-1, 64, 128, 128]          36,864
      BatchNorm2d-35         [-1, 64, 128, 128]             128
      BatchNorm2d-36         [-1, 64, 128, 128]             128
             ReLU-37         [-1, 64, 128, 128]               0
             ReLU-38         [-1, 64, 128, 128]               0
       BasicBlock-39         [-1, 64, 128, 128]               0
       BasicBlock-40         [-1, 64, 128, 128]               0
           Conv2d-41         [-1, 64, 128, 128]          36,864
           Conv2d-42         [-1, 64, 128, 128]          36,864
      BatchNorm2d-43         [-1, 64, 128, 128]             128
      BatchNorm2d-44         [-1, 64, 128, 128]             128
             ReLU-45         [-1, 64, 128, 128]               0
             ReLU-46         [-1, 64, 128, 128]               0
           Conv2d-47         [-1, 64, 128, 128]          36,864
           Conv2d-48         [-1, 64, 128, 128]          36,864
      BatchNorm2d-49         [-1, 64, 128, 128]             128
      BatchNorm2d-50         [-1, 64, 128, 128]             128
             ReLU-51         [-1, 64, 128, 128]               0
             ReLU-52         [-1, 64, 128, 128]               0
       BasicBlock-53         [-1, 64, 128, 128]               0
       BasicBlock-54         [-1, 64, 128, 128]               0
           Conv2d-55          [-1, 128, 64, 64]          73,728
           Conv2d-56          [-1, 128, 64, 64]          73,728
      BatchNorm2d-57          [-1, 128, 64, 64]             256
      BatchNorm2d-58          [-1, 128, 64, 64]             256
             ReLU-59          [-1, 128, 64, 64]               0
             ReLU-60          [-1, 128, 64, 64]               0
           Conv2d-61          [-1, 128, 64, 64]         147,456
           Conv2d-62          [-1, 128, 64, 64]         147,456
      BatchNorm2d-63          [-1, 128, 64, 64]             256
      BatchNorm2d-64          [-1, 128, 64, 64]             256
           Conv2d-65          [-1, 128, 64, 64]           8,192
           Conv2d-66          [-1, 128, 64, 64]           8,192
      BatchNorm2d-67          [-1, 128, 64, 64]             256
      BatchNorm2d-68          [-1, 128, 64, 64]             256
             ReLU-69          [-1, 128, 64, 64]               0
             ReLU-70          [-1, 128, 64, 64]               0
       BasicBlock-71          [-1, 128, 64, 64]               0
       BasicBlock-72          [-1, 128, 64, 64]               0
           Conv2d-73          [-1, 128, 64, 64]         147,456
           Conv2d-74          [-1, 128, 64, 64]         147,456
      BatchNorm2d-75          [-1, 128, 64, 64]             256
      BatchNorm2d-76          [-1, 128, 64, 64]             256
             ReLU-77          [-1, 128, 64, 64]               0
             ReLU-78          [-1, 128, 64, 64]               0
           Conv2d-79          [-1, 128, 64, 64]         147,456
           Conv2d-80          [-1, 128, 64, 64]         147,456
      BatchNorm2d-81          [-1, 128, 64, 64]             256
      BatchNorm2d-82          [-1, 128, 64, 64]             256
             ReLU-83          [-1, 128, 64, 64]               0
             ReLU-84          [-1, 128, 64, 64]               0
       BasicBlock-85          [-1, 128, 64, 64]               0
       BasicBlock-86          [-1, 128, 64, 64]               0
           Conv2d-87          [-1, 128, 64, 64]         147,456
           Conv2d-88          [-1, 128, 64, 64]         147,456
      BatchNorm2d-89          [-1, 128, 64, 64]             256
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
             ReLU-92          [-1, 128, 64, 64]               0
           Conv2d-93          [-1, 128, 64, 64]         147,456
           Conv2d-94          [-1, 128, 64, 64]         147,456
      BatchNorm2d-95          [-1, 128, 64, 64]             256
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
             ReLU-98          [-1, 128, 64, 64]               0
       BasicBlock-99          [-1, 128, 64, 64]               0
      BasicBlock-100          [-1, 128, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]         147,456
          Conv2d-102          [-1, 128, 64, 64]         147,456
     BatchNorm2d-103          [-1, 128, 64, 64]             256
     BatchNorm2d-104          [-1, 128, 64, 64]             256
            ReLU-105          [-1, 128, 64, 64]               0
            ReLU-106          [-1, 128, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]         147,456
          Conv2d-108          [-1, 128, 64, 64]         147,456
     BatchNorm2d-109          [-1, 128, 64, 64]             256
     BatchNorm2d-110          [-1, 128, 64, 64]             256
            ReLU-111          [-1, 128, 64, 64]               0
            ReLU-112          [-1, 128, 64, 64]               0
      BasicBlock-113          [-1, 128, 64, 64]               0
      BasicBlock-114          [-1, 128, 64, 64]               0
          Conv2d-115          [-1, 256, 32, 32]         294,912
          Conv2d-116          [-1, 256, 32, 32]         294,912
     BatchNorm2d-117          [-1, 256, 32, 32]             512
     BatchNorm2d-118          [-1, 256, 32, 32]             512
            ReLU-119          [-1, 256, 32, 32]               0
            ReLU-120          [-1, 256, 32, 32]               0
          Conv2d-121          [-1, 256, 32, 32]         589,824
          Conv2d-122          [-1, 256, 32, 32]         589,824
     BatchNorm2d-123          [-1, 256, 32, 32]             512
     BatchNorm2d-124          [-1, 256, 32, 32]             512
          Conv2d-125          [-1, 256, 32, 32]          32,768
          Conv2d-126          [-1, 256, 32, 32]          32,768
     BatchNorm2d-127          [-1, 256, 32, 32]             512
     BatchNorm2d-128          [-1, 256, 32, 32]             512
            ReLU-129          [-1, 256, 32, 32]               0
            ReLU-130          [-1, 256, 32, 32]               0
      BasicBlock-131          [-1, 256, 32, 32]               0
      BasicBlock-132          [-1, 256, 32, 32]               0
          Conv2d-133          [-1, 256, 32, 32]         589,824
          Conv2d-134          [-1, 256, 32, 32]         589,824
     BatchNorm2d-135          [-1, 256, 32, 32]             512
     BatchNorm2d-136          [-1, 256, 32, 32]             512
            ReLU-137          [-1, 256, 32, 32]               0
            ReLU-138          [-1, 256, 32, 32]               0
          Conv2d-139          [-1, 256, 32, 32]         589,824
          Conv2d-140          [-1, 256, 32, 32]         589,824
     BatchNorm2d-141          [-1, 256, 32, 32]             512
     BatchNorm2d-142          [-1, 256, 32, 32]             512
            ReLU-143          [-1, 256, 32, 32]               0
            ReLU-144          [-1, 256, 32, 32]               0
      BasicBlock-145          [-1, 256, 32, 32]               0
      BasicBlock-146          [-1, 256, 32, 32]               0
          Conv2d-147          [-1, 256, 32, 32]         589,824
          Conv2d-148          [-1, 256, 32, 32]         589,824
     BatchNorm2d-149          [-1, 256, 32, 32]             512
     BatchNorm2d-150          [-1, 256, 32, 32]             512
            ReLU-151          [-1, 256, 32, 32]               0
            ReLU-152          [-1, 256, 32, 32]               0
          Conv2d-153          [-1, 256, 32, 32]         589,824
          Conv2d-154          [-1, 256, 32, 32]         589,824
     BatchNorm2d-155          [-1, 256, 32, 32]             512
     BatchNorm2d-156          [-1, 256, 32, 32]             512
            ReLU-157          [-1, 256, 32, 32]               0
            ReLU-158          [-1, 256, 32, 32]               0
      BasicBlock-159          [-1, 256, 32, 32]               0
      BasicBlock-160          [-1, 256, 32, 32]               0
          Conv2d-161          [-1, 256, 32, 32]         589,824
          Conv2d-162          [-1, 256, 32, 32]         589,824
     BatchNorm2d-163          [-1, 256, 32, 32]             512
     BatchNorm2d-164          [-1, 256, 32, 32]             512
            ReLU-165          [-1, 256, 32, 32]               0
            ReLU-166          [-1, 256, 32, 32]               0
          Conv2d-167          [-1, 256, 32, 32]         589,824
          Conv2d-168          [-1, 256, 32, 32]         589,824
     BatchNorm2d-169          [-1, 256, 32, 32]             512
     BatchNorm2d-170          [-1, 256, 32, 32]             512
            ReLU-171          [-1, 256, 32, 32]               0
            ReLU-172          [-1, 256, 32, 32]               0
      BasicBlock-173          [-1, 256, 32, 32]               0
      BasicBlock-174          [-1, 256, 32, 32]               0
          Conv2d-175          [-1, 256, 32, 32]         589,824
          Conv2d-176          [-1, 256, 32, 32]         589,824
     BatchNorm2d-177          [-1, 256, 32, 32]             512
     BatchNorm2d-178          [-1, 256, 32, 32]             512
            ReLU-179          [-1, 256, 32, 32]               0
            ReLU-180          [-1, 256, 32, 32]               0
          Conv2d-181          [-1, 256, 32, 32]         589,824
          Conv2d-182          [-1, 256, 32, 32]         589,824
     BatchNorm2d-183          [-1, 256, 32, 32]             512
     BatchNorm2d-184          [-1, 256, 32, 32]             512
            ReLU-185          [-1, 256, 32, 32]               0
            ReLU-186          [-1, 256, 32, 32]               0
      BasicBlock-187          [-1, 256, 32, 32]               0
      BasicBlock-188          [-1, 256, 32, 32]               0
          Conv2d-189          [-1, 256, 32, 32]         589,824
          Conv2d-190          [-1, 256, 32, 32]         589,824
     BatchNorm2d-191          [-1, 256, 32, 32]             512
     BatchNorm2d-192          [-1, 256, 32, 32]             512
            ReLU-193          [-1, 256, 32, 32]               0
            ReLU-194          [-1, 256, 32, 32]               0
          Conv2d-195          [-1, 256, 32, 32]         589,824
          Conv2d-196          [-1, 256, 32, 32]         589,824
     BatchNorm2d-197          [-1, 256, 32, 32]             512
     BatchNorm2d-198          [-1, 256, 32, 32]             512
            ReLU-199          [-1, 256, 32, 32]               0
            ReLU-200          [-1, 256, 32, 32]               0
      BasicBlock-201          [-1, 256, 32, 32]               0
      BasicBlock-202          [-1, 256, 32, 32]               0
          Conv2d-203          [-1, 512, 16, 16]       1,179,648
          Conv2d-204          [-1, 512, 16, 16]       1,179,648
     BatchNorm2d-205          [-1, 512, 16, 16]           1,024
     BatchNorm2d-206          [-1, 512, 16, 16]           1,024
            ReLU-207          [-1, 512, 16, 16]               0
            ReLU-208          [-1, 512, 16, 16]               0
          Conv2d-209          [-1, 512, 16, 16]       2,359,296
          Conv2d-210          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-211          [-1, 512, 16, 16]           1,024
     BatchNorm2d-212          [-1, 512, 16, 16]           1,024
          Conv2d-213          [-1, 512, 16, 16]         131,072
          Conv2d-214          [-1, 512, 16, 16]         131,072
     BatchNorm2d-215          [-1, 512, 16, 16]           1,024
     BatchNorm2d-216          [-1, 512, 16, 16]           1,024
            ReLU-217          [-1, 512, 16, 16]               0
            ReLU-218          [-1, 512, 16, 16]               0
      BasicBlock-219          [-1, 512, 16, 16]               0
      BasicBlock-220          [-1, 512, 16, 16]               0
          Conv2d-221          [-1, 512, 16, 16]       2,359,296
          Conv2d-222          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-223          [-1, 512, 16, 16]           1,024
     BatchNorm2d-224          [-1, 512, 16, 16]           1,024
            ReLU-225          [-1, 512, 16, 16]               0
            ReLU-226          [-1, 512, 16, 16]               0
          Conv2d-227          [-1, 512, 16, 16]       2,359,296
          Conv2d-228          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-229          [-1, 512, 16, 16]           1,024
     BatchNorm2d-230          [-1, 512, 16, 16]           1,024
            ReLU-231          [-1, 512, 16, 16]               0
            ReLU-232          [-1, 512, 16, 16]               0
      BasicBlock-233          [-1, 512, 16, 16]               0
      BasicBlock-234          [-1, 512, 16, 16]               0
          Conv2d-235          [-1, 512, 16, 16]       2,359,296
          Conv2d-236          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-237          [-1, 512, 16, 16]           1,024
     BatchNorm2d-238          [-1, 512, 16, 16]           1,024
            ReLU-239          [-1, 512, 16, 16]               0
            ReLU-240          [-1, 512, 16, 16]               0
          Conv2d-241          [-1, 512, 16, 16]       2,359,296
          Conv2d-242          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-243          [-1, 512, 16, 16]           1,024
     BatchNorm2d-244          [-1, 512, 16, 16]           1,024
            ReLU-245          [-1, 512, 16, 16]               0
            ReLU-246          [-1, 512, 16, 16]               0
      BasicBlock-247          [-1, 512, 16, 16]               0
      BasicBlock-248          [-1, 512, 16, 16]               0
          Conv2d-249          [-1, 512, 16, 16]         262,656
            ReLU-250          [-1, 512, 16, 16]               0
        Upsample-251          [-1, 512, 32, 32]               0
          Conv2d-252          [-1, 256, 32, 32]          65,792
            ReLU-253          [-1, 256, 32, 32]               0
          Conv2d-254          [-1, 512, 32, 32]       3,539,456
            ReLU-255          [-1, 512, 32, 32]               0
        Upsample-256          [-1, 512, 64, 64]               0
          Conv2d-257          [-1, 128, 64, 64]          16,512
            ReLU-258          [-1, 128, 64, 64]               0
          Conv2d-259          [-1, 256, 64, 64]       1,474,816
            ReLU-260          [-1, 256, 64, 64]               0
        Upsample-261        [-1, 256, 128, 128]               0
          Conv2d-262         [-1, 64, 128, 128]           4,160
            ReLU-263         [-1, 64, 128, 128]               0
          Conv2d-264        [-1, 256, 128, 128]         737,536
            ReLU-265        [-1, 256, 128, 128]               0
        Upsample-266        [-1, 256, 256, 256]               0
          Conv2d-267         [-1, 64, 256, 256]           4,160
            ReLU-268         [-1, 64, 256, 256]               0
          Conv2d-269        [-1, 128, 256, 256]         368,768
            ReLU-270        [-1, 128, 256, 256]               0
        Upsample-271        [-1, 128, 512, 512]               0
          Conv2d-272         [-1, 64, 512, 512]         110,656
            ReLU-273         [-1, 64, 512, 512]               0
          Conv2d-274          [-1, 1, 512, 512]              65
================================================================
Total params: 49,192,641
Trainable params: 49,192,641
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 2522.00
Params size (MB): 187.66
Estimated Total Size (MB): 2712.66
----------------------------------------------------------------
Start training now...
training: bce: 0.575198, dice: 0.999367, loss: 0.787282
training IoU in current batch 0 is 0.0
training IoU uptillnow 0 is 0.0
testing: bce: 0.022123, dice: 0.038437, loss: 0.030280
IoU in current test batch is 0.0
training: bce: 0.752480, dice: 0.751483, loss: 0.751981
training IoU in current batch 100 is 0.682624845233182
training IoU uptillnow 100 is 0.0033793309169959504
testing: bce: 2.923095, dice: 2.919221, loss: 2.921158
IoU in current test batch is 0.6897583022182664
training: bce: 0.442246, dice: 0.547600, loss: 0.494923
training IoU in current batch 200 is 0.385547402973547
training IoU uptillnow 200 is 0.0026571448960366396
testing: bce: 3.418902, dice: 4.233370, loss: 3.826136
IoU in current test batch is 0.609678305680644
training: bce: 0.312878, dice: 0.424675, loss: 0.368777
training IoU in current batch 300 is 0.7987609662799884
training IoU uptillnow 300 is 0.00310121796426365
testing: bce: 3.622166, dice: 4.916429, loss: 4.269297
IoU in current test batch is 0.7068059946694387
training: bce: 0.242977, dice: 0.348016, loss: 0.295497
training IoU in current batch 400 is 0.7289344467940218
training IoU uptillnow 400 is 0.0032367427198014206
testing: bce: 3.747457, dice: 5.367481, loss: 4.557469
IoU in current test batch is 0.8296231085921281
Epoch   478: reducing learning rate of group 0 to 9.9900e-04.
training: bce: 0.208850, dice: 0.318677, loss: 0.263763
training IoU in current batch 500 is 0.7387551804113912
training IoU uptillnow 500 is 0.0033279669078763777
testing: bce: 4.024376, dice: 6.140654, loss: 5.082515
IoU in current test batch is 0.6737804160558466
Epoch   579: reducing learning rate of group 0 to 9.9800e-04.
training: bce: 0.184621, dice: 0.292638, loss: 0.238630
training IoU in current batch 600 is 0.8885508849557522
training IoU uptillnow 600 is 0.0035134556794075562
testing: bce: 4.267592, dice: 6.764447, loss: 5.516020
IoU in current test batch is 0.80913175810568
Epoch   680: reducing learning rate of group 0 to 9.9700e-04.
training: bce: 0.165165, dice: 0.272294, loss: 0.218730
training IoU in current batch 700 is 0.7088836368691783
training IoU uptillnow 700 is 0.00351787258453428
testing: bce: 4.453100, dice: 7.341473, loss: 5.897287
IoU in current test batch is 0.7568839696199104
Epoch   781: reducing learning rate of group 0 to 9.9601e-04.
training: bce: 0.149418, dice: 0.253447, loss: 0.201433
training IoU in current batch 800 is 0.888143798111261
training IoU uptillnow 800 is 0.0036330843705545076
testing: bce: 4.603236, dice: 7.808127, loss: 6.205681
IoU in current test batch is 0.844899067959928
Epoch   882: reducing learning rate of group 0 to 9.9501e-04.
training: bce: 0.135449, dice: 0.234902, loss: 0.185176
training IoU in current batch 900 is 0.9322993083211066
training IoU uptillnow 900 is 0.0037472255660096717
testing: bce: 4.693832, dice: 8.140262, loss: 6.417047
IoU in current test batch is 0.8497135823209335
Epoch   983: reducing learning rate of group 0 to 9.9401e-04.
training: bce: 0.123826, dice: 0.218935, loss: 0.171381
training IoU in current batch 1000 is 0.932537749634681
training IoU uptillnow 1000 is 0.0038386804293626922
testing: bce: 4.767308, dice: 8.429004, loss: 6.598156
IoU in current test batch is 0.8596347444807311
Epoch  1084: reducing learning rate of group 0 to 9.9302e-04.
training: bce: 0.116736, dice: 0.210681, loss: 0.163709
training IoU in current batch 1100 is 0.8649537422721094
training IoU uptillnow 1100 is 0.0038828301370827513
testing: bce: 4.943306, dice: 8.921548, loss: 6.932427
IoU in current test batch is 0.7944948491733369
Epoch  1185: reducing learning rate of group 0 to 9.9203e-04.
training: bce: 0.130350, dice: 0.232626, loss: 0.181488
training IoU in current batch 1200 is 0.5984279475982532
training IoU uptillnow 1200 is 0.0038086677391567323
testing: bce: 6.021185, dice: 10.745546, loss: 8.383365
IoU in current test batch is 0.6795056086979702
Epoch  1286: reducing learning rate of group 0 to 9.9104e-04.
training: bce: 0.123910, dice: 0.226011, loss: 0.174960
training IoU in current batch 1300 is 0.8380865365789989
training IoU uptillnow 1300 is 0.0038380117010120944
testing: bce: 6.200264, dice: 11.309225, loss: 8.754745
IoU in current test batch is 0.8640913030322595
Epoch  1387: reducing learning rate of group 0 to 9.9004e-04.
training: bce: 0.116985, dice: 0.216847, loss: 0.166916
training IoU in current batch 1400 is 0.8974369455913012
training IoU uptillnow 1400 is 0.0038843481055049153
testing: bce: 6.303706, dice: 11.684721, loss: 8.994214
IoU in current test batch is 0.8191842414955067
Epoch  1488: reducing learning rate of group 0 to 9.8905e-04.
training: bce: 0.110783, dice: 0.208103, loss: 0.159443
training IoU in current batch 1500 is 0.7917828222487721
training IoU uptillnow 1500 is 0.0038893158607173697
testing: bce: 6.395564, dice: 12.013925, loss: 9.204745
IoU in current test batch is 0.8587693925688031
Epoch  1589: reducing learning rate of group 0 to 9.8807e-04.
training: bce: 0.106630, dice: 0.201034, loss: 0.153832
training IoU in current batch 1600 is 0.9257991673135276
training IoU uptillnow 1600 is 0.003935516983506269
testing: bce: 6.565951, dice: 12.379080, loss: 9.472515
IoU in current test batch is 0.839593943186106
Epoch  1690: reducing learning rate of group 0 to 9.8708e-04.
training: bce: 0.101749, dice: 0.194261, loss: 0.148005
training IoU in current batch 1700 is 0.9340693786570076
training IoU uptillnow 1700 is 0.003978716860624362
testing: bce: 6.656731, dice: 12.709163, loss: 9.682947
IoU in current test batch is 0.8281335215948056
Epoch  1791: reducing learning rate of group 0 to 9.8609e-04.
training: bce: 0.098300, dice: 0.188782, loss: 0.143541
training IoU in current batch 1800 is 0.9003120449344706
training IoU uptillnow 1800 is 0.004007747586001818
testing: bce: 6.809165, dice: 13.076786, loss: 9.942976
IoU in current test batch is 0.8503894814670541
Epoch  1892: reducing learning rate of group 0 to 9.8510e-04.
training: bce: 0.094099, dice: 0.182607, loss: 0.138353
training IoU in current batch 1900 is 0.9132192426896645
training IoU uptillnow 1900 is 0.004037118897282539
testing: bce: 6.880048, dice: 13.351359, loss: 10.115704
IoU in current test batch is 0.8394008939183333
training: bce: 0.090656, dice: 0.177278, loss: 0.133967
training IoU in current batch 2000 is 0.8279616724738676
training IoU uptillnow 2000 is 0.004042250804583228
testing: bce: 6.977060, dice: 13.643607, loss: 10.310334
IoU in current test batch is 0.8837759829542429
training: bce: 0.087271, dice: 0.172317, loss: 0.129794
training IoU in current batch 2100 is 0.9311954441488121
training IoU uptillnow 2100 is 0.004071461961944524
testing: bce: 7.052143, dice: 13.924513, loss: 10.488328
IoU in current test batch is 0.8873159487277399
Epoch  2172: reducing learning rate of group 0 to 9.8412e-04.
training: bce: 0.084789, dice: 0.168080, loss: 0.126434
training IoU in current batch 2200 is 0.8420705716922265
training IoU uptillnow 2200 is 0.004077772316170631
testing: bce: 7.177710, dice: 14.228611, loss: 10.703160
IoU in current test batch is 0.8783490542142324
Epoch  2273: reducing learning rate of group 0 to 9.8314e-04.
training: bce: 0.081884, dice: 0.163794, loss: 0.122839
training IoU in current batch 2300 is 0.8962629757785467
training IoU uptillnow 2300 is 0.004095310019896059
testing: bce: 7.246705, dice: 14.495793, loss: 10.871249
IoU in current test batch is 0.8904653233262478
Epoch  2374: reducing learning rate of group 0 to 9.8215e-04.
training: bce: 0.079400, dice: 0.160425, loss: 0.119913
training IoU in current batch 2400 is 0.8587519936204147
training IoU uptillnow 2400 is 0.004103575323861325
testing: bce: 7.332309, dice: 14.814642, loss: 11.073476
IoU in current test batch is 0.8550341784578751
Epoch  2475: reducing learning rate of group 0 to 9.8117e-04.
training: bce: 0.077003, dice: 0.157040, loss: 0.117021
training IoU in current batch 2500 is 0.9177374427172252
training IoU uptillnow 2500 is 0.004122972040763556
testing: bce: 7.407095, dice: 15.106018, loss: 11.256556
IoU in current test batch is 0.8495078021521183
Epoch  2576: reducing learning rate of group 0 to 9.8019e-04.
training: bce: 0.074694, dice: 0.153319, loss: 0.114007
training IoU in current batch 2600 is 0.8406661786237188
training IoU uptillnow 2600 is 0.004126061577570747
testing: bce: 7.472316, dice: 15.337783, loss: 11.405049
IoU in current test batch is 0.9075114760377766
Epoch  2677: reducing learning rate of group 0 to 9.7921e-04.
training: bce: 0.072688, dice: 0.150324, loss: 0.111506
training IoU in current batch 2700 is 0.907252799430222
training IoU uptillnow 2700 is 0.0041412486349413635
testing: bce: 7.551210, dice: 15.616325, loss: 11.583767
IoU in current test batch is 0.9066149329820709
Epoch  2778: reducing learning rate of group 0 to 9.7823e-04.
training: bce: 0.071009, dice: 0.148077, loss: 0.109543
training IoU in current batch 2800 is 0.8295454545454546
training IoU uptillnow 2800 is 0.004141479932256105
testing: bce: 7.649853, dice: 15.952397, loss: 11.801125
IoU in current test batch is 0.8667881723307643
Epoch  2879: reducing learning rate of group 0 to 9.7725e-04.
training: bce: 0.069190, dice: 0.145604, loss: 0.107397
training IoU in current batch 2900 is 0.7799911465250111
training IoU uptillnow 2900 is 0.004133154382458412
testing: bce: 7.720035, dice: 16.246050, loss: 11.983042
IoU in current test batch is 0.8762582099831243
training: bce: 0.068139, dice: 0.144129, loss: 0.106134
training IoU in current batch 3000 is 0.8692580734966593
training IoU uptillnow 3000 is 0.004140256547904094
testing: bce: 7.864822, dice: 16.635821, loss: 12.250321
IoU in current test batch is 0.82268045152245
Epoch  3038: reducing learning rate of group 0 to 9.7627e-04.
training: bce: 0.066756, dice: 0.142325, loss: 0.104540
training IoU in current batch 3100 is 0.9089805825242718
training IoU uptillnow 3100 is 0.004153305447121032
testing: bce: 7.961895, dice: 16.974940, loss: 12.468417
IoU in current test batch is 0.8840805973320445
Epoch  3139: reducing learning rate of group 0 to 9.7530e-04.
training: bce: 0.065477, dice: 0.140219, loss: 0.102848
training IoU in current batch 3200 is 0.8450581900295293
training IoU uptillnow 3200 is 0.004155554291326799
testing: bce: 8.061201, dice: 17.263149, loss: 12.662175
IoU in current test batch is 0.8907915022941343
Epoch  3240: reducing learning rate of group 0 to 9.7432e-04.
training: bce: 0.064084, dice: 0.138224, loss: 0.101154
training IoU in current batch 3300 is 0.911549639073779
training IoU uptillnow 3300 is 0.004167738293266881
testing: bce: 8.136178, dice: 17.549118, loss: 12.842648
IoU in current test batch is 0.855224242161255
Epoch  3341: reducing learning rate of group 0 to 9.7335e-04.
training: bce: 0.062759, dice: 0.136316, loss: 0.099537
training IoU in current batch 3400 is 0.9476433380942665
training IoU uptillnow 3400 is 0.00418451213617204
testing: bce: 8.209374, dice: 17.831155, loss: 13.020264
IoU in current test batch is 0.8682375729689042
training: bce: 0.061483, dice: 0.134411, loss: 0.097947
training IoU in current batch 3500 is 0.8844485463774804
training IoU uptillnow 3500 is 0.004191302498803156
testing: bce: 8.278972, dice: 18.098994, loss: 13.188983
IoU in current test batch is 0.9108597798374641
Epoch  3532: reducing learning rate of group 0 to 9.7237e-04.
training: bce: 0.060610, dice: 0.132923, loss: 0.096766
training IoU in current batch 3600 is 0.9346198156682027
training IoU uptillnow 3600 is 0.004204682020589822
testing: bce: 8.394523, dice: 18.409784, loss: 13.402154
IoU in current test batch is 0.8567529583503362
Epoch  3633: reducing learning rate of group 0 to 9.7140e-04.
training: bce: 0.059507, dice: 0.131196, loss: 0.095351
training IoU in current batch 3700 is 0.755424371554791
training IoU uptillnow 3700 is 0.004193129462826627
testing: bce: 8.470544, dice: 18.675227, loss: 13.572886
IoU in current test batch is 0.8892515921497496
Epoch  3734: reducing learning rate of group 0 to 9.7043e-04.
training: bce: 0.058436, dice: 0.129577, loss: 0.094006
training IoU in current batch 3800 is 0.9243837848849052
training IoU uptillnow 3800 is 0.004204410427351697
testing: bce: 8.542881, dice: 18.943142, loss: 13.743012
IoU in current test batch is 0.8910145010323031
training: bce: 0.057502, dice: 0.128133, loss: 0.092818
training IoU in current batch 3900 is 0.9210196078431373
training IoU uptillnow 3900 is 0.004214681834987277
testing: bce: 8.627568, dice: 19.224853, loss: 13.926211
IoU in current test batch is 0.8861794301670574
Epoch  3915: reducing learning rate of group 0 to 9.6946e-04.
training: bce: 0.056505, dice: 0.126545, loss: 0.091525
training IoU in current batch 4000 is 0.8867035811879764
training IoU uptillnow 4000 is 0.004220151369377494
testing: bce: 8.695226, dice: 19.473394, loss: 14.084310
IoU in current test batch is 0.8749861502301783
Epoch  4016: reducing learning rate of group 0 to 9.6849e-04.
training: bce: 0.055517, dice: 0.125047, loss: 0.090282
training IoU in current batch 4100 is 0.895259150085974
training IoU uptillnow 4100 is 0.004226397269915226
testing: bce: 8.756739, dice: 19.723740, loss: 14.240240
IoU in current test batch is 0.8999971927342565
Epoch  4121: reducing learning rate of group 0 to 9.6752e-04.
training: bce: 0.054833, dice: 0.123757, loss: 0.089295
training IoU in current batch 4200 is 0.8915941775582463
training IoU uptillnow 4200 is 0.004231909615020582
testing: bce: 8.859747, dice: 19.996247, loss: 14.427997
IoU in current test batch is 0.8909193852841839
Epoch  4222: reducing learning rate of group 0 to 9.6656e-04.
training: bce: 0.054446, dice: 0.122868, loss: 0.088657
training IoU in current batch 4300 is 0.9195737122557727
training IoU uptillnow 4300 is 0.00424041830942324
testing: bce: 9.006597, dice: 20.325238, loss: 14.665918
IoU in current test batch is 0.8593590882932675
Epoch  4323: reducing learning rate of group 0 to 9.6559e-04.
training: bce: 0.053680, dice: 0.121737, loss: 0.087709
training IoU in current batch 4400 is 0.9037076085046565
training IoU uptillnow 4400 is 0.00424673777620579
testing: bce: 9.086436, dice: 20.606375, loss: 14.846405
IoU in current test batch is 0.8982974060868946
Epoch  4424: reducing learning rate of group 0 to 9.6462e-04.
training: bce: 0.053048, dice: 0.120676, loss: 0.086862
training IoU in current batch 4500 is 0.905621313332552
training IoU uptillnow 4500 is 0.004252989026826918
testing: bce: 9.183502, dice: 20.890789, loss: 15.037145
IoU in current test batch is 0.8825776442055636
training: bce: 0.052226, dice: 0.119307, loss: 0.085766
training IoU in current batch 4600 is 0.9363931462382381
training IoU uptillnow 4600 is 0.0042623125804970826
testing: bce: 9.241933, dice: 21.112754, loss: 15.177343
IoU in current test batch is 0.8776114677782073
Epoch  4605: reducing learning rate of group 0 to 9.6366e-04.
training: bce: 0.051612, dice: 0.118315, loss: 0.084963
training IoU in current batch 4700 is 0.9024236983842011
training IoU uptillnow 4700 is 0.0042676264692744475
testing: bce: 9.331757, dice: 21.392182, loss: 15.361970
IoU in current test batch is 0.8643607236736567
Epoch  4706: reducing learning rate of group 0 to 9.6269e-04.
training: bce: 0.051013, dice: 0.117454, loss: 0.084234
training IoU in current batch 4800 is 0.9414913638074527
training IoU uptillnow 4800 is 0.004276787692972903
testing: bce: 9.419785, dice: 21.688403, loss: 15.554094
IoU in current test batch is 0.8905705932789895
Epoch  4807: reducing learning rate of group 0 to 9.6173e-04.
training: bce: 0.050412, dice: 0.116592, loss: 0.083502
training IoU in current batch 4900 is 0.8966589586157404
training IoU uptillnow 4900 is 0.004281001263674918
testing: bce: 9.502580, dice: 21.977660, loss: 15.740120
IoU in current test batch is 0.8706295949866446
Epoch  4908: reducing learning rate of group 0 to 9.6077e-04.
training: bce: 0.049926, dice: 0.115577, loss: 0.082752
training IoU in current batch 5000 is 0.9439440133037694
training IoU uptillnow 5000 is 0.00428977388520749
testing: bce: 9.603161, dice: 22.230845, loss: 15.917003
IoU in current test batch is 0.8570252427771253
Epoch  5009: reducing learning rate of group 0 to 9.5981e-04.
training: bce: 0.049350, dice: 0.114738, loss: 0.082044
training IoU in current batch 5100 is 0.884921188450272
training IoU uptillnow 5100 is 0.004292417132748048
testing: bce: 9.682118, dice: 22.510660, loss: 16.096389
IoU in current test batch is 0.8842040374685413
Epoch  5110: reducing learning rate of group 0 to 9.5885e-04.
training: bce: 0.048856, dice: 0.113885, loss: 0.081370
training IoU in current batch 5200 is 0.9332744854150784
training IoU uptillnow 5200 is 0.004299607198011024
testing: bce: 9.773036, dice: 22.781285, loss: 16.277160
IoU in current test batch is 0.8739320682631019
Epoch  5211: reducing learning rate of group 0 to 9.5789e-04.
training: bce: 0.048420, dice: 0.113391, loss: 0.080906
training IoU in current batch 5300 is 0.9152240615174817
training IoU uptillnow 5300 is 0.004304823442296562
testing: bce: 9.872099, dice: 23.118743, loss: 16.495421
IoU in current test batch is 0.8813755059125465
Epoch  5312: reducing learning rate of group 0 to 9.5693e-04.
training: bce: 0.047846, dice: 0.112361, loss: 0.080104
training IoU in current batch 5400 is 0.943666654353367
training IoU uptillnow 5400 is 0.004312479613921636
testing: bce: 9.939139, dice: 23.340869, loss: 16.640004
IoU in current test batch is 0.8698852679405367
Epoch  5413: reducing learning rate of group 0 to 9.5598e-04.
training: bce: 0.047306, dice: 0.111593, loss: 0.079450
training IoU in current batch 5500 is 0.9325146376701392
training IoU uptillnow 5500 is 0.004318843794514784
testing: bce: 10.008867, dice: 23.610589, loss: 16.809728
IoU in current test batch is 0.8736702426822739
Epoch  5514: reducing learning rate of group 0 to 9.5502e-04.
training: bce: 0.046799, dice: 0.110803, loss: 0.078801
training IoU in current batch 5600 is 0.9255193876692741
training IoU uptillnow 5600 is 0.004324356259143093
testing: bce: 10.081498, dice: 23.869485, loss: 16.975492
IoU in current test batch is 0.8549943682249656
Epoch  5615: reducing learning rate of group 0 to 9.5406e-04.
training: bce: 0.046277, dice: 0.109950, loss: 0.078114
training IoU in current batch 5700 is 0.9091871388876638
training IoU uptillnow 5700 is 0.00432824293578395
testing: bce: 10.147095, dice: 24.108757, loss: 17.127926
IoU in current test batch is 0.8764597434742585
Epoch  5716: reducing learning rate of group 0 to 9.5311e-04.
training: bce: 0.045788, dice: 0.109181, loss: 0.077485
training IoU in current batch 5800 is 0.9516212043232115
training IoU uptillnow 5800 is 0.004335653090685382
testing: bce: 10.216114, dice: 24.359927, loss: 17.288020
IoU in current test batch is 0.8399964747041497
Epoch  5817: reducing learning rate of group 0 to 9.5216e-04.
training: bce: 0.045269, dice: 0.108290, loss: 0.076780
training IoU in current batch 5900 is 0.8910272557811477
training IoU uptillnow 5900 is 0.004337677886283083
testing: bce: 10.274275, dice: 24.577756, loss: 17.426016
IoU in current test batch is 0.9035508901593414
Epoch  5918: reducing learning rate of group 0 to 9.5121e-04.
training: bce: 0.044836, dice: 0.107459, loss: 0.076147
training IoU in current batch 6000 is 0.843652561247216
training IoU uptillnow 6000 is 0.004335687966602247
testing: bce: 10.348439, dice: 24.802382, loss: 17.575411
IoU in current test batch is 0.8914505370048873
Epoch  6019: reducing learning rate of group 0 to 9.5025e-04.
training: bce: 0.044391, dice: 0.106877, loss: 0.075634
training IoU in current batch 6100 is 0.8092707566031779
training IoU uptillnow 6100 is 0.004330945560708355
testing: bce: 10.416557, dice: 25.079026, loss: 17.747791
IoU in current test batch is 0.8876518984701105
Epoch  6120: reducing learning rate of group 0 to 9.4930e-04.
training: bce: 0.043961, dice: 0.106245, loss: 0.075103
training IoU in current batch 6200 is 0.9070690318950455
training IoU uptillnow 6200 is 0.004334241796779422
testing: bce: 10.484769, dice: 25.339381, loss: 17.912075
IoU in current test batch is 0.8848942449237034
Epoch  6221: reducing learning rate of group 0 to 9.4835e-04.
training: bce: 0.043705, dice: 0.105792, loss: 0.074749
training IoU in current batch 6300 is 0.9099580463864482
training IoU uptillnow 6300 is 0.004337662657518239
testing: bce: 10.591715, dice: 25.638350, loss: 18.115033
IoU in current test batch is 0.8659621326053519
Epoch  6322: reducing learning rate of group 0 to 9.4741e-04.
training: bce: 0.043268, dice: 0.105111, loss: 0.074189
training IoU in current batch 6400 is 0.9504280907485758
training IoU uptillnow 6400 is 0.0043441378613336525
testing: bce: 10.652223, dice: 25.877525, loss: 18.264874
IoU in current test batch is 0.8527495075229223
Epoch  6423: reducing learning rate of group 0 to 9.4646e-04.
training: bce: 0.042941, dice: 0.104683, loss: 0.073812
training IoU in current batch 6500 is 0.9197478805095655
training IoU uptillnow 6500 is 0.00434805420560706
testing: bce: 10.736877, dice: 26.174887, loss: 18.455882
IoU in current test batch is 0.868597200935133
Epoch  6524: reducing learning rate of group 0 to 9.4551e-04.
training: bce: 0.042648, dice: 0.104054, loss: 0.073351
training IoU in current batch 6600 is 0.9492985080661465
training IoU uptillnow 6600 is 0.004354090235522583
testing: bce: 10.827672, dice: 26.417604, loss: 18.622638
IoU in current test batch is 0.8779974536187117
Epoch  6625: reducing learning rate of group 0 to 9.4457e-04.
training: bce: 0.042398, dice: 0.103600, loss: 0.072999
training IoU in current batch 6700 is 0.8892738629709155
training IoU uptillnow 6700 is 0.004355467329677664
testing: bce: 10.927329, dice: 26.700834, loss: 18.814082
IoU in current test batch is 0.8966365366480675
Epoch  6726: reducing learning rate of group 0 to 9.4362e-04.
training: bce: 0.042118, dice: 0.103251, loss: 0.072685
training IoU in current batch 6800 is 0.8667688799779848
training IoU uptillnow 6800 is 0.004355149392171595
testing: bce: 11.017094, dice: 27.008192, loss: 19.012643
IoU in current test batch is 0.8546459389336394
Epoch  6827: reducing learning rate of group 0 to 9.4268e-04.
training: bce: 0.041745, dice: 0.102632, loss: 0.072189
training IoU in current batch 6900 is 0.9181883097011119
training IoU uptillnow 6900 is 0.004358566174613763
testing: bce: 11.080165, dice: 27.240895, loss: 19.160530
IoU in current test batch is 0.8809663019629212
Epoch  6928: reducing learning rate of group 0 to 9.4174e-04.
training: bce: 0.041427, dice: 0.102179, loss: 0.071803
training IoU in current batch 7000 is 0.9476765957446809
training IoU uptillnow 7000 is 0.004363991353932569
testing: bce: 11.154919, dice: 27.513555, loss: 19.334237
IoU in current test batch is 0.9061575244620073
Epoch  7029: reducing learning rate of group 0 to 9.4079e-04.
training: bce: 0.041152, dice: 0.101671, loss: 0.071411
training IoU in current batch 7100 is 0.8746566562900567
training IoU uptillnow 7100 is 0.004364122207721017
testing: bce: 11.239133, dice: 27.768010, loss: 19.503572
IoU in current test batch is 0.8905908637382083
Epoch  7130: reducing learning rate of group 0 to 9.3985e-04.
training: bce: 0.040955, dice: 0.101272, loss: 0.071114
training IoU in current batch 7200 is 0.9188577748730582
training IoU uptillnow 7200 is 0.004367318523047282
testing: bce: 11.343069, dice: 28.048383, loss: 19.695726
IoU in current test batch is 0.887963211907607
Epoch  7231: reducing learning rate of group 0 to 9.3891e-04.
training: bce: 0.040831, dice: 0.100888, loss: 0.070859
training IoU in current batch 7300 is 0.7389030348909229
training IoU uptillnow 7300 is 0.004358103301179145
testing: bce: 11.465571, dice: 28.330066, loss: 19.897818
IoU in current test batch is 0.668230631787472
Epoch  7332: reducing learning rate of group 0 to 9.3797e-04.
training: bce: 0.040579, dice: 0.100571, loss: 0.070575
training IoU in current batch 7400 is 0.7613209788548349
training IoU uptillnow 7400 is 0.004350651626987753
testing: bce: 11.550861, dice: 28.627827, loss: 20.089344
IoU in current test batch is 0.8844458951209694
Epoch  7433: reducing learning rate of group 0 to 9.3704e-04.
training: bce: 0.040237, dice: 0.100026, loss: 0.070131
training IoU in current batch 7500 is 0.9524651078484684
training IoU uptillnow 7500 is 0.004356139880717317
testing: bce: 11.608393, dice: 28.857479, loss: 20.232936
IoU in current test batch is 0.8859698926807149
Epoch  7534: reducing learning rate of group 0 to 9.3610e-04.
training: bce: 0.040043, dice: 0.099718, loss: 0.069880
training IoU in current batch 7600 is 0.9285572402833874
training IoU uptillnow 7600 is 0.004359911046625744
testing: bce: 11.706389, dice: 29.152101, loss: 20.429245
IoU in current test batch is 0.8941948106124453
Epoch  7635: reducing learning rate of group 0 to 9.3516e-04.
training: bce: 0.039728, dice: 0.099230, loss: 0.069479
training IoU in current batch 7700 is 0.9460407180614434
training IoU uptillnow 7700 is 0.004364719416235944
testing: bce: 11.767244, dice: 29.391142, loss: 20.579193
IoU in current test batch is 0.9002381846699793
Epoch  7736: reducing learning rate of group 0 to 9.3423e-04.
training: bce: 0.039701, dice: 0.098830, loss: 0.069266
training IoU in current batch 7800 is 0.9371880551560782
training IoU uptillnow 7800 is 0.0043688371044752015
testing: bce: 11.911902, dice: 29.652875, loss: 20.782388
IoU in current test batch is 0.866806083362701
Epoch  7837: reducing learning rate of group 0 to 9.3329e-04.
training: bce: 0.040771, dice: 0.098882, loss: 0.069826
training IoU in current batch 7900 is 0.8296396548807309
training IoU uptillnow 7900 is 0.004366044561378486
testing: bce: 12.389690, dice: 30.048675, loss: 21.219182
IoU in current test batch is 0.7949809655071236
Epoch  7938: reducing learning rate of group 0 to 9.3236e-04.
training: bce: 0.040495, dice: 0.098473, loss: 0.069484
training IoU in current batch 8000 is 0.9383403845348302
training IoU uptillnow 8000 is 0.0043701147696186505
testing: bce: 12.461560, dice: 30.303186, loss: 21.382373
IoU in current test batch is 0.8684629097435747
Epoch  8039: reducing learning rate of group 0 to 9.3143e-04.
training: bce: 0.040204, dice: 0.098039, loss: 0.069121
training IoU in current batch 8100 is 0.9319618882633175
training IoU uptillnow 8100 is 0.004373690805561102
testing: bce: 12.526692, dice: 30.546538, loss: 21.536615
IoU in current test batch is 0.8864800106142673
Epoch  8140: reducing learning rate of group 0 to 9.3050e-04.
training: bce: 0.040227, dice: 0.097729, loss: 0.068978
training IoU in current batch 8200 is 0.9587517433751743
training IoU uptillnow 8200 is 0.00437881296031436
testing: bce: 12.688376, dice: 30.825907, loss: 21.757141
IoU in current test batch is 0.8570429970293755
Epoch  8241: reducing learning rate of group 0 to 9.2957e-04.
training: bce: 0.039962, dice: 0.097344, loss: 0.068653
training IoU in current batch 8300 is 0.9362465591820684
training IoU uptillnow 8300 is 0.00438245613385485
testing: bce: 12.758480, dice: 31.078962, loss: 21.918721
IoU in current test batch is 0.8576838223245099
Epoch  8342: reducing learning rate of group 0 to 9.2864e-04.
training: bce: 0.039689, dice: 0.096949, loss: 0.068319
training IoU in current batch 8400 is 0.8591826041393765
training IoU uptillnow 8400 is 0.004381425981335412
testing: bce: 12.824208, dice: 31.325799, loss: 22.075004
IoU in current test batch is 0.8916821470137598
Epoch  8443: reducing learning rate of group 0 to 9.2771e-04.
training: bce: 0.039407, dice: 0.096495, loss: 0.067951
training IoU in current batch 8500 is 0.9490100859170714
training IoU uptillnow 8500 is 0.004385703412793474
testing: bce: 12.884494, dice: 31.550024, loss: 22.217259
IoU in current test batch is 0.9112292011529803
Epoch  8544: reducing learning rate of group 0 to 9.2678e-04.
training: bce: 0.039121, dice: 0.096002, loss: 0.067561
training IoU in current batch 8600 is 0.9455723345043113
training IoU uptillnow 8600 is 0.00438968153463661
testing: bce: 12.941549, dice: 31.758151, loss: 22.349850
IoU in current test batch is 0.8809830032702167
Epoch  8645: reducing learning rate of group 0 to 9.2585e-04.
training: bce: 0.038917, dice: 0.095620, loss: 0.067268
training IoU in current batch 8700 is 0.29315686670707886
training IoU uptillnow 8700 is 0.004356077383376971
testing: bce: 13.023723, dice: 31.999467, loss: 22.511595
IoU in current test batch is 0.8989587373135565
Epoch  8746: reducing learning rate of group 0 to 9.2493e-04.
training: bce: 0.038718, dice: 0.095244, loss: 0.066981
training IoU in current batch 8800 is 0.6179511490360738
training IoU uptillnow 8800 is 0.004341688999804688
testing: bce: 13.106200, dice: 32.239978, loss: 22.673089
IoU in current test batch is 0.8941443003461574
Epoch  8847: reducing learning rate of group 0 to 9.2400e-04.
training: bce: 0.038568, dice: 0.095028, loss: 0.066798
training IoU in current batch 8900 is 0.7895983470241394
training IoU uptillnow 8900 is 0.004337265932006867
testing: bce: 13.203522, dice: 32.532551, loss: 22.868037
IoU in current test batch is 0.8823406240744173
Epoch  8948: reducing learning rate of group 0 to 9.2308e-04.
training: bce: 0.038325, dice: 0.094638, loss: 0.066481
training IoU in current batch 9000 is 0.9042445447012246
training IoU uptillnow 9000 is 0.0043393096692749406
testing: bce: 13.267733, dice: 32.762935, loss: 23.015334
IoU in current test batch is 0.8786761994440787
Epoch  9049: reducing learning rate of group 0 to 9.2216e-04.
training: bce: 0.038142, dice: 0.094337, loss: 0.066239
training IoU in current batch 9100 is 0.9233579479207151
training IoU uptillnow 9100 is 0.004342358565773442
testing: bce: 13.351005, dice: 33.021648, loss: 23.186327
IoU in current test batch is 0.8952914775258749
Epoch  9150: reducing learning rate of group 0 to 9.2123e-04.
training: bce: 0.037902, dice: 0.093976, loss: 0.065939
training IoU in current batch 9200 is 0.9488626186638008
training IoU uptillnow 9200 is 0.004346727161877621
testing: bce: 13.413015, dice: 33.256777, loss: 23.334896
IoU in current test batch is 0.8955366419073795
Epoch  9251: reducing learning rate of group 0 to 9.2031e-04.
training: bce: 0.037645, dice: 0.093521, loss: 0.065583
training IoU in current batch 9300 is 0.9260235329052374
training IoU uptillnow 9300 is 0.004349774043961791
testing: bce: 13.466671, dice: 33.455474, loss: 23.461073
IoU in current test batch is 0.8901559150946411
Epoch  9364: reducing learning rate of group 0 to 9.1939e-04.
training: bce: 0.037385, dice: 0.093093, loss: 0.065239
training IoU in current batch 9400 is 0.9040008658321007
training IoU uptillnow 9400 is 0.004351584811807751
testing: bce: 13.517585, dice: 33.660218, loss: 23.588902
IoU in current test batch is 0.8621557029284641
Epoch  9465: reducing learning rate of group 0 to 9.1847e-04.
training: bce: 0.037152, dice: 0.092743, loss: 0.064948
training IoU in current batch 9500 is 0.9581637976004174
training IoU uptillnow 9500 is 0.004356207842817059
testing: bce: 13.576327, dice: 33.890352, loss: 23.733339
IoU in current test batch is 0.9074860541901422
Epoch  9566: reducing learning rate of group 0 to 9.1755e-04.
training: bce: 0.036901, dice: 0.092307, loss: 0.064604
training IoU in current batch 9600 is 0.9067320763638513
training IoU uptillnow 9600 is 0.004358056114236726
testing: bce: 13.626520, dice: 34.086187, loss: 23.856354
IoU in current test batch is 0.8832815014823221
Epoch  9667: reducing learning rate of group 0 to 9.1664e-04.
training: bce: 0.036670, dice: 0.091918, loss: 0.064294
training IoU in current batch 9700 is 0.9185344142425688
training IoU uptillnow 9700 is 0.004360474586115667
testing: bce: 13.682013, dice: 34.296090, loss: 23.989052
IoU in current test batch is 0.8991721894278398
Epoch  9768: reducing learning rate of group 0 to 9.1572e-04.
training: bce: 0.036468, dice: 0.091569, loss: 0.064018
training IoU in current batch 9800 is 0.8268709707558628
training IoU uptillnow 9800 is 0.004358167477327418
testing: bce: 13.746848, dice: 34.518059, loss: 24.132454
IoU in current test batch is 0.8672977538499556
Epoch  9869: reducing learning rate of group 0 to 9.1480e-04.
training: bce: 0.036352, dice: 0.091330, loss: 0.063841
training IoU in current batch 9900 is 0.8021803315920963
training IoU uptillnow 9900 is 0.0043546600960591925
testing: bce: 13.843081, dice: 34.779198, loss: 24.311140
IoU in current test batch is 0.8706994173383499
Epoch  9970: reducing learning rate of group 0 to 9.1389e-04.
training: bce: 0.036147, dice: 0.091033, loss: 0.063590
training IoU in current batch 10000 is 0.9215662004373478
training IoU uptillnow 10000 is 0.004357191551974877
testing: bce: 13.904135, dice: 35.016093, loss: 24.460114
IoU in current test batch is 0.8916542635424581
Epoch 10071: reducing learning rate of group 0 to 9.1298e-04.
training: bce: 0.035944, dice: 0.090732, loss: 0.063338
training IoU in current batch 10100 is 0.8820456811814427
training IoU uptillnow 10100 is 0.004357716617353872
testing: bce: 13.964355, dice: 35.249377, loss: 24.606866
IoU in current test batch is 0.9090715295052021
Epoch 10172: reducing learning rate of group 0 to 9.1206e-04.
training: bce: 0.035738, dice: 0.090410, loss: 0.063074
training IoU in current batch 10200 is 0.9353719008264463
training IoU uptillnow 10200 is 0.004360845162464924
testing: bce: 14.021488, dice: 35.472194, loss: 24.746841
IoU in current test batch is 0.9074797457791318
Epoch 10273: reducing learning rate of group 0 to 9.1115e-04.
training: bce: 0.035561, dice: 0.090149, loss: 0.062855
training IoU in current batch 10300 is 0.8526460245304057
training IoU uptillnow 10300 is 0.004359897535634394
testing: bce: 14.088937, dice: 35.716212, loss: 24.902574
IoU in current test batch is 0.8116481385471526
Epoch 10374: reducing learning rate of group 0 to 9.1024e-04.
training: bce: 0.035671, dice: 0.090090, loss: 0.062880
training IoU in current batch 10400 is 0.8589541922875257
training IoU uptillnow 10400 is 0.0043592713787822
testing: bce: 14.269789, dice: 36.039297, loss: 25.154543
IoU in current test batch is 0.8585790998073611
Epoch 10475: reducing learning rate of group 0 to 9.0933e-04.
training: bce: 0.035506, dice: 0.089879, loss: 0.062692
training IoU in current batch 10500 is 0.840963746086483
training IoU uptillnow 10500 is 0.004357800541258633
testing: bce: 14.340314, dice: 36.300721, loss: 25.320518
IoU in current test batch is 0.8457901372362426
Epoch 10576: reducing learning rate of group 0 to 9.0842e-04.
training: bce: 0.035314, dice: 0.089585, loss: 0.062449
training IoU in current batch 10600 is 0.8326312695924765
training IoU uptillnow 10600 is 0.004355964448500438
testing: bce: 14.398428, dice: 36.526644, loss: 25.462536
IoU in current test batch is 0.9177300264670492
Epoch 10677: reducing learning rate of group 0 to 9.0751e-04.
training: bce: 0.035512, dice: 0.089391, loss: 0.062452
training IoU in current batch 10700 is 0.1883997839344288
training IoU uptillnow 10700 is 0.004324061210215901
testing: bce: 14.616068, dice: 36.791285, loss: 25.703676
IoU in current test batch is 0.8751961484604015
Epoch 10778: reducing learning rate of group 0 to 9.0660e-04.
training: bce: 0.035364, dice: 0.089211, loss: 0.062287
training IoU in current batch 10800 is 0.9056691932301941
training IoU uptillnow 10800 is 0.004325952560608782
testing: bce: 14.690828, dice: 37.060280, loss: 25.875554
IoU in current test batch is 0.8846466766493665
Epoch 10879: reducing learning rate of group 0 to 9.0570e-04.
training: bce: 0.035184, dice: 0.088947, loss: 0.062065
training IoU in current batch 10900 is 0.9381071329447682
training IoU uptillnow 10900 is 0.004329297052894949
testing: bce: 14.751438, dice: 37.292571, loss: 26.022004
IoU in current test batch is 0.8863904984193856
Epoch 10980: reducing learning rate of group 0 to 9.0479e-04.
training: bce: 0.035018, dice: 0.088732, loss: 0.061875
training IoU in current batch 11000 is 0.9262112575703598
training IoU uptillnow 11000 is 0.004332040069302156
testing: bce: 14.816864, dice: 37.544000, loss: 26.180432
IoU in current test batch is 0.8897617030308526
Epoch 11081: reducing learning rate of group 0 to 9.0389e-04.
training: bce: 0.034858, dice: 0.088564, loss: 0.061711
training IoU in current batch 11100 is 0.9431337788081277
training IoU uptillnow 11100 is 0.004335495873506628
testing: bce: 14.882871, dice: 37.813378, loss: 26.348124
IoU in current test batch is 0.9094849057444917
Epoch 11182: reducing learning rate of group 0 to 9.0298e-04.
training: bce: 0.034765, dice: 0.088360, loss: 0.061562
training IoU in current batch 11200 is 0.9045464725643897
training IoU uptillnow 11200 is 0.004337167478625058
testing: bce: 14.977095, dice: 38.066096, loss: 26.521596
IoU in current test batch is 0.8837478391524611
Epoch 11283: reducing learning rate of group 0 to 9.0208e-04.
training: bce: 0.034588, dice: 0.088084, loss: 0.061336
training IoU in current batch 11300 is 0.9267682838870991
training IoU uptillnow 11300 is 0.004339792679410922
testing: bce: 15.033964, dice: 38.286231, loss: 26.660098
IoU in current test batch is 0.8719444042956105
Epoch 11384: reducing learning rate of group 0 to 9.0118e-04.
training: bce: 0.034542, dice: 0.087927, loss: 0.061234
training IoU in current batch 11400 is 0.8963462572930029
training IoU uptillnow 11400 is 0.004341037645703827
testing: bce: 15.146479, dice: 38.555850, loss: 26.851164
IoU in current test batch is 0.8352201574109901
Epoch 11485: reducing learning rate of group 0 to 9.0028e-04.
training: bce: 0.034400, dice: 0.087731, loss: 0.061066
training IoU in current batch 11500 is 0.7741924775088802
training IoU uptillnow 11500 is 0.0043369503901768336
testing: bce: 15.216877, dice: 38.807320, loss: 27.012099
IoU in current test batch is 0.9024597050204993
Epoch 11586: reducing learning rate of group 0 to 8.9938e-04.
training: bce: 0.034237, dice: 0.087487, loss: 0.060862
training IoU in current batch 11600 is 0.9396033269353806
training IoU uptillnow 11600 is 0.00434006276190772
testing: bce: 15.276104, dice: 39.035819, loss: 27.155961
IoU in current test batch is 0.8784490565715134
Epoch 11687: reducing learning rate of group 0 to 8.9848e-04.
training: bce: 0.034066, dice: 0.087220, loss: 0.060643
training IoU in current batch 11700 is 0.9180158132530121
training IoU uptillnow 11700 is 0.004342199470773264
testing: bce: 15.330941, dice: 39.252543, loss: 27.291742
IoU in current test batch is 0.885013113992032
Epoch 11788: reducing learning rate of group 0 to 8.9758e-04.
training: bce: 0.033953, dice: 0.087104, loss: 0.060529
training IoU in current batch 11800 is 0.9516519485833135
training IoU uptillnow 11800 is 0.004345725106500264
testing: bce: 15.410582, dice: 39.535391, loss: 27.472987
IoU in current test batch is 0.8943293692114085
Epoch 11889: reducing learning rate of group 0 to 8.9668e-04.
training: bce: 0.033818, dice: 0.086894, loss: 0.060356
training IoU in current batch 11900 is 0.9343355726691791
training IoU uptillnow 11900 is 0.004348463975140258
testing: bce: 15.479644, dice: 39.774284, loss: 27.626964
IoU in current test batch is 0.8779825067625584
Epoch 11990: reducing learning rate of group 0 to 8.9578e-04.
training: bce: 0.033687, dice: 0.086713, loss: 0.060200
training IoU in current batch 12000 is 0.9344191284761327
training IoU uptillnow 12000 is 0.0043511606809751085
testing: bce: 15.549163, dice: 40.024524, loss: 27.786843
IoU in current test batch is 0.8799770305007281
Epoch 12091: reducing learning rate of group 0 to 8.9489e-04.
training: bce: 0.033541, dice: 0.086504, loss: 0.060023
training IoU in current batch 12100 is 0.8286266924564797
training IoU uptillnow 12100 is 0.00434944158983642
testing: bce: 15.610973, dice: 40.261072, loss: 27.936022
IoU in current test batch is 0.9126219130826883
training: bce: 0.033466, dice: 0.086368, loss: 0.059917
training IoU in current batch 12200 is 0.5745471222196744
training IoU uptillnow 12200 is 0.004337338434531625
testing: bce: 15.704714, dice: 40.529975, loss: 28.117345
IoU in current test batch is 0.8934095825096766
Epoch 12272: reducing learning rate of group 0 to 8.9399e-04.
training: bce: 0.033347, dice: 0.086208, loss: 0.059778
training IoU in current batch 12300 is 0.9313158285120902
training IoU uptillnow 12300 is 0.004339933676447151
testing: bce: 15.776950, dice: 40.786415, loss: 28.281683
IoU in current test batch is 0.914244321455466
Epoch 12373: reducing learning rate of group 0 to 8.9310e-04.
training: bce: 0.033228, dice: 0.086056, loss: 0.059642
training IoU in current batch 12400 is 0.9379146531247441
training IoU uptillnow 12400 is 0.004342753123178677
testing: bce: 15.848321, dice: 41.045375, loss: 28.446848
IoU in current test batch is 0.8973587003524331
Epoch 12474: reducing learning rate of group 0 to 8.9221e-04.
training: bce: 0.033180, dice: 0.085977, loss: 0.059578
training IoU in current batch 12500 is 0.9051668083829341
training IoU uptillnow 12500 is 0.004344217653366149
testing: bce: 15.953400, dice: 41.338182, loss: 28.645791
IoU in current test batch is 0.8693742012658062
Epoch 12575: reducing learning rate of group 0 to 8.9131e-04.
training: bce: 0.033044, dice: 0.085772, loss: 0.059408
training IoU in current batch 12600 is 0.9208938636183218
training IoU uptillnow 12600 is 0.004346282978854012
testing: bce: 16.014992, dice: 41.569721, loss: 28.792356
IoU in current test batch is 0.9081484067723611
Epoch 12676: reducing learning rate of group 0 to 8.9042e-04.
training: bce: 0.032949, dice: 0.085631, loss: 0.059290
training IoU in current batch 12700 is 0.9486199939906426
training IoU uptillnow 12700 is 0.004349407276083357
testing: bce: 16.095705, dice: 41.830983, loss: 28.963344
IoU in current test batch is 0.8976085984647753
Epoch 12777: reducing learning rate of group 0 to 8.8953e-04.
training: bce: 0.032820, dice: 0.085425, loss: 0.059123
training IoU in current batch 12800 is 0.9404620966202024
training IoU uptillnow 12800 is 0.004352164117009985
testing: bce: 16.159032, dice: 42.058711, loss: 29.108872
IoU in current test batch is 0.906096223063841
Epoch 12878: reducing learning rate of group 0 to 8.8864e-04.
training: bce: 0.032689, dice: 0.085243, loss: 0.058966
training IoU in current batch 12900 is 0.9097508826015613
training IoU uptillnow 12900 is 0.0043536879546659645
testing: bce: 16.220007, dice: 42.297007, loss: 29.258507
IoU in current test batch is 0.9091555085887517
Epoch 12979: reducing learning rate of group 0 to 8.8775e-04.
training: bce: 0.032650, dice: 0.085123, loss: 0.058887
training IoU in current batch 13000 is 0.8822437449556093
training IoU uptillnow 13000 is 0.0043541304650121845
testing: bce: 16.326302, dice: 42.565004, loss: 29.445653
IoU in current test batch is 0.9080615677289786
Epoch 13080: reducing learning rate of group 0 to 8.8687e-04.
training: bce: 0.032541, dice: 0.084926, loss: 0.058733
training IoU in current batch 13100 is 0.9240674807839484
training IoU uptillnow 13100 is 0.004356162423938278
testing: bce: 16.396783, dice: 42.792765, loss: 29.594774
IoU in current test batch is 0.8070930290362482
Epoch 13181: reducing learning rate of group 0 to 8.8598e-04.
training: bce: 0.032414, dice: 0.084752, loss: 0.058583
training IoU in current batch 13200 is 0.9157346269595292
training IoU uptillnow 13200 is 0.004357847983447857
testing: bce: 16.457481, dice: 43.031448, loss: 29.744465
IoU in current test batch is 0.8836914849639607
Epoch 13282: reducing learning rate of group 0 to 8.8509e-04.
training: bce: 0.032308, dice: 0.084589, loss: 0.058449
training IoU in current batch 13300 is 0.7631437768240343
training IoU uptillnow 13300 is 0.004353772131261347
testing: bce: 16.528051, dice: 43.273889, loss: 29.900970
IoU in current test batch is 0.8915358674734227
Epoch 13383: reducing learning rate of group 0 to 8.8421e-04.
training: bce: 0.032264, dice: 0.084486, loss: 0.058375
training IoU in current batch 13400 is 0.936062113134638
training IoU uptillnow 13400 is 0.00435620880340829
testing: bce: 16.629552, dice: 43.545945, loss: 30.087748
IoU in current test batch is 0.8464431274168263
Epoch 13484: reducing learning rate of group 0 to 8.8333e-04.
training: bce: 0.032149, dice: 0.084329, loss: 0.058239
training IoU in current batch 13500 is 0.8951239212685457
training IoU uptillnow 13500 is 0.004357093262358993
testing: bce: 16.693860, dice: 43.789249, loss: 30.241555
IoU in current test batch is 0.8686447854539855
Epoch 13585: reducing learning rate of group 0 to 8.8244e-04.
training: bce: 0.032010, dice: 0.084098, loss: 0.058054
training IoU in current batch 13600 is 0.9280677009873061
training IoU uptillnow 13600 is 0.004359175794838793
testing: bce: 16.744691, dice: 43.993167, loss: 30.368929
IoU in current test batch is 0.8704046467337136
Epoch 13686: reducing learning rate of group 0 to 8.8156e-04.
training: bce: 0.031905, dice: 0.083941, loss: 0.057923
training IoU in current batch 13700 is 0.9015022330491271
training IoU uptillnow 13700 is 0.004360258455742426
testing: bce: 16.812601, dice: 44.233422, loss: 30.523011
IoU in current test batch is 0.8978676103187916
Epoch 13787: reducing learning rate of group 0 to 8.8068e-04.
training: bce: 0.031880, dice: 0.083762, loss: 0.057821
training IoU in current batch 13800 is 0.9345638221108199
training IoU uptillnow 13800 is 0.004362523223910035
testing: bce: 16.922390, dice: 44.461700, loss: 30.692045
IoU in current test batch is 0.9094710396944911
Epoch 13888: reducing learning rate of group 0 to 8.7980e-04.
training: bce: 0.031749, dice: 0.083573, loss: 0.057661
training IoU in current batch 13900 is 0.9504711163731439
training IoU uptillnow 13900 is 0.004365327571496221
testing: bce: 16.974496, dice: 44.682733, loss: 30.828615
IoU in current test batch is 0.8938181949719571
Epoch 13989: reducing learning rate of group 0 to 8.7892e-04.
training: bce: 0.031692, dice: 0.083460, loss: 0.057576
training IoU in current batch 14000 is 0.9145699688856451
training IoU uptillnow 14000 is 0.0043668097675745865
testing: bce: 17.065981, dice: 44.943063, loss: 31.004522
IoU in current test batch is 0.8851290863823205
Epoch 14090: reducing learning rate of group 0 to 8.7804e-04.
training: bce: 0.031561, dice: 0.083221, loss: 0.057391
training IoU in current batch 14100 is 0.9407907729983805
training IoU uptillnow 14100 is 0.004369200690895041
testing: bce: 17.116825, dice: 45.134743, loss: 31.125784
IoU in current test batch is 0.8942038633778735
Epoch 14191: reducing learning rate of group 0 to 8.7716e-04.
training: bce: 0.031442, dice: 0.083052, loss: 0.057247
training IoU in current batch 14200 is 0.877140811718707
training IoU uptillnow 14200 is 0.004369316903610332
testing: bce: 17.173514, dice: 45.362148, loss: 31.267831
IoU in current test batch is 0.9007565755404963
Epoch 14292: reducing learning rate of group 0 to 8.7628e-04.
training: bce: 0.031397, dice: 0.082986, loss: 0.057192
training IoU in current batch 14300 is 0.9092911493792551
training IoU uptillnow 14300 is 0.004370555550161524
testing: bce: 17.269601, dice: 45.645595, loss: 31.457598
IoU in current test batch is 0.8993670570739594
Epoch 14393: reducing learning rate of group 0 to 8.7541e-04.
training: bce: 0.031279, dice: 0.082796, loss: 0.057037
training IoU in current batch 14400 is 0.9460860116186735
training IoU uptillnow 14400 is 0.004373054505150287
testing: bce: 17.324931, dice: 45.859304, loss: 31.592118
IoU in current test batch is 0.883621151020452
Epoch 14494: reducing learning rate of group 0 to 8.7453e-04.
training: bce: 0.031180, dice: 0.082686, loss: 0.056933
training IoU in current batch 14500 is 0.8703926626540556
training IoU uptillnow 14500 is 0.004372909058685355
testing: bce: 17.390113, dice: 46.116408, loss: 31.753260
IoU in current test batch is 0.881028574498735
Epoch 14595: reducing learning rate of group 0 to 8.7366e-04.
training: bce: 0.031367, dice: 0.082798, loss: 0.057083
training IoU in current batch 14600 is 0.6171792152704135
training IoU uptillnow 14600 is 0.004364094505008666
testing: bce: 17.614908, dice: 46.497680, loss: 32.056294
IoU in current test batch is 0.8081901505265647
Epoch 14696: reducing learning rate of group 0 to 8.7278e-04.
training: bce: 0.031316, dice: 0.082722, loss: 0.057019
training IoU in current batch 14700 is 0.9276676420887772
training IoU uptillnow 14700 is 0.004365959981543834
testing: bce: 17.706818, dice: 46.772839, loss: 32.239829
IoU in current test batch is 0.8284477323780928
Epoch 14797: reducing learning rate of group 0 to 8.7191e-04.
training: bce: 0.031282, dice: 0.082650, loss: 0.056966
training IoU in current batch 14800 is 0.931577480490524
training IoU uptillnow 14800 is 0.004367932330850697
testing: bce: 17.808006, dice: 47.050251, loss: 32.429129
IoU in current test batch is 0.8550260510013215
Epoch 14898: reducing learning rate of group 0 to 8.7104e-04.
training: bce: 0.031184, dice: 0.082490, loss: 0.056837
training IoU in current batch 14900 is 0.9364518176049316
training IoU uptillnow 14900 is 0.00437004176482945
testing: bce: 17.871976, dice: 47.276259, loss: 32.574117
IoU in current test batch is 0.8788511651902604
Epoch 14999: reducing learning rate of group 0 to 8.7017e-04.
training: bce: 0.031123, dice: 0.082416, loss: 0.056769
training IoU in current batch 15000 is 0.9343001214619603
training IoU uptillnow 15000 is 0.0043720513564732095
testing: bce: 17.956528, dice: 47.550711, loss: 32.753620
IoU in current test batch is 0.8921517833237689
Epoch 15100: reducing learning rate of group 0 to 8.6930e-04.
training: bce: 0.031081, dice: 0.082298, loss: 0.056690
training IoU in current batch 15100 is 0.9440226535888702
training IoU uptillnow 15100 is 0.004374356249602612
testing: bce: 18.052374, dice: 47.799522, loss: 32.925948
IoU in current test batch is 0.9051490778626483
training: bce: 0.031006, dice: 0.082250, loss: 0.056628
training IoU in current batch 15200 is 0.95472017491272
training IoU uptillnow 15200 is 0.004376982686185475
testing: bce: 18.127950, dice: 48.087563, loss: 33.107757
IoU in current test batch is 0.8830023665223758
Epoch 15201: reducing learning rate of group 0 to 8.6843e-04.
training: bce: 0.030961, dice: 0.082163, loss: 0.056562
training IoU in current batch 15300 is 0.9548704200178731
training IoU uptillnow 15300 is 0.004379579702157659
testing: bce: 18.220390, dice: 48.352782, loss: 33.286586
IoU in current test batch is 0.8734010233873968
Epoch 15302: reducing learning rate of group 0 to 8.6756e-04.
training: bce: 0.030854, dice: 0.082019, loss: 0.056437
training IoU in current batch 15400 is 0.9379496402877698
training IoU uptillnow 15400 is 0.004381593652545823
testing: bce: 18.276413, dice: 48.583538, loss: 33.429975
IoU in current test batch is 0.8704735645213286
Epoch 15403: reducing learning rate of group 0 to 8.6669e-04.
training: bce: 0.030762, dice: 0.081853, loss: 0.056307
training IoU in current batch 15500 is 0.8829901907944379
training IoU uptillnow 15500 is 0.004381808847058606
testing: bce: 18.339905, dice: 48.799985, loss: 33.569945
IoU in current test batch is 0.8900316883336266
Epoch 15504: reducing learning rate of group 0 to 8.6583e-04.
training: bce: 0.030654, dice: 0.081686, loss: 0.056170
training IoU in current batch 15600 is 0.8137239122270508
training IoU uptillnow 15600 is 0.004379801352116465
testing: bce: 18.393691, dice: 49.014504, loss: 33.704097
IoU in current test batch is 0.8989870857397807
Epoch 15605: reducing learning rate of group 0 to 8.6496e-04.
training: bce: 0.030556, dice: 0.081523, loss: 0.056039
training IoU in current batch 15700 is 0.8992497320471597
training IoU uptillnow 15700 is 0.004380543007476757
testing: bce: 18.452003, dice: 49.230190, loss: 33.841096
IoU in current test batch is 0.9098551219058819
Epoch 15706: reducing learning rate of group 0 to 8.6409e-04.
training: bce: 0.030524, dice: 0.081460, loss: 0.055992
training IoU in current batch 15800 is 0.9409181337036898
training IoU uptillnow 15800 is 0.004382593812242542
testing: bce: 18.550297, dice: 49.505917, loss: 34.028107
IoU in current test batch is 0.8932623223326931
Epoch 15807: reducing learning rate of group 0 to 8.6323e-04.
training: bce: 0.030435, dice: 0.081336, loss: 0.055886
training IoU in current batch 15900 is 0.9191552815056624
training IoU uptillnow 15900 is 0.004383934498962155
testing: bce: 18.613262, dice: 49.743329, loss: 34.178295
IoU in current test batch is 0.8937425761881547
Epoch 15908: reducing learning rate of group 0 to 8.6237e-04.
training: bce: 0.030403, dice: 0.081244, loss: 0.055824
training IoU in current batch 16000 is 0.8969135802469136
training IoU uptillnow 16000 is 0.004384563418418892
testing: bce: 18.710951, dice: 49.999470, loss: 34.355210
IoU in current test batch is 0.8831199136031037
Epoch 16009: reducing learning rate of group 0 to 8.6150e-04.
training: bce: 0.030303, dice: 0.081077, loss: 0.055690
training IoU in current batch 16100 is 0.8704946855270519
training IoU uptillnow 16100 is 0.004384364114085102
testing: bce: 18.765779, dice: 50.208729, loss: 34.487254
IoU in current test batch is 0.911952246115786
Epoch 16110: reducing learning rate of group 0 to 8.6064e-04.
training: bce: 0.030274, dice: 0.081027, loss: 0.055650
training IoU in current batch 16200 is 0.9118339529120199
training IoU uptillnow 16200 is 0.004385443094706514
testing: bce: 18.863895, dice: 50.489076, loss: 34.676485
IoU in current test batch is 0.8967303393634732
Epoch 16211: reducing learning rate of group 0 to 8.5978e-04.
training: bce: 0.030225, dice: 0.080938, loss: 0.055582
training IoU in current batch 16300 is 0.959314706072143
training IoU uptillnow 16300 is 0.004387965212586731
testing: bce: 18.949650, dice: 50.745288, loss: 34.847469
IoU in current test batch is 0.885992375945285
Epoch 16312: reducing learning rate of group 0 to 8.5892e-04.
training: bce: 0.030155, dice: 0.080831, loss: 0.055493
training IoU in current batch 16400 is 0.7119862646206675
training IoU uptillnow 16400 is 0.0043829165333020325
testing: bce: 19.022048, dice: 50.989120, loss: 35.005584
IoU in current test batch is 0.8710864808221476
Epoch 16413: reducing learning rate of group 0 to 8.5806e-04.
training: bce: 0.030287, dice: 0.080766, loss: 0.055527
training IoU in current batch 16500 is 0.9327370304114491
training IoU uptillnow 16500 is 0.004384618058171769
testing: bce: 19.221729, dice: 51.258506, loss: 35.240118
IoU in current test batch is 0.9059003182568885
Epoch 16514: reducing learning rate of group 0 to 8.5721e-04.
training: bce: 0.030207, dice: 0.080691, loss: 0.055449
training IoU in current batch 16600 is 0.9377020163152224
training IoU uptillnow 16600 is 0.004386448622736581
testing: bce: 19.287372, dice: 51.521045, loss: 35.404208
IoU in current test batch is 0.8853725376629218
Epoch 16615: reducing learning rate of group 0 to 8.5635e-04.
training: bce: 0.030144, dice: 0.080617, loss: 0.055380
training IoU in current batch 16700 is 0.9414785092491839
training IoU uptillnow 16700 is 0.004388370327565689
testing: bce: 19.362818, dice: 51.783947, loss: 35.573382
IoU in current test batch is 0.8924129084215154
Epoch 16716: reducing learning rate of group 0 to 8.5549e-04.
training: bce: 0.030064, dice: 0.080470, loss: 0.055267
training IoU in current batch 16800 is 0.9304617010834686
training IoU uptillnow 16800 is 0.0043899412946381946
testing: bce: 19.427023, dice: 51.998842, loss: 35.712933
IoU in current test batch is 0.8820539998534342
Epoch 16817: reducing learning rate of group 0 to 8.5464e-04.
training: bce: 0.029968, dice: 0.080329, loss: 0.055148
training IoU in current batch 16900 is 0.9564164648910412
training IoU uptillnow 16900 is 0.004392261518470021
testing: bce: 19.480070, dice: 52.216998, loss: 35.848534
IoU in current test batch is 0.8827870171334331
Epoch 16918: reducing learning rate of group 0 to 8.5378e-04.
training: bce: 0.029877, dice: 0.080199, loss: 0.055038
training IoU in current batch 17000 is 0.8562902840773454
training IoU uptillnow 17000 is 0.004391609732703988
testing: bce: 19.535888, dice: 52.440765, loss: 35.988326
IoU in current test batch is 0.9062454554899679
Epoch 17019: reducing learning rate of group 0 to 8.5293e-04.
training: bce: 0.029797, dice: 0.080074, loss: 0.054936
training IoU in current batch 17100 is 0.9245947059043768
training IoU uptillnow 17100 is 0.004392962658245289
testing: bce: 19.598132, dice: 52.667443, loss: 36.132788
IoU in current test batch is 0.8907413611854836
Epoch 17120: reducing learning rate of group 0 to 8.5208e-04.
training: bce: 0.029715, dice: 0.079942, loss: 0.054829
training IoU in current batch 17200 is 0.9281566774923729
training IoU uptillnow 17200 is 0.00439440339267478
testing: bce: 19.659004, dice: 52.887672, loss: 36.273338
IoU in current test batch is 0.8940498616808554
Epoch 17221: reducing learning rate of group 0 to 8.5122e-04.
training: bce: 0.029657, dice: 0.079848, loss: 0.054752
training IoU in current batch 17300 is 0.9119095914227761
training IoU uptillnow 17300 is 0.004395357930357221
testing: bce: 19.734166, dice: 53.132608, loss: 36.433387
IoU in current test batch is 0.862927121185495
Epoch 17322: reducing learning rate of group 0 to 8.5037e-04.
training: bce: 0.029602, dice: 0.079737, loss: 0.054670
training IoU in current batch 17400 is 0.911001605256281
training IoU uptillnow 17400 is 0.0043962754069156035
testing: bce: 19.811942, dice: 53.365755, loss: 36.588848
IoU in current test batch is 0.9005025915599886
Epoch 17423: reducing learning rate of group 0 to 8.4952e-04.
training: bce: 0.029549, dice: 0.079647, loss: 0.054598
training IoU in current batch 17500 is 0.9201275126184362
training IoU uptillnow 17500 is 0.004397443123938497
testing: bce: 19.889792, dice: 53.611374, loss: 36.750583
IoU in current test batch is 0.887083268580562
Epoch 17524: reducing learning rate of group 0 to 8.4867e-04.
training: bce: 0.029480, dice: 0.079546, loss: 0.054513
training IoU in current batch 17600 is 0.893039283252929
training IoU uptillnow 17600 is 0.004397828063955122
testing: bce: 19.956735, dice: 53.849517, loss: 36.903126
IoU in current test batch is 0.8883116704440415
Epoch 17625: reducing learning rate of group 0 to 8.4782e-04.
training: bce: 0.029392, dice: 0.079401, loss: 0.054396
training IoU in current batch 17700 is 0.9139945532812088
training IoU uptillnow 17700 is 0.004398800577951229
testing: bce: 20.010163, dice: 54.056824, loss: 37.033493
IoU in current test batch is 0.9192220109533574
Epoch 17726: reducing learning rate of group 0 to 8.4698e-04.
training: bce: 0.029389, dice: 0.079291, loss: 0.054340
training IoU in current batch 17800 is 0.9255609756097561
training IoU uptillnow 17800 is 0.004400087046689489
testing: bce: 20.121317, dice: 54.286595, loss: 37.203956
IoU in current test batch is 0.867194450272786
Epoch 17827: reducing learning rate of group 0 to 8.4613e-04.
training: bce: 0.029322, dice: 0.079217, loss: 0.054269
training IoU in current batch 17900 is 0.8737112105819042
training IoU uptillnow 17900 is 0.004399910905726526
testing: bce: 20.188171, dice: 54.540630, loss: 37.364401
IoU in current test batch is 0.8975728180605812
Epoch 17928: reducing learning rate of group 0 to 8.4528e-04.
training: bce: 0.029301, dice: 0.079166, loss: 0.054233
training IoU in current batch 18000 is 0.9572988727745197
training IoU uptillnow 18000 is 0.004402058472295862
testing: bce: 20.286154, dice: 54.810427, loss: 37.548290
IoU in current test batch is 0.8944601934849996
Epoch 18029: reducing learning rate of group 0 to 8.4444e-04.
training: bce: 0.029235, dice: 0.079104, loss: 0.054169
training IoU in current batch 18100 is 0.9145246524036735
training IoU uptillnow 18100 is 0.004403000767139917
testing: bce: 20.353263, dice: 55.071321, loss: 37.712292
IoU in current test batch is 0.9007303793060834
Epoch 18130: reducing learning rate of group 0 to 8.4359e-04.
training: bce: 0.029168, dice: 0.078985, loss: 0.054076
training IoU in current batch 18200 is 0.8344012023414017
training IoU uptillnow 18200 is 0.004401731634919529
testing: bce: 20.418434, dice: 55.292537, loss: 37.855485
IoU in current test batch is 0.9022820872959014
Epoch 18231: reducing learning rate of group 0 to 8.4275e-04.
training: bce: 0.029100, dice: 0.078898, loss: 0.053999
training IoU in current batch 18300 is 0.8649295831811356
training IoU uptillnow 18300 is 0.004401310435427622
testing: bce: 20.483261, dice: 55.535249, loss: 38.009255
IoU in current test batch is 0.9078435106197843
Epoch 18332: reducing learning rate of group 0 to 8.4191e-04.
training: bce: 0.029119, dice: 0.078855, loss: 0.053987
training IoU in current batch 18400 is 0.9416105092553101
training IoU uptillnow 18400 is 0.004402977421519948
testing: bce: 20.608558, dice: 55.808306, loss: 38.208432
IoU in current test batch is 0.8869379442529723
Epoch 18433: reducing learning rate of group 0 to 8.4106e-04.
training: bce: 0.029038, dice: 0.078723, loss: 0.053880
training IoU in current batch 18500 is 0.826819982907311
training IoU uptillnow 18500 is 0.004401524108147788
testing: bce: 20.662429, dice: 56.017575, loss: 38.340002
IoU in current test batch is 0.8824967494854081
Epoch 18534: reducing learning rate of group 0 to 8.4022e-04.
training: bce: 0.028959, dice: 0.078584, loss: 0.053771
training IoU in current batch 18600 is 0.8914220575662581
training IoU uptillnow 18600 is 0.00440182294250983
testing: bce: 20.717925, dice: 56.220754, loss: 38.469339
IoU in current test batch is 0.9040627861514493
Epoch 18635: reducing learning rate of group 0 to 8.3938e-04.
training: bce: 0.028902, dice: 0.078486, loss: 0.053694
training IoU in current batch 18700 is 0.8348070025922375
training IoU uptillnow 18700 is 0.004400604890375994
testing: bce: 20.788278, dice: 56.452546, loss: 38.620412
IoU in current test batch is 0.9093118581522024
Epoch 18736: reducing learning rate of group 0 to 8.3854e-04.
training: bce: 0.028894, dice: 0.078439, loss: 0.053667
training IoU in current batch 18800 is 0.9411975048670984
training IoU uptillnow 18800 is 0.0044022291796901765
testing: bce: 20.893820, dice: 56.720477, loss: 38.807149
IoU in current test batch is 0.8726150486738379
Epoch 18837: reducing learning rate of group 0 to 8.3771e-04.
training: bce: 0.028873, dice: 0.078328, loss: 0.053600
training IoU in current batch 18900 is 0.9387351054078826
training IoU uptillnow 18900 is 0.004403771142270724
testing: bce: 20.989303, dice: 56.941357, loss: 38.965330
IoU in current test batch is 0.9000440184984241
Epoch 18938: reducing learning rate of group 0 to 8.3687e-04.
training: bce: 0.028800, dice: 0.078273, loss: 0.053537
training IoU in current batch 19000 is 0.8953507656586877
training IoU uptillnow 19000 is 0.004404155241455097
testing: bce: 21.047251, dice: 57.202715, loss: 39.124983
IoU in current test batch is 0.8649105049500361
Epoch 19039: reducing learning rate of group 0 to 8.3603e-04.
training: bce: 0.028726, dice: 0.078163, loss: 0.053444
training IoU in current batch 19100 is 0.8261321244746659
training IoU uptillnow 19100 is 0.004402723407419802
testing: bce: 21.103601, dice: 57.422599, loss: 39.263100
IoU in current test batch is 0.8635356546474087
Epoch 19140: reducing learning rate of group 0 to 8.3519e-04.
training: bce: 0.028669, dice: 0.078080, loss: 0.053374
training IoU in current batch 19200 is 0.9352675006654245
training IoU uptillnow 19200 is 0.004404148406617277
testing: bce: 21.171760, dice: 57.661786, loss: 39.416773
IoU in current test batch is 0.8436724173532806
Epoch 19241: reducing learning rate of group 0 to 8.3436e-04.
training: bce: 0.028640, dice: 0.078070, loss: 0.053355
training IoU in current batch 19300 is 0.9489169603611453
training IoU uptillnow 19300 is 0.004405912234373292
testing: bce: 21.260805, dice: 57.955161, loss: 39.607983
IoU in current test batch is 0.9053251623784024
Epoch 19342: reducing learning rate of group 0 to 8.3353e-04.
training: bce: 0.028586, dice: 0.077964, loss: 0.053275
training IoU in current batch 19400 is 0.9367790669978563
training IoU uptillnow 19400 is 0.0044073450630966364
testing: bce: 21.330713, dice: 58.176391, loss: 39.753552
IoU in current test batch is 0.9053984321409559
Epoch 19443: reducing learning rate of group 0 to 8.3269e-04.
training: bce: 0.028519, dice: 0.077897, loss: 0.053208
training IoU in current batch 19500 is 0.9180603526631521
training IoU uptillnow 19500 is 0.004408283254472562
testing: bce: 21.390344, dice: 58.425813, loss: 39.908079
IoU in current test batch is 0.9010064366073546
Epoch 19544: reducing learning rate of group 0 to 8.3186e-04.
training: bce: 0.028463, dice: 0.077835, loss: 0.053149
training IoU in current batch 19600 is 0.93402391029925
training IoU uptillnow 19600 is 0.004409619085792513
testing: bce: 21.457723, dice: 58.678982, loss: 40.068353
IoU in current test batch is 0.9005186454313867
Epoch 19645: reducing learning rate of group 0 to 8.3103e-04.
training: bce: 0.028387, dice: 0.077697, loss: 0.053042
training IoU in current batch 19700 is 0.9165969316596931
training IoU uptillnow 19700 is 0.004410499069410126
testing: bce: 21.509773, dice: 58.873260, loss: 40.191517
IoU in current test batch is 0.8811748807352294
Epoch 19746: reducing learning rate of group 0 to 8.3020e-04.
training: bce: 0.028348, dice: 0.077634, loss: 0.052991
training IoU in current batch 19800 is 0.9241372421715027
training IoU uptillnow 19800 is 0.004411560567018567
testing: bce: 21.589322, dice: 59.124004, loss: 40.356663
IoU in current test batch is 0.8818220478912033
Epoch 19847: reducing learning rate of group 0 to 8.2937e-04.
training: bce: 0.028299, dice: 0.077560, loss: 0.052929
training IoU in current batch 19900 is 0.9219412724306688
training IoU uptillnow 19900 is 0.0044125562244987674
testing: bce: 21.660650, dice: 59.366061, loss: 40.513356
IoU in current test batch is 0.8947610818135404
Epoch 19948: reducing learning rate of group 0 to 8.2854e-04.
training: bce: 0.028302, dice: 0.077576, loss: 0.052939
training IoU in current batch 20000 is 0.9336409395973154
training IoU uptillnow 20000 is 0.004413834402957284
testing: bce: 21.772052, dice: 59.677140, loss: 40.724596
IoU in current test batch is 0.8725371431810207
Epoch 20049: reducing learning rate of group 0 to 8.2771e-04.
training: bce: 0.028232, dice: 0.077453, loss: 0.052843
training IoU in current batch 20100 is 0.9451643704815168
training IoU uptillnow 20100 is 0.004415386502103845
testing: bce: 21.826680, dice: 59.880447, loss: 40.853563
IoU in current test batch is 0.9137215103924731
Epoch 20150: reducing learning rate of group 0 to 8.2688e-04.
training: bce: 0.028161, dice: 0.077346, loss: 0.052754
training IoU in current batch 20200 is 0.949260391703165
training IoU uptillnow 20200 is 0.0044170246163378534
testing: bce: 21.880182, dice: 60.094807, loss: 40.987494
IoU in current test batch is 0.909751289277352
Epoch 20251: reducing learning rate of group 0 to 8.2605e-04.
training: bce: 0.028098, dice: 0.077254, loss: 0.052676
training IoU in current batch 20300 is 0.9141542476478264
training IoU uptillnow 20300 is 0.004417781951552381
testing: bce: 21.939337, dice: 60.320647, loss: 41.129992
IoU in current test batch is 0.8971043790604477
Epoch 20352: reducing learning rate of group 0 to 8.2523e-04.
training: bce: 0.028041, dice: 0.077184, loss: 0.052612
training IoU in current batch 20400 is 0.9404319654427645
training IoU uptillnow 20400 is 0.004419175892416366
testing: bce: 22.002101, dice: 60.562970, loss: 41.282536
IoU in current test batch is 0.888584174283422
Epoch 20453: reducing learning rate of group 0 to 8.2440e-04.
training: bce: 0.027984, dice: 0.077120, loss: 0.052552
training IoU in current batch 20500 is 0.9572516769621404
training IoU uptillnow 20500 is 0.004420966451376389
testing: bce: 22.065515, dice: 60.809022, loss: 41.437269
IoU in current test batch is 0.9069337772678204
Epoch 20554: reducing learning rate of group 0 to 8.2358e-04.
training: bce: 0.027923, dice: 0.077033, loss: 0.052478
training IoU in current batch 20600 is 0.8939748018695387
training IoU uptillnow 20600 is 0.00442120385518189
testing: bce: 22.124626, dice: 61.037002, loss: 41.580814
IoU in current test batch is 0.884496106739237
Epoch 20655: reducing learning rate of group 0 to 8.2275e-04.
training: bce: 0.027854, dice: 0.076913, loss: 0.052384
training IoU in current batch 20700 is 0.9175690211545356
training IoU uptillnow 20700 is 0.004422008846489512
testing: bce: 22.177306, dice: 61.237429, loss: 41.707368
IoU in current test batch is 0.8808434304334961
Epoch 20756: reducing learning rate of group 0 to 8.2193e-04.
training: bce: 0.027808, dice: 0.076842, loss: 0.052325
training IoU in current batch 20800 is 0.9005556720486475
training IoU uptillnow 20800 is 0.0044223971427913905
testing: bce: 22.247637, dice: 61.476446, loss: 41.862042
IoU in current test batch is 0.8949217324433787
Epoch 20857: reducing learning rate of group 0 to 8.2111e-04.
training: bce: 0.027817, dice: 0.076827, loss: 0.052322
training IoU in current batch 20900 is 0.8729633133606644
training IoU uptillnow 20900 is 0.004422121650824556
testing: bce: 22.361272, dice: 61.760437, loss: 42.060854
IoU in current test batch is 0.8971425585563181
Epoch 20958: reducing learning rate of group 0 to 8.2029e-04.
training: bce: 0.027806, dice: 0.076785, loss: 0.052295
training IoU in current batch 21000 is 0.8537718087109326
training IoU uptillnow 21000 is 0.004421391863636947
testing: bce: 22.459715, dice: 62.021547, loss: 42.240631
IoU in current test batch is 0.9089114846639178
Epoch 21059: reducing learning rate of group 0 to 8.1947e-04.
training: bce: 0.027754, dice: 0.076691, loss: 0.052222
training IoU in current batch 21100 is 0.9013108707571151
training IoU uptillnow 21100 is 0.004421795458206629
testing: bce: 22.524367, dice: 62.240332, loss: 42.382350
IoU in current test batch is 0.8872798557006795
Epoch 21160: reducing learning rate of group 0 to 8.1865e-04.
training: bce: 0.027734, dice: 0.076640, loss: 0.052187
training IoU in current batch 21200 is 0.8975637718543995
training IoU uptillnow 21200 is 0.004422106874654275
testing: bce: 22.615173, dice: 62.494164, loss: 42.554669
IoU in current test batch is 0.8852658909993812
Epoch 21261: reducing learning rate of group 0 to 8.1783e-04.
training: bce: 0.027673, dice: 0.076552, loss: 0.052112
training IoU in current batch 21300 is 0.9461186845192346
training IoU uptillnow 21300 is 0.0044235551003147695
testing: bce: 22.671814, dice: 62.716417, loss: 42.694115
IoU in current test batch is 0.870914050682818
Epoch 21362: reducing learning rate of group 0 to 8.1701e-04.
training: bce: 0.027654, dice: 0.076503, loss: 0.052078
training IoU in current batch 21400 is 0.9280278875993708
training IoU uptillnow 21400 is 0.004424567129368001
testing: bce: 22.762025, dice: 62.970641, loss: 42.866333
IoU in current test batch is 0.8957862252201987
Epoch 21463: reducing learning rate of group 0 to 8.1620e-04.
training: bce: 0.027597, dice: 0.076440, loss: 0.052019
training IoU in current batch 21500 is 0.9007095954246982
training IoU uptillnow 21500 is 0.004424934465062878
testing: bce: 22.822029, dice: 63.212832, loss: 43.017430
IoU in current test batch is 0.8933245712411437
Epoch 21564: reducing learning rate of group 0 to 8.1538e-04.
training: bce: 0.027574, dice: 0.076396, loss: 0.051985
training IoU in current batch 21600 is 0.9344744303265777
training IoU uptillnow 21600 is 0.004426079956876081
testing: bce: 22.908466, dice: 63.470415, loss: 43.189441
IoU in current test batch is 0.8788914297301577
Epoch 21665: reducing learning rate of group 0 to 8.1456e-04.
training: bce: 0.027514, dice: 0.076304, loss: 0.051909
training IoU in current batch 21700 is 0.9535964561925856
training IoU uptillnow 21700 is 0.004427655471018687
testing: bce: 22.964465, dice: 63.687139, loss: 43.325802
IoU in current test batch is 0.8984194680741935
Epoch 21766: reducing learning rate of group 0 to 8.1375e-04.
training: bce: 0.027448, dice: 0.076203, loss: 0.051826
training IoU in current batch 21800 is 0.9063659233360337
training IoU uptillnow 21800 is 0.0044281333121528625
testing: bce: 23.015559, dice: 63.895834, loss: 43.455697
IoU in current test batch is 0.8726994105855813
Epoch 21867: reducing learning rate of group 0 to 8.1294e-04.
training: bce: 0.027394, dice: 0.076112, loss: 0.051753
training IoU in current batch 21900 is 0.8784233576642336
training IoU uptillnow 21900 is 0.0044279688606491336
testing: bce: 23.075621, dice: 64.112391, loss: 43.594006
IoU in current test batch is 0.9088554445197417
Epoch 21968: reducing learning rate of group 0 to 8.1212e-04.
training: bce: 0.027336, dice: 0.076007, loss: 0.051672
training IoU in current batch 22000 is 0.8685208596713021
training IoU uptillnow 22000 is 0.004427580857547944
testing: bce: 23.131283, dice: 64.316917, loss: 43.724100
IoU in current test batch is 0.8879099461761958
Epoch 22069: reducing learning rate of group 0 to 8.1131e-04.
training: bce: 0.027297, dice: 0.075949, loss: 0.051623
training IoU in current batch 22100 is 0.9214099834397919
training IoU uptillnow 22100 is 0.004428392897997023
testing: bce: 23.203735, dice: 64.559553, loss: 43.881644
IoU in current test batch is 0.8813735918373093
Epoch 22170: reducing learning rate of group 0 to 8.1050e-04.
training: bce: 0.027236, dice: 0.075847, loss: 0.051541
training IoU in current batch 22200 is 0.9076859332918997
training IoU uptillnow 22200 is 0.004428888536790152
testing: bce: 23.256157, dice: 64.764160, loss: 44.010159
IoU in current test batch is 0.8908925365960928
Epoch 22271: reducing learning rate of group 0 to 8.0969e-04.
training: bce: 0.027184, dice: 0.075772, loss: 0.051478
training IoU in current batch 22300 is 0.9463389989534842
training IoU uptillnow 22300 is 0.004430246352394731
testing: bce: 23.316654, dice: 64.991819, loss: 44.154237
IoU in current test batch is 0.8974835752672257
Epoch 22372: reducing learning rate of group 0 to 8.0888e-04.
training: bce: 0.027166, dice: 0.075778, loss: 0.051472
training IoU in current batch 22400 is 0.9003751058766587
training IoU uptillnow 22400 is 0.00443056611123134
testing: bce: 23.405321, dice: 65.288460, loss: 44.346890
IoU in current test batch is 0.7501638127246548
Epoch 22473: reducing learning rate of group 0 to 8.0807e-04.
training: bce: 0.027137, dice: 0.075744, loss: 0.051441
training IoU in current batch 22500 is 0.9200414983882322
training IoU uptillnow 22500 is 0.004431320039415464
testing: bce: 23.485297, dice: 65.550788, loss: 44.518042
IoU in current test batch is 0.9032701044961916
Epoch 22574: reducing learning rate of group 0 to 8.0726e-04.
training: bce: 0.027106, dice: 0.075690, loss: 0.051398
training IoU in current batch 22600 is 0.917598528900154
training IoU uptillnow 22600 is 0.0044320132503578346
testing: bce: 23.562137, dice: 65.794937, loss: 44.678537
IoU in current test batch is 0.9093996055345345
Epoch 22675: reducing learning rate of group 0 to 8.0645e-04.
training: bce: 0.027043, dice: 0.075588, loss: 0.051315
training IoU in current batch 22700 is 0.9496991978609626
training IoU uptillnow 22700 is 0.004433407386030038
testing: bce: 23.611529, dice: 65.996996, loss: 44.804263
IoU in current test batch is 0.8701959145547052
Epoch 22776: reducing learning rate of group 0 to 8.0565e-04.
training: bce: 0.027000, dice: 0.075509, loss: 0.051255
training IoU in current batch 22800 is 0.9142757629848927
training IoU uptillnow 22800 is 0.004434012497336097
testing: bce: 23.678221, dice: 66.218309, loss: 44.948265
IoU in current test batch is 0.8853753974729061
Epoch 22877: reducing learning rate of group 0 to 8.0484e-04.
training: bce: 0.027029, dice: 0.075494, loss: 0.051262
training IoU in current batch 22900 is 0.8323323633211566
training IoU uptillnow 22900 is 0.004432823244985848
testing: bce: 23.807692, dice: 66.495655, loss: 45.151673
IoU in current test batch is 0.8807454432010974
Epoch 22978: reducing learning rate of group 0 to 8.0404e-04.
training: bce: 0.026977, dice: 0.075428, loss: 0.051203
training IoU in current batch 23000 is 0.8957563092750361
training IoU uptillnow 23000 is 0.00443302305500015
testing: bce: 23.865224, dice: 66.727922, loss: 45.296573
IoU in current test batch is 0.8972230895376084
Epoch 23079: reducing learning rate of group 0 to 8.0323e-04.
training: bce: 0.026994, dice: 0.075360, loss: 0.051177
training IoU in current batch 23100 is 0.9321462472586822
training IoU uptillnow 23100 is 0.004434008762031418
testing: bce: 23.984018, dice: 66.957028, loss: 45.470523
IoU in current test batch is 0.893930961464735
Epoch 23180: reducing learning rate of group 0 to 8.0243e-04.
training: bce: 0.026950, dice: 0.075303, loss: 0.051127
training IoU in current batch 23200 is 0.8554965745655824
training IoU uptillnow 23200 is 0.004433334110554311
testing: bce: 24.049022, dice: 67.196690, loss: 45.622856
IoU in current test batch is 0.885950747893889
Epoch 23281: reducing learning rate of group 0 to 8.0163e-04.
training: bce: 0.026895, dice: 0.075218, loss: 0.051057
training IoU in current batch 23300 is 0.9315628192032687
training IoU uptillnow 23300 is 0.004434297502621013
testing: bce: 24.103046, dice: 67.409934, loss: 45.756490
IoU in current test batch is 0.8991427924551723
Epoch 23382: reducing learning rate of group 0 to 8.0083e-04.
training: bce: 0.026867, dice: 0.075174, loss: 0.051021
training IoU in current batch 23400 is 0.8850998294923271
training IoU uptillnow 23400 is 0.0044342599044194
testing: bce: 24.181396, dice: 67.659878, loss: 45.920637
IoU in current test batch is 0.9109860909874927
Epoch 23483: reducing learning rate of group 0 to 8.0003e-04.
training: bce: 0.026818, dice: 0.075116, loss: 0.050967
training IoU in current batch 23500 is 0.8825259098577971
training IoU uptillnow 23500 is 0.004434167864271617
testing: bce: 24.240702, dice: 67.895994, loss: 46.068348
IoU in current test batch is 0.8860222594594528
Epoch 23584: reducing learning rate of group 0 to 7.9923e-04.
training: bce: 0.026770, dice: 0.075022, loss: 0.050896
training IoU in current batch 23600 is 0.9228488000729994
training IoU uptillnow 23600 is 0.00443493086641599
testing: bce: 24.299783, dice: 68.099499, loss: 46.199641
IoU in current test batch is 0.8809158576080294
Epoch 23685: reducing learning rate of group 0 to 7.9843e-04.
training: bce: 0.026722, dice: 0.074966, loss: 0.050844
training IoU in current batch 23700 is 0.9468856288865222
training IoU uptillnow 23700 is 0.004436194514692504
testing: bce: 24.359193, dice: 68.337585, loss: 46.348389
IoU in current test batch is 0.9125278253078964
Epoch 23786: reducing learning rate of group 0 to 7.9763e-04.
training: bce: 0.026700, dice: 0.074913, loss: 0.050807
training IoU in current batch 23800 is 0.922223638913681
training IoU uptillnow 23800 is 0.004436929457257421
testing: bce: 24.441565, dice: 68.577466, loss: 46.509515
IoU in current test batch is 0.9074015752612368
Epoch 23887: reducing learning rate of group 0 to 7.9683e-04.
training: bce: 0.026647, dice: 0.074842, loss: 0.050744
training IoU in current batch 23900 is 0.9213066072754269
training IoU uptillnow 23900 is 0.004437639065973039
testing: bce: 24.495801, dice: 68.799564, loss: 46.647683
IoU in current test batch is 0.9037994401734495
Epoch 23988: reducing learning rate of group 0 to 7.9603e-04.
training: bce: 0.026639, dice: 0.074813, loss: 0.050726
training IoU in current batch 24000 is 0.35297323656808466
training IoU uptillnow 24000 is 0.004426502976297056
testing: bce: 24.591196, dice: 69.061122, loss: 46.826159
IoU in current test batch is 0.8686070159948698
Epoch 24089: reducing learning rate of group 0 to 7.9524e-04.
training: bce: 0.026588, dice: 0.074738, loss: 0.050663
training IoU in current batch 24100 is 0.7997031067755275
training IoU uptillnow 24100 is 0.004424727168478213
testing: bce: 24.645756, dice: 69.279564, loss: 46.962660
IoU in current test batch is 0.9030149586719786
Epoch 24190: reducing learning rate of group 0 to 7.9444e-04.
training: bce: 0.026548, dice: 0.074685, loss: 0.050616
training IoU in current batch 24200 is 0.9184686974543463
training IoU uptillnow 24200 is 0.004425419769274848
testing: bce: 24.710745, dice: 69.517439, loss: 47.114092
IoU in current test batch is 0.9025274102188583
Epoch 24291: reducing learning rate of group 0 to 7.9365e-04.
training: bce: 0.026503, dice: 0.074612, loss: 0.050558
training IoU in current batch 24300 is 0.9018288752819852
training IoU uptillnow 24300 is 0.00442576430080497
testing: bce: 24.771577, dice: 69.735953, loss: 47.253765
IoU in current test batch is 0.9057447493645893
Epoch 24392: reducing learning rate of group 0 to 7.9285e-04.
training: bce: 0.026520, dice: 0.074624, loss: 0.050572
training IoU in current batch 24400 is 0.9226195496331281
training IoU uptillnow 24400 is 0.004426532029370852
testing: bce: 24.888621, dice: 70.034990, loss: 47.461805
IoU in current test batch is 0.8868297569465402
Epoch 24493: reducing learning rate of group 0 to 7.9206e-04.
training: bce: 0.026473, dice: 0.074557, loss: 0.050515
training IoU in current batch 24500 is 0.961804355340477
training IoU uptillnow 24500 is 0.004428093148293882
testing: bce: 24.946318, dice: 70.258162, loss: 47.602240
IoU in current test batch is 0.8759596243567767
Epoch 24594: reducing learning rate of group 0 to 7.9127e-04.
training: bce: 0.026431, dice: 0.074489, loss: 0.050460
training IoU in current batch 24600 is 0.9077960665943499
training IoU uptillnow 24600 is 0.004428543890884337
testing: bce: 25.009276, dice: 70.480851, loss: 47.745063
IoU in current test batch is 0.899722185319274
Epoch 24695: reducing learning rate of group 0 to 7.9048e-04.
training: bce: 0.026401, dice: 0.074459, loss: 0.050430
training IoU in current batch 24700 is 0.9356677524429967
training IoU uptillnow 24700 is 0.004429555165210602
testing: bce: 25.082187, dice: 70.739175, loss: 47.910681
IoU in current test batch is 0.9079603244181658
Epoch 24796: reducing learning rate of group 0 to 7.8969e-04.
training: bce: 0.026349, dice: 0.074391, loss: 0.050370
training IoU in current batch 24800 is 0.8117774360726693
training IoU uptillnow 24800 is 0.0044280605965043105
testing: bce: 25.133929, dice: 70.960806, loss: 48.047367
IoU in current test batch is 0.8986126439586738
Epoch 24897: reducing learning rate of group 0 to 7.8890e-04.
training: bce: 0.026304, dice: 0.074321, loss: 0.050312
training IoU in current batch 24900 is 0.9602353018502087
training IoU uptillnow 24900 is 0.004429558993808623
testing: bce: 25.191709, dice: 71.179753, loss: 48.185731
IoU in current test batch is 0.8529737551493977
Epoch 24998: reducing learning rate of group 0 to 7.8811e-04.
training: bce: 0.026257, dice: 0.074257, loss: 0.050257
training IoU in current batch 25000 is 0.9300759747054275
training IoU uptillnow 25000 is 0.004430442241997569
testing: bce: 25.247713, dice: 71.403444, loss: 48.325578
IoU in current test batch is 0.9106655759908955
Epoch 25099: reducing learning rate of group 0 to 7.8732e-04.
training: bce: 0.026212, dice: 0.074188, loss: 0.050200
training IoU in current batch 25100 is 0.8482046467965266
training IoU uptillnow 25100 is 0.004429687614659954
testing: bce: 25.306105, dice: 71.622853, loss: 48.464479
IoU in current test batch is 0.9114867003344733
Epoch 25200: reducing learning rate of group 0 to 7.8653e-04.
training: bce: 0.026166, dice: 0.074114, loss: 0.050140
training IoU in current batch 25200 is 0.8936342530100967
training IoU uptillnow 25200 is 0.0044298403214985335
testing: bce: 25.361484, dice: 71.836328, loss: 48.598906
IoU in current test batch is 0.8906244886811584
training: bce: 0.026122, dice: 0.074041, loss: 0.050082
training IoU in current batch 25300 is 0.901549463647199
training IoU uptillnow 25300 is 0.004430148242121187
testing: bce: 25.419823, dice: 72.050836, loss: 48.735330
IoU in current test batch is 0.8698018047462531
Epoch 25301: reducing learning rate of group 0 to 7.8575e-04.
training: bce: 0.026078, dice: 0.073969, loss: 0.050023
training IoU in current batch 25400 is 0.9158387534843828
training IoU uptillnow 25400 is 0.00443073501242669
testing: bce: 25.477134, dice: 72.264772, loss: 48.870953
IoU in current test batch is 0.8876389854377589
Epoch 25402: reducing learning rate of group 0 to 7.8496e-04.
training: bce: 0.026038, dice: 0.073906, loss: 0.049972
training IoU in current batch 25500 is 0.7627798778714743
training IoU uptillnow 25500 is 0.00442831614405655
testing: bce: 25.537835, dice: 72.487530, loss: 49.012683
IoU in current test batch is 0.9016188928021294
Epoch 25503: reducing learning rate of group 0 to 7.8418e-04.
training: bce: 0.025995, dice: 0.073847, loss: 0.049921
training IoU in current batch 25600 is 0.8853964329860498
training IoU uptillnow 25600 is 0.00442831093340413
testing: bce: 25.595873, dice: 72.713436, loss: 49.154654
IoU in current test batch is 0.8876615363336924
Epoch 25604: reducing learning rate of group 0 to 7.8339e-04.
training: bce: 0.025981, dice: 0.073812, loss: 0.049897
training IoU in current batch 25700 is 0.9018846209178536
training IoU uptillnow 25700 is 0.004428626532685033
testing: bce: 25.682549, dice: 72.963472, loss: 49.323010
IoU in current test batch is 0.9097136271692925
Epoch 25705: reducing learning rate of group 0 to 7.8261e-04.
training: bce: 0.025931, dice: 0.073731, loss: 0.049831
training IoU in current batch 25800 is 0.8819020725127326
training IoU uptillnow 25800 is 0.004428552441874129
testing: bce: 25.732644, dice: 73.167160, loss: 49.449902
IoU in current test batch is 0.8878762888195693
Epoch 25806: reducing learning rate of group 0 to 7.8183e-04.
training: bce: 0.025882, dice: 0.073641, loss: 0.049762
training IoU in current batch 25900 is 0.8832123537842711
training IoU uptillnow 25900 is 0.004428504217199589
testing: bce: 25.783867, dice: 73.360995, loss: 49.572431
IoU in current test batch is 0.878700858464937
Epoch 25907: reducing learning rate of group 0 to 7.8104e-04.
training: bce: 0.025848, dice: 0.073577, loss: 0.049712
training IoU in current batch 26000 is 0.9365503080082136
training IoU uptillnow 26000 is 0.004429482053909106
testing: bce: 25.849388, dice: 73.579423, loss: 49.714406
IoU in current test batch is 0.8890358618948146
Epoch 26008: reducing learning rate of group 0 to 7.8026e-04.
training: bce: 0.025802, dice: 0.073506, loss: 0.049654
training IoU in current batch 26100 is 0.9432362153537025
training IoU uptillnow 26100 is 0.004430580475513103
testing: bce: 25.902381, dice: 73.791134, loss: 49.846757
IoU in current test batch is 0.8507864547296984
Epoch 26109: reducing learning rate of group 0 to 7.7948e-04.
training: bce: 0.025765, dice: 0.073447, loss: 0.049606
training IoU in current batch 26200 is 0.9240006383148488
training IoU uptillnow 26200 is 0.004431303435385097
testing: bce: 25.964316, dice: 74.014364, loss: 49.989340
IoU in current test batch is 0.8743172073168822
Epoch 26210: reducing learning rate of group 0 to 7.7870e-04.
training: bce: 0.025732, dice: 0.073370, loss: 0.049551
training IoU in current batch 26300 is 0.9629000073904368
training IoU uptillnow 26300 is 0.004432760401285888
testing: bce: 26.029580, dice: 74.219747, loss: 50.124663
IoU in current test batch is 0.8968715661002714
Epoch 26311: reducing learning rate of group 0 to 7.7792e-04.
training: bce: 0.025694, dice: 0.073294, loss: 0.049494
training IoU in current batch 26400 is 0.9177822177822178
training IoU uptillnow 26400 is 0.004433351858759564
testing: bce: 26.090019, dice: 74.424449, loss: 50.257234
IoU in current test batch is 0.8882152369698371
Epoch 26412: reducing learning rate of group 0 to 7.7715e-04.
training: bce: 0.025649, dice: 0.073217, loss: 0.049433
training IoU in current batch 26500 is 0.944394532503369
training IoU uptillnow 26500 is 0.004434440952770195
testing: bce: 26.143135, dice: 74.628113, loss: 50.385624
IoU in current test batch is 0.8740668365458342
Epoch 26513: reducing learning rate of group 0 to 7.7637e-04.
training: bce: 0.025604, dice: 0.073136, loss: 0.049370
training IoU in current batch 26600 is 0.9203412231166647
training IoU uptillnow 26600 is 0.004435069745532922
testing: bce: 26.195940, dice: 74.826262, loss: 50.511101
IoU in current test batch is 0.898990997212615
Epoch 26614: reducing learning rate of group 0 to 7.7559e-04.
training: bce: 0.025565, dice: 0.073056, loss: 0.049311
training IoU in current batch 26700 is 0.957323568575233
training IoU uptillnow 26700 is 0.004436386355762288
testing: bce: 26.254725, dice: 75.025842, loss: 50.640284
IoU in current test batch is 0.8881796531876509
Epoch 26715: reducing learning rate of group 0 to 7.7482e-04.
training: bce: 0.025578, dice: 0.073003, loss: 0.049290
training IoU in current batch 26800 is 0.9324974411463665
training IoU uptillnow 26800 is 0.004437229984171562
testing: bce: 26.365532, dice: 75.252373, loss: 50.808953
IoU in current test batch is 0.9018419129746734
Epoch 26816: reducing learning rate of group 0 to 7.7404e-04.
training: bce: 0.025544, dice: 0.072954, loss: 0.049249
training IoU in current batch 26900 is 0.9293093321355769
training IoU uptillnow 26900 is 0.004438008084154858
testing: bce: 26.429228, dice: 75.482133, loss: 50.955680
IoU in current test batch is 0.9186180576448594
Epoch 26917: reducing learning rate of group 0 to 7.7327e-04.
training: bce: 0.025570, dice: 0.072947, loss: 0.049259
training IoU in current batch 27000 is 0.934507406131588
training IoU uptillnow 27000 is 0.004438876677712515
testing: bce: 26.554684, dice: 75.755702, loss: 51.155193
IoU in current test batch is 0.8802576192000038
Epoch 27018: reducing learning rate of group 0 to 7.7250e-04.
training: bce: 0.025548, dice: 0.072904, loss: 0.049226
training IoU in current batch 27100 is 0.8940089248979334
training IoU uptillnow 27100 is 0.0044389916843424445
testing: bce: 26.629617, dice: 75.991617, loss: 51.310617
IoU in current test batch is 0.9026306744262652
Epoch 27119: reducing learning rate of group 0 to 7.7172e-04.
training: bce: 0.025508, dice: 0.072842, loss: 0.049175
training IoU in current batch 27200 is 0.9196074106749007
training IoU uptillnow 27200 is 0.004439576388467411
testing: bce: 26.686258, dice: 76.207075, loss: 51.446666
IoU in current test batch is 0.8979948067982595
Epoch 27220: reducing learning rate of group 0 to 7.7095e-04.
training: bce: 0.025470, dice: 0.072795, loss: 0.049132
training IoU in current batch 27300 is 0.9534951174707532
training IoU uptillnow 27300 is 0.0044407774404394505
testing: bce: 26.744306, dice: 76.437124, loss: 51.590715
IoU in current test batch is 0.8851506987075095
Epoch 27321: reducing learning rate of group 0 to 7.7018e-04.
training: bce: 0.025457, dice: 0.072746, loss: 0.049102
training IoU in current batch 27400 is 0.9370699855383672
training IoU uptillnow 27400 is 0.004441670008182424
testing: bce: 26.828918, dice: 76.666384, loss: 51.747651
IoU in current test batch is 0.9096342047129473
Epoch 27422: reducing learning rate of group 0 to 7.6941e-04.
training: bce: 0.025418, dice: 0.072693, loss: 0.049055
training IoU in current batch 27500 is 0.9109622250628113
training IoU uptillnow 27500 is 0.004442081415466274
testing: bce: 26.884988, dice: 76.889447, loss: 51.887217
IoU in current test batch is 0.9016706730233424
Epoch 27523: reducing learning rate of group 0 to 7.6864e-04.
training: bce: 0.025407, dice: 0.072664, loss: 0.049035
training IoU in current batch 27600 is 0.9111080229757272
training IoU uptillnow 27600 is 0.004442492482816777
testing: bce: 26.971646, dice: 77.137966, loss: 52.054806
IoU in current test batch is 0.895356773251552
Epoch 27680: reducing learning rate of group 0 to 7.6787e-04.
training: bce: 0.025387, dice: 0.072638, loss: 0.049012
training IoU in current batch 27700 is 0.9230769230769231
training IoU uptillnow 27700 is 0.004443116619608113
testing: bce: 27.047682, dice: 77.390145, loss: 52.218914
IoU in current test batch is 0.8817118359442634
Epoch 27781: reducing learning rate of group 0 to 7.6710e-04.
training: bce: 0.025351, dice: 0.072581, loss: 0.048966
training IoU in current batch 27800 is 0.9516457782210851
training IoU uptillnow 27800 is 0.00444425007621578
testing: bce: 27.107385, dice: 77.608605, loss: 52.357995
IoU in current test batch is 0.9053319395119254
Epoch 27882: reducing learning rate of group 0 to 7.6634e-04.
training: bce: 0.025322, dice: 0.072547, loss: 0.048935
training IoU in current batch 27900 is 0.8656513783129686
training IoU uptillnow 27900 is 0.004443834344934998
testing: bce: 27.173716, dice: 77.851015, loss: 52.512365
IoU in current test batch is 0.9024704593868953
Epoch 27983: reducing learning rate of group 0 to 7.6557e-04.
training: bce: 0.025306, dice: 0.072498, loss: 0.048902
training IoU in current batch 28000 is 0.920288081669755
training IoU uptillnow 28000 is 0.004444397203630808
testing: bce: 27.253283, dice: 78.077706, loss: 52.665494
IoU in current test batch is 0.8754328003128093
Epoch 28084: reducing learning rate of group 0 to 7.6481e-04.
training: bce: 0.025268, dice: 0.072430, loss: 0.048849
training IoU in current batch 28100 is 0.9461715412421952
training IoU uptillnow 28100 is 0.00444541659974689
testing: bce: 27.309503, dice: 78.282646, loss: 52.796074
IoU in current test batch is 0.8970775059468193
Epoch 28185: reducing learning rate of group 0 to 7.6404e-04.
training: bce: 0.025251, dice: 0.072386, loss: 0.048818
training IoU in current batch 28200 is 0.9005350351318249
training IoU uptillnow 28200 is 0.004445619637142416
testing: bce: 27.388289, dice: 78.513471, loss: 52.950880
IoU in current test batch is 0.9085458208139797
Epoch 28286: reducing learning rate of group 0 to 7.6328e-04.
training: bce: 0.025210, dice: 0.072313, loss: 0.048761
training IoU in current batch 28300 is 0.9023014586709887
training IoU uptillnow 28300 is 0.004445852447489091
testing: bce: 27.441350, dice: 78.712365, loss: 53.076857
IoU in current test batch is 0.9004187519205277
Epoch 28387: reducing learning rate of group 0 to 7.6251e-04.
training: bce: 0.025203, dice: 0.072282, loss: 0.048742
training IoU in current batch 28400 is 0.943256549394473
training IoU uptillnow 28400 is 0.0044468046333257984
testing: bce: 27.530624, dice: 78.956664, loss: 53.243644
IoU in current test batch is 0.8711700346918444
Epoch 28488: reducing learning rate of group 0 to 7.6175e-04.
training: bce: 0.025169, dice: 0.072238, loss: 0.048703
training IoU in current batch 28500 is 0.929945155393053
training IoU uptillnow 28500 is 0.004447516612356848
testing: bce: 27.589801, dice: 79.186537, loss: 53.388169
IoU in current test batch is 0.8776217742466011
Epoch 28589: reducing learning rate of group 0 to 7.6099e-04.
training: bce: 0.025145, dice: 0.072180, loss: 0.048662
training IoU in current batch 28600 is 0.9355341398400656
training IoU uptillnow 28600 is 0.004448321318789642
testing: bce: 27.660167, dice: 79.400633, loss: 53.530400
IoU in current test batch is 0.8907335296712926
Epoch 28690: reducing learning rate of group 0 to 7.6023e-04.
training: bce: 0.025149, dice: 0.072162, loss: 0.048656
training IoU in current batch 28700 is 0.938337801608579
training IoU uptillnow 28700 is 0.004449169260287336
testing: bce: 27.762139, dice: 79.658590, loss: 53.710364
IoU in current test batch is 0.8939186930002154
Epoch 28791: reducing learning rate of group 0 to 7.5947e-04.
training: bce: 0.025148, dice: 0.072140, loss: 0.048644
training IoU in current batch 28800 is 0.9008229238385627
training IoU uptillnow 28800 is 0.0044493600361593735
testing: bce: 27.857362, dice: 79.911254, loss: 53.884308
IoU in current test batch is 0.8713190786088938
Epoch 28892: reducing learning rate of group 0 to 7.5871e-04.
training: bce: 0.025113, dice: 0.072094, loss: 0.048603
training IoU in current batch 28900 is 0.9062303942859901
training IoU uptillnow 28900 is 0.004449643043443795
testing: bce: 27.915084, dice: 80.137602, loss: 54.026343
IoU in current test batch is 0.8862931400966206
Epoch 28993: reducing learning rate of group 0 to 7.5795e-04.
training: bce: 0.025074, dice: 0.072031, loss: 0.048552
training IoU in current batch 29000 is 0.9395103216514642
training IoU uptillnow 29000 is 0.004450497871087027
testing: bce: 27.967808, dice: 80.345016, loss: 54.156412
IoU in current test batch is 0.882588071688329
Epoch 29094: reducing learning rate of group 0 to 7.5719e-04.
training: bce: 0.025051, dice: 0.071998, loss: 0.048525
training IoU in current batch 29100 is 0.9658882183154028
training IoU uptillnow 29100 is 0.004451800036718758
testing: bce: 28.039265, dice: 80.585387, loss: 54.312326
IoU in current test batch is 0.9015625300252679
Epoch 29195: reducing learning rate of group 0 to 7.5643e-04.
training: bce: 0.025038, dice: 0.071998, loss: 0.048518
training IoU in current batch 29200 is 0.9100029291154071
training IoU uptillnow 29200 is 0.0044521363766004685
testing: bce: 28.120752, dice: 80.861879, loss: 54.491315
IoU in current test batch is 0.8705215755958758
Epoch 29296: reducing learning rate of group 0 to 7.5568e-04.
training: bce: 0.025009, dice: 0.071941, loss: 0.048475
training IoU in current batch 29300 is 0.9420547649849073
training IoU uptillnow 29300 is 0.00445301736171471
testing: bce: 28.184552, dice: 81.074907, loss: 54.629729
IoU in current test batch is 0.8871284006877347
Epoch 29397: reducing learning rate of group 0 to 7.5492e-04.
training: bce: 0.024997, dice: 0.071911, loss: 0.048454
training IoU in current batch 29400 is 0.9319589414872883
training IoU uptillnow 29400 is 0.004453720662098104
testing: bce: 28.266575, dice: 81.317521, loss: 54.792048
IoU in current test batch is 0.8966757791875561
Epoch 29498: reducing learning rate of group 0 to 7.5417e-04.
training: bce: 0.024957, dice: 0.071840, loss: 0.048399
training IoU in current batch 29500 is 0.8845020993921163
training IoU uptillnow 29500 is 0.00445361486851437
testing: bce: 28.317973, dice: 81.513948, loss: 54.915961
IoU in current test batch is 0.8536217880550887
Epoch 29599: reducing learning rate of group 0 to 7.5341e-04.
training: bce: 0.024930, dice: 0.071812, loss: 0.048371
training IoU in current batch 29600 is 0.8598597359735973
training IoU uptillnow 29600 is 0.004453093547651405
testing: bce: 28.382755, dice: 81.757603, loss: 55.070179
IoU in current test batch is 0.9044850325694394
Epoch 29700: reducing learning rate of group 0 to 7.5266e-04.
training: bce: 0.024906, dice: 0.071783, loss: 0.048345
training IoU in current batch 29700 is 0.9179583110636024
training IoU uptillnow 29700 is 0.004453553794806944
testing: bce: 28.451387, dice: 82.001428, loss: 55.226407
IoU in current test batch is 0.9018039186750657
training: bce: 0.024880, dice: 0.071761, loss: 0.048320
training IoU in current batch 29800 is 0.9557717492984097
training IoU uptillnow 29800 is 0.004454645385531031
testing: bce: 28.517024, dice: 82.251412, loss: 55.384218
IoU in current test batch is 0.8795222889328151
Epoch 29801: reducing learning rate of group 0 to 7.5191e-04.
training: bce: 0.024840, dice: 0.071699, loss: 0.048270
training IoU in current batch 29900 is 0.9654965813773173
training IoU uptillnow 29900 is 0.004455892292060429
testing: bce: 28.567486, dice: 82.456357, loss: 55.511921
IoU in current test batch is 0.9050006426001359
Epoch 29902: reducing learning rate of group 0 to 7.5116e-04.
training: bce: 0.024810, dice: 0.071661, loss: 0.048236
training IoU in current batch 30000 is 0.9360215907286871
training IoU uptillnow 30000 is 0.004456639652687018
testing: bce: 28.628142, dice: 82.688753, loss: 55.658448
IoU in current test batch is 0.9039535086690154
Epoch 30003: reducing learning rate of group 0 to 7.5040e-04.
training: bce: 0.024797, dice: 0.071628, loss: 0.048212
training IoU in current batch 30100 is 0.9082166999904789
training IoU uptillnow 30100 is 0.004456920187709992
testing: bce: 28.708020, dice: 82.926099, loss: 55.817059
IoU in current test batch is 0.9015256170514591
Epoch 30104: reducing learning rate of group 0 to 7.4965e-04.
training: bce: 0.024769, dice: 0.071586, loss: 0.048177
training IoU in current batch 30200 is 0.9127260473997529
training IoU uptillnow 30200 is 0.004457273520544298
testing: bce: 28.770538, dice: 83.152617, loss: 55.961577
IoU in current test batch is 0.9224195269224599
Epoch 30205: reducing learning rate of group 0 to 7.4890e-04.
training: bce: 0.024736, dice: 0.071543, loss: 0.048140
training IoU in current batch 30300 is 0.900634933473531
training IoU uptillnow 30300 is 0.004457425004478239
testing: bce: 28.827903, dice: 83.377961, loss: 56.102932
IoU in current test batch is 0.9078682938280338
Epoch 30306: reducing learning rate of group 0 to 7.4816e-04.
training: bce: 0.024837, dice: 0.071571, loss: 0.048204
training IoU in current batch 30400 is 0.8825587698979843
training IoU uptillnow 30400 is 0.004457278196297626
testing: bce: 29.041018, dice: 83.685591, loss: 56.363305
IoU in current test batch is 0.862275588337124
Epoch 30407: reducing learning rate of group 0 to 7.4741e-04.
training: bce: 0.024803, dice: 0.071537, loss: 0.048170
training IoU in current batch 30500 is 0.9685826771653543
training IoU uptillnow 30500 is 0.0044585425325145665
testing: bce: 29.096412, dice: 83.920650, loss: 56.508531
IoU in current test batch is 0.9104285434960335
Epoch 30508: reducing learning rate of group 0 to 7.4666e-04.
training: bce: 0.024769, dice: 0.071474, loss: 0.048122
training IoU in current batch 30600 is 0.8415224269495626
training IoU uptillnow 30600 is 0.004457722525332556
testing: bce: 29.152228, dice: 84.122446, loss: 56.637337
IoU in current test batch is 0.9104778922293628
Epoch 30609: reducing learning rate of group 0 to 7.4591e-04.
training: bce: 0.024736, dice: 0.071437, loss: 0.048087
training IoU in current batch 30700 is 0.9084374832376764
training IoU uptillnow 30700 is 0.004457997646308602
testing: bce: 29.208909, dice: 84.352943, loss: 56.780926
IoU in current test batch is 0.8829949937641732
Epoch 30710: reducing learning rate of group 0 to 7.4517e-04.
training: bce: 0.024749, dice: 0.071437, loss: 0.048093
training IoU in current batch 30800 is 0.9510535557506584
training IoU uptillnow 30800 is 0.004458962777740843
testing: bce: 29.318555, dice: 84.628613, loss: 56.973584
IoU in current test batch is 0.892863706112814
Epoch 30811: reducing learning rate of group 0 to 7.4442e-04.
training: bce: 0.024720, dice: 0.071393, loss: 0.048056
training IoU in current batch 30900 is 0.8693299454169019
training IoU uptillnow 30900 is 0.004458599316847486
testing: bce: 29.379985, dice: 84.849999, loss: 57.114992
IoU in current test batch is 0.8977017455920012
Epoch 30912: reducing learning rate of group 0 to 7.4368e-04.
training: bce: 0.024684, dice: 0.071328, loss: 0.048006
training IoU in current batch 31000 is 0.9398368456832087
training IoU uptillnow 31000 is 0.004459375372173341
testing: bce: 29.431415, dice: 85.047342, loss: 57.239378
IoU in current test batch is 0.892975464301065
Epoch 31013: reducing learning rate of group 0 to 7.4293e-04.
training: bce: 0.024679, dice: 0.071296, loss: 0.047988
training IoU in current batch 31100 is 0.9377438808087974
training IoU uptillnow 31100 is 0.004460112789079135
testing: bce: 29.520975, dice: 85.284107, loss: 57.402541
IoU in current test batch is 0.9061543238199493
Epoch 31114: reducing learning rate of group 0 to 7.4219e-04.
training: bce: 0.024653, dice: 0.071276, loss: 0.047965
training IoU in current batch 31200 is 0.9305235289093859
training IoU uptillnow 31200 is 0.004460729772045923
testing: bce: 29.584604, dice: 85.534282, loss: 57.559443
IoU in current test batch is 0.9026987039117306
Epoch 31215: reducing learning rate of group 0 to 7.4145e-04.
training: bce: 0.024617, dice: 0.071217, loss: 0.047917
training IoU in current batch 31300 is 0.9459616502033701
training IoU uptillnow 31300 is 0.004461589420232789
testing: bce: 29.635531, dice: 85.736982, loss: 57.686256
IoU in current test batch is 0.9033957267958451
Epoch 31316: reducing learning rate of group 0 to 7.4071e-04.
training: bce: 0.024588, dice: 0.071166, loss: 0.047877
training IoU in current batch 31400 is 0.9373283799063156
training IoU uptillnow 31400 is 0.004462306125048873
testing: bce: 29.695786, dice: 85.949796, loss: 57.822791
IoU in current test batch is 0.894532326917264
Epoch 31417: reducing learning rate of group 0 to 7.3997e-04.
training: bce: 0.024560, dice: 0.071117, loss: 0.047838
training IoU in current batch 31500 is 0.8727719489839697
training IoU uptillnow 31500 is 0.004461993606779203
testing: bce: 29.755736, dice: 86.164306, loss: 57.960021
IoU in current test batch is 0.8993115613011298
Epoch 31518: reducing learning rate of group 0 to 7.3923e-04.
training: bce: 0.024550, dice: 0.071097, loss: 0.047823
training IoU in current batch 31600 is 0.9519153542619688
training IoU uptillnow 31600 is 0.0044629352958540135
testing: bce: 29.838976, dice: 86.412419, loss: 58.125698
IoU in current test batch is 0.9102025377403199
Epoch 31619: reducing learning rate of group 0 to 7.3849e-04.
training: bce: 0.024515, dice: 0.071040, loss: 0.047778
training IoU in current batch 31700 is 0.8891526661796932
training IoU uptillnow 31700 is 0.004462881127326347
testing: bce: 29.890724, dice: 86.617478, loss: 58.254101
IoU in current test batch is 0.9023201563727571
Epoch 31720: reducing learning rate of group 0 to 7.3775e-04.
training: bce: 0.024489, dice: 0.070988, loss: 0.047739
training IoU in current batch 31800 is 0.9171702284450994
training IoU uptillnow 31800 is 0.00446326781332647
testing: bce: 29.952940, dice: 86.826605, loss: 58.389772
IoU in current test batch is 0.8964092134474879
Epoch 31821: reducing learning rate of group 0 to 7.3701e-04.
training: bce: 0.024459, dice: 0.070926, loss: 0.047693
training IoU in current batch 31900 is 0.9541115746305593
training IoU uptillnow 31900 is 0.0044642310748537785
testing: bce: 30.010865, dice: 87.023698, loss: 58.517281
IoU in current test batch is 0.9023252782604307
Epoch 31922: reducing learning rate of group 0 to 7.3627e-04.
training: bce: 0.024433, dice: 0.070892, loss: 0.047662
training IoU in current batch 32000 is 0.9034390856439957
training IoU uptillnow 32000 is 0.004464396583285909
testing: bce: 30.072379, dice: 87.254114, loss: 58.663247
IoU in current test batch is 0.871441456395215
Epoch 32023: reducing learning rate of group 0 to 7.3554e-04.
training: bce: 0.024422, dice: 0.070878, loss: 0.047650
training IoU in current batch 32100 is 0.9496382565731427
training IoU uptillnow 32100 is 0.004465280651382167
testing: bce: 30.153311, dice: 87.510373, loss: 58.831842
IoU in current test batch is 0.894663440479725
Epoch 32124: reducing learning rate of group 0 to 7.3480e-04.
training: bce: 0.024406, dice: 0.070845, loss: 0.047625
training IoU in current batch 32200 is 0.945965255320749
training IoU uptillnow 32200 is 0.004466102196133018
testing: bce: 30.226961, dice: 87.741335, loss: 58.984148
IoU in current test batch is 0.878994222640097
Epoch 32225: reducing learning rate of group 0 to 7.3407e-04.
training: bce: 0.024386, dice: 0.070818, loss: 0.047602
training IoU in current batch 32300 is 0.9336843424846434
training IoU uptillnow 32300 is 0.004466728552952591
testing: bce: 30.295784, dice: 87.980425, loss: 59.138104
IoU in current test batch is 0.9097990828233828
Epoch 32326: reducing learning rate of group 0 to 7.3333e-04.
training: bce: 0.024351, dice: 0.070759, loss: 0.047555
training IoU in current batch 32400 is 0.9064583637513992
training IoU uptillnow 32400 is 0.004466930902465892
testing: bce: 30.346081, dice: 88.178994, loss: 59.262537
IoU in current test batch is 0.8964133052780487
Epoch 32427: reducing learning rate of group 0 to 7.3260e-04.
training: bce: 0.024321, dice: 0.070723, loss: 0.047522
training IoU in current batch 32500 is 0.7993456392225803
training IoU uptillnow 32500 is 0.004465484169422745
testing: bce: 30.401766, dice: 88.406431, loss: 59.404099
IoU in current test batch is 0.8941033617639108
Epoch 32528: reducing learning rate of group 0 to 7.3187e-04.
training: bce: 0.328509, dice: 0.990707, loss: 0.659608
training IoU in current batch 0 is 0.0032363874917996937
training IoU uptillnow 0 is 4.9714857245114265e-08
testing: bce: 0.012635, dice: 0.038104, loss: 0.025370
IoU in current test batch is 0.0
Epoch 32629: reducing learning rate of group 0 to 7.3114e-04.
training: bce: 0.075443, dice: 0.249696, loss: 0.162570
training IoU in current batch 100 is 0.8296154153673942
training IoU uptillnow 100 is 1.2754434261768081e-05
testing: bce: 0.293068, dice: 0.969972, loss: 0.631520
IoU in current test batch is 0.8399645091046268
Epoch 32730: reducing learning rate of group 0 to 7.3040e-04.
training: bce: 0.050518, dice: 0.171418, loss: 0.110968
training IoU in current batch 200 is 0.8225635124369302
training IoU uptillnow 200 is 2.5273902125164108e-05
testing: bce: 0.390543, dice: 1.325191, loss: 0.857867
IoU in current test batch is 0.8499238662684655
Epoch 32831: reducing learning rate of group 0 to 7.2967e-04.
training: bce: 0.041009, dice: 0.141905, loss: 0.091457
training IoU in current batch 300 is 0.8508456597324533
training IoU uptillnow 300 is 3.814762743768668e-05
testing: bce: 0.474761, dice: 1.642827, loss: 1.058794
IoU in current test batch is 0.8631924123090348
Epoch 32932: reducing learning rate of group 0 to 7.2894e-04.
training: bce: 0.036010, dice: 0.126299, loss: 0.081155
training IoU in current batch 400 is 0.9404240251405303
training IoU uptillnow 400 is 5.23025387360826e-05
testing: bce: 0.555384, dice: 1.947922, loss: 1.251653
IoU in current test batch is 0.8337637875504319
Epoch 33033: reducing learning rate of group 0 to 7.2822e-04.
training: bce: 0.034359, dice: 0.119118, loss: 0.076738
training IoU in current batch 500 is 0.9109918578830496
training IoU uptillnow 500 is 6.592651716443754e-05
testing: bce: 0.662072, dice: 2.295303, loss: 1.478688
IoU in current test batch is 0.8384024440358095
Epoch 33134: reducing learning rate of group 0 to 7.2749e-04.
training: bce: 0.031529, dice: 0.110971, loss: 0.071250
training IoU in current batch 600 is 0.9267105416877605
training IoU uptillnow 600 is 7.970538620099726e-05
testing: bce: 0.728816, dice: 2.565141, loss: 1.646979
IoU in current test batch is 0.8722405482341743
Epoch 33235: reducing learning rate of group 0 to 7.2676e-04.
training: bce: 0.030065, dice: 0.106558, loss: 0.068312
training IoU in current batch 700 is 0.8912514597119502
training IoU uptillnow 700 is 9.286814627967138e-05
testing: bce: 0.810611, dice: 2.872963, loss: 1.841787
IoU in current test batch is 0.8493833746736831
Epoch 33336: reducing learning rate of group 0 to 7.2603e-04.
training: bce: 0.028910, dice: 0.102802, loss: 0.065856
training IoU in current batch 800 is 0.9117282420034714
training IoU uptillnow 800 is 0.00010625897092093343
testing: bce: 0.890640, dice: 3.167093, loss: 2.028867
IoU in current test batch is 0.8742248073345387
Epoch 33437: reducing learning rate of group 0 to 7.2531e-04.
training: bce: 0.027702, dice: 0.099545, loss: 0.063624
training IoU in current batch 900 is 0.8638166593212869
training IoU uptillnow 900 is 0.00011885355178368326
testing: bce: 0.959970, dice: 3.449634, loss: 2.204802
IoU in current test batch is 0.8694967640801556
Epoch 33538: reducing learning rate of group 0 to 7.2458e-04.
training: bce: 0.026749, dice: 0.096538, loss: 0.061643
training IoU in current batch 1000 is 0.8543972583707683
training IoU uptillnow 1000 is 0.000131232671413097
testing: bce: 1.029818, dice: 3.716695, loss: 2.373256
IoU in current test batch is 0.8583257112122081
Epoch 33639: reducing learning rate of group 0 to 7.2386e-04.
training: bce: 0.026977, dice: 0.094854, loss: 0.060915
training IoU in current batch 1100 is 0.9034344014944676
training IoU uptillnow 1100 is 0.00014426686014118876
testing: bce: 1.142380, dice: 4.016693, loss: 2.579536
IoU in current test batch is 0.8799875476407937
Epoch 33740: reducing learning rate of group 0 to 7.2313e-04.
training: bce: 0.026189, dice: 0.092822, loss: 0.059505
training IoU in current batch 1200 is 0.8125396438155648
training IoU uptillnow 1200 is 0.00015587719913565278
testing: bce: 1.209709, dice: 4.287647, loss: 2.748678
IoU in current test batch is 0.8726364800576687
Epoch 33841: reducing learning rate of group 0 to 7.2241e-04.
training: bce: 0.025522, dice: 0.090870, loss: 0.058196
training IoU in current batch 1300 is 0.86446763692072
training IoU uptillnow 1300 is 0.00016818598061091224
testing: bce: 1.277091, dice: 4.546990, loss: 2.912041
IoU in current test batch is 0.8869136233241016
Epoch 33942: reducing learning rate of group 0 to 7.2169e-04.
training: bce: 0.024940, dice: 0.089323, loss: 0.057131
training IoU in current batch 1400 is 0.8768310546875
training IoU uptillnow 1400 is 0.00018060433520472537
testing: bce: 1.343864, dice: 4.813112, loss: 3.078488
IoU in current test batch is 0.844523722968475
Epoch 34043: reducing learning rate of group 0 to 7.2097e-04.
training: bce: 0.025060, dice: 0.088171, loss: 0.056615
training IoU in current batch 1500 is 0.931210706275966
training IoU uptillnow 1500 is 0.00019374828503122826
testing: bce: 1.446738, dice: 5.090154, loss: 3.268446
IoU in current test batch is 0.8698563785293657
Epoch 34144: reducing learning rate of group 0 to 7.2024e-04.
training: bce: 0.024822, dice: 0.086873, loss: 0.055848
training IoU in current batch 1600 is 0.920597259153201
training IoU uptillnow 1600 is 0.00020665985917062935
testing: bce: 1.528459, dice: 5.349380, loss: 3.438919
IoU in current test batch is 0.8641314479824678
Epoch 34245: reducing learning rate of group 0 to 7.1952e-04.
training: bce: 0.024451, dice: 0.085719, loss: 0.055085
training IoU in current batch 1700 is 0.9196233102825173
training IoU uptillnow 1700 is 0.00021948181771671603
testing: bce: 1.599631, dice: 5.607995, loss: 3.603813
IoU in current test batch is 0.83546163030577
Epoch 34346: reducing learning rate of group 0 to 7.1881e-04.
training: bce: 0.024042, dice: 0.084318, loss: 0.054180
training IoU in current batch 1800 is 0.933378424078458
training IoU uptillnow 1800 is 0.00023242934330711928
testing: bce: 1.665392, dice: 5.840639, loss: 3.753015
IoU in current test batch is 0.8920819513862692
Epoch 34447: reducing learning rate of group 0 to 7.1809e-04.
training: bce: 0.023571, dice: 0.082988, loss: 0.053280
training IoU in current batch 1900 is 0.9231232065167083
training IoU uptillnow 1900 is 0.0002451528565345288
testing: bce: 1.723416, dice: 6.067716, loss: 3.895566
IoU in current test batch is 0.9010028020038026
Epoch 34548: reducing learning rate of group 0 to 7.1737e-04.
training: bce: 0.023261, dice: 0.082111, loss: 0.052686
training IoU in current batch 2000 is 0.9086805555555556
training IoU uptillnow 2000 is 0.0002575937020496397
testing: bce: 1.790224, dice: 6.319412, loss: 4.054818
IoU in current test batch is 0.8707449197725166
Epoch 34649: reducing learning rate of group 0 to 7.1665e-04.
training: bce: 0.023158, dice: 0.081683, loss: 0.052421
training IoU in current batch 2100 is 0.9067164179104478
training IoU uptillnow 2100 is 0.0002699343949528637
testing: bce: 1.871377, dice: 6.600618, loss: 4.235997
IoU in current test batch is 0.9096012980786843
training: bce: 0.023594, dice: 0.081005, loss: 0.052299
training IoU in current batch 2200 is 0.921436459880422
training IoU uptillnow 2200 is 0.00028241586347600576
testing: bce: 1.997295, dice: 6.857392, loss: 4.427343
IoU in current test batch is 0.8745208957491386
Epoch 34750: reducing learning rate of group 0 to 7.1593e-04.
training: bce: 0.023337, dice: 0.080493, loss: 0.051915
training IoU in current batch 2300 is 0.8439816448994
training IoU uptillnow 2300 is 0.0002937144254669124
testing: bce: 2.065282, dice: 7.123618, loss: 4.594450
IoU in current test batch is 0.8778901479305433
Epoch 34851: reducing learning rate of group 0 to 7.1522e-04.
training: bce: 0.023391, dice: 0.080153, loss: 0.051772
training IoU in current batch 2400 is 0.9136870634837903
training IoU uptillnow 2400 is 0.0003059455615116399
testing: bce: 2.160085, dice: 7.401824, loss: 4.780955
IoU in current test batch is 0.8705825664751907
Epoch 34952: reducing learning rate of group 0 to 7.1450e-04.
training: bce: 0.023205, dice: 0.079648, loss: 0.051426
training IoU in current batch 2500 is 0.9341709064479286
training IoU uptillnow 2500 is 0.0003183991171136542
testing: bce: 2.232117, dice: 7.661534, loss: 4.946825
IoU in current test batch is 0.888913156281518
Epoch 35053: reducing learning rate of group 0 to 7.1379e-04.
training: bce: 0.023057, dice: 0.079428, loss: 0.051243
training IoU in current batch 2600 is 0.8382440818202712
training IoU uptillnow 2600 is 0.00032941725760494907
testing: bce: 2.306575, dice: 7.945890, loss: 5.126232
IoU in current test batch is 0.8603214466873486
Epoch 35154: reducing learning rate of group 0 to 7.1307e-04.
training: bce: 0.023174, dice: 0.079357, loss: 0.051266
training IoU in current batch 2700 is 0.8882762925205451
training IoU uptillnow 2700 is 0.00034108256975121435
testing: bce: 2.407459, dice: 8.244004, loss: 5.325732
IoU in current test batch is 0.8990027305360677
Epoch 35255: reducing learning rate of group 0 to 7.1236e-04.
training: bce: 0.022984, dice: 0.078863, loss: 0.050924
training IoU in current batch 2800 is 0.9185465139167749
training IoU uptillnow 2800 is 0.00035311003831465273
testing: bce: 2.476128, dice: 8.496003, loss: 5.486066
IoU in current test batch is 0.8808260321755399
Epoch 35356: reducing learning rate of group 0 to 7.1165e-04.
training: bce: 0.022749, dice: 0.078240, loss: 0.050494
training IoU in current batch 2900 is 0.9475767400732662
training IoU uptillnow 2900 is 0.0003654791088574014
testing: bce: 2.538220, dice: 8.729725, loss: 5.633973
IoU in current test batch is 0.8883561648701712
Epoch 35457: reducing learning rate of group 0 to 7.1094e-04.
training: bce: 0.022470, dice: 0.077524, loss: 0.049997
training IoU in current batch 3000 is 0.8781129240017175
training IoU uptillnow 3000 is 0.0003768015902176207
testing: bce: 2.593541, dice: 8.948086, loss: 5.770813
IoU in current test batch is 0.8672230641260837
Epoch 35558: reducing learning rate of group 0 to 7.1023e-04.
training: bce: 0.022297, dice: 0.076931, loss: 0.049614
training IoU in current batch 3100 is 0.8070966250577901
training IoU uptillnow 3100 is 0.00038706451546221415
testing: bce: 2.659323, dice: 9.175499, loss: 5.917411
IoU in current test batch is 0.8891519455868276
Epoch 35659: reducing learning rate of group 0 to 7.0952e-04.
training: bce: 0.022058, dice: 0.076343, loss: 0.049200
training IoU in current batch 3200 is 0.8019866970068266
training IoU uptillnow 3200 is 0.0003971985564126384
testing: bce: 2.715656, dice: 9.398951, loss: 6.057304
IoU in current test batch is 0.897369323702471
Epoch 35760: reducing learning rate of group 0 to 7.0881e-04.
training: bce: 0.021895, dice: 0.075822, loss: 0.048859
training IoU in current batch 3300 is 0.9421542998860514
training IoU uptillnow 3300 is 0.0004092310057997083
testing: bce: 2.779843, dice: 9.626479, loss: 6.203161
IoU in current test batch is 0.8654291517451335
Epoch 35861: reducing learning rate of group 0 to 7.0810e-04.
training: bce: 0.021719, dice: 0.075402, loss: 0.048561
training IoU in current batch 3400 is 0.919170243204578
training IoU uptillnow 3400 is 0.00042087684290515674
testing: bce: 2.841032, dice: 9.863222, loss: 6.352127
IoU in current test batch is 0.831071310209858
Epoch 35962: reducing learning rate of group 0 to 7.0739e-04.
training: bce: 0.021709, dice: 0.075240, loss: 0.048475
training IoU in current batch 3500 is 0.8596309261420113
training IoU uptillnow 3500 is 0.00043163227026976625
testing: bce: 2.923189, dice: 10.131371, loss: 6.527280
IoU in current test batch is 0.8667342240021187
Epoch 36063: reducing learning rate of group 0 to 7.0668e-04.
training: bce: 0.021573, dice: 0.074980, loss: 0.048276
training IoU in current batch 3600 is 0.9439681240883703
training IoU uptillnow 3600 is 0.0004434946981046521
testing: bce: 2.987849, dice: 10.384723, loss: 6.686286
IoU in current test batch is 0.8846350691799313
Epoch 36164: reducing learning rate of group 0 to 7.0598e-04.
training: bce: 0.021548, dice: 0.074621, loss: 0.048084
training IoU in current batch 3700 is 0.9285119543237778
training IoU uptillnow 3700 is 0.00045507848567003716
testing: bce: 3.067214, dice: 10.621983, loss: 6.844599
IoU in current test batch is 0.899720263599671
Epoch 36265: reducing learning rate of group 0 to 7.0527e-04.
training: bce: 0.021445, dice: 0.074261, loss: 0.047853
training IoU in current batch 3800 is 0.8638253117644112
training IoU uptillnow 3800 is 0.00046570875038661375
testing: bce: 3.135123, dice: 10.856445, loss: 6.995784
IoU in current test batch is 0.8902790208955209
Epoch 36366: reducing learning rate of group 0 to 7.0456e-04.
training: bce: 0.021401, dice: 0.074026, loss: 0.047714
training IoU in current batch 3900 is 0.905411781786505
training IoU uptillnow 3900 is 0.00047685115332367985
testing: bce: 3.211004, dice: 11.106790, loss: 7.158897
IoU in current test batch is 0.8945930365474702
Epoch 36467: reducing learning rate of group 0 to 7.0386e-04.
training: bce: 0.021269, dice: 0.073768, loss: 0.047518
training IoU in current batch 4000 is 0.9313441345559937
training IoU uptillnow 4000 is 0.0004882873412864598
testing: bce: 3.272961, dice: 11.351716, loss: 7.312338
IoU in current test batch is 0.8861382719187573
Epoch 36568: reducing learning rate of group 0 to 7.0316e-04.
training: bce: 0.021178, dice: 0.073577, loss: 0.047378
training IoU in current batch 4100 is 0.9101145129605673
training IoU uptillnow 4100 is 0.0004993714903840366
testing: bce: 3.340499, dice: 11.605429, loss: 7.472964
IoU in current test batch is 0.861457501563685
Epoch 36669: reducing learning rate of group 0 to 7.0245e-04.
training: bce: 0.021060, dice: 0.073248, loss: 0.047154
training IoU in current batch 4200 is 0.8025990826767023
training IoU uptillnow 4200 is 0.0005089325018889536
testing: bce: 3.402847, dice: 11.835197, loss: 7.619022
IoU in current test batch is 0.8928452411612537
Epoch 36770: reducing learning rate of group 0 to 7.0175e-04.
training: bce: 0.020953, dice: 0.072922, loss: 0.046937
training IoU in current batch 4300 is 0.9458536230830729
training IoU uptillnow 4300 is 0.0005203853997940172
testing: bce: 3.466151, dice: 12.062931, loss: 7.764541
IoU in current test batch is 0.875413731447099
Epoch 36871: reducing learning rate of group 0 to 7.0105e-04.
training: bce: 0.020817, dice: 0.072573, loss: 0.046695
training IoU in current batch 4400 is 0.8344102843150591
training IoU uptillnow 4400 is 0.0005302682561839041
testing: bce: 3.523614, dice: 12.284418, loss: 7.904016
IoU in current test batch is 0.8925114157744017
Epoch 36972: reducing learning rate of group 0 to 7.0035e-04.
training: bce: 0.020731, dice: 0.072450, loss: 0.046590
training IoU in current batch 4500 is 0.9112357696749711
training IoU uptillnow 4500 is 0.0005411345582721671
testing: bce: 3.588862, dice: 12.542152, loss: 8.065507
IoU in current test batch is 0.8736984062173482
Epoch 37073: reducing learning rate of group 0 to 6.9965e-04.
training: bce: 0.020687, dice: 0.072244, loss: 0.046465
training IoU in current batch 4600 is 0.9391951301995265
training IoU uptillnow 4600 is 0.0005523186686712989
testing: bce: 3.660794, dice: 12.784364, loss: 8.222579
IoU in current test batch is 0.8987941688738393
Epoch 37174: reducing learning rate of group 0 to 6.9895e-04.
training: bce: 0.021078, dice: 0.072275, loss: 0.046677
training IoU in current batch 4700 is 0.8947932003747825
training IoU uptillnow 4700 is 0.0005628467222913545
testing: bce: 3.811120, dice: 13.067946, loss: 8.439533
IoU in current test batch is 0.8978385464602987
Epoch 37275: reducing learning rate of group 0 to 6.9825e-04.
training: bce: 0.021327, dice: 0.072297, loss: 0.046812
training IoU in current batch 4800 is 0.8010434350544491
training IoU uptillnow 4800 is 0.0005720633662972472
testing: bce: 3.938156, dice: 13.349981, loss: 8.644068
IoU in current test batch is 0.9127641456925646
Epoch 37376: reducing learning rate of group 0 to 6.9755e-04.
training: bce: 0.021168, dice: 0.071925, loss: 0.046546
training IoU in current batch 4900 is 0.9166927286942924
training IoU uptillnow 4900 is 0.0005827748585125617
testing: bce: 3.990170, dice: 13.557810, loss: 8.773990
IoU in current test batch is 0.8980091819854508
Epoch 37477: reducing learning rate of group 0 to 6.9685e-04.
training: bce: 0.021176, dice: 0.071791, loss: 0.046484
training IoU in current batch 5000 is 0.9202815099168267
training IoU uptillnow 5000 is 0.0005934770854159069
testing: bce: 4.073201, dice: 13.808806, loss: 8.941003
IoU in current test batch is 0.8969864374108126
Epoch 37578: reducing learning rate of group 0 to 6.9616e-04.
training: bce: 0.021128, dice: 0.071771, loss: 0.046450
training IoU in current batch 5100 is 0.9380479513329755
training IoU uptillnow 5100 is 0.0006043584056758014
testing: bce: 4.145170, dice: 14.080932, loss: 9.113051
IoU in current test batch is 0.8478097107394676
Epoch 37679: reducing learning rate of group 0 to 6.9546e-04.
training: bce: 0.021155, dice: 0.071672, loss: 0.046414
training IoU in current batch 5200 is 0.9170455078033463
training IoU uptillnow 5200 is 0.0006149038940487358
testing: bce: 4.231911, dice: 14.337239, loss: 9.284575
IoU in current test batch is 0.8865621625624367
Epoch 37780: reducing learning rate of group 0 to 6.9476e-04.
training: bce: 0.021026, dice: 0.071355, loss: 0.046190
training IoU in current batch 5300 is 0.9347335140018067
training IoU uptillnow 5300 is 0.000625627321507382
testing: bce: 4.286786, dice: 14.548274, loss: 9.417530
IoU in current test batch is 0.8991618905648059
Epoch 37881: reducing learning rate of group 0 to 6.9407e-04.
training: bce: 0.020928, dice: 0.071156, loss: 0.046042
training IoU in current batch 5400 is 0.9603924564719128
training IoU uptillnow 5400 is 0.0006366323017069953
testing: bce: 4.347371, dice: 14.781360, loss: 9.564366
IoU in current test batch is 0.8805699130661362
Epoch 37982: reducing learning rate of group 0 to 6.9338e-04.
training: bce: 0.020828, dice: 0.070912, loss: 0.045870
training IoU in current batch 5500 is 0.8191405089612833
training IoU uptillnow 5500 is 0.000645723275946077
testing: bce: 4.406795, dice: 15.003332, loss: 9.705064
IoU in current test batch is 0.9016661153117151
Epoch 38083: reducing learning rate of group 0 to 6.9268e-04.
training: bce: 0.020732, dice: 0.070686, loss: 0.045709
training IoU in current batch 5600 is 0.8673492735384968
training IoU uptillnow 5600 is 0.0006553984305136242
testing: bce: 4.466242, dice: 15.227459, loss: 9.846851
IoU in current test batch is 0.9018915186977329
Epoch 38184: reducing learning rate of group 0 to 6.9199e-04.
training: bce: 0.020639, dice: 0.070518, loss: 0.045579
training IoU in current batch 5700 is 0.9275943222769729
training IoU uptillnow 5700 is 0.0006658105226478252
testing: bce: 4.525448, dice: 15.462480, loss: 9.993964
IoU in current test batch is 0.8672886352234339
Epoch 38285: reducing learning rate of group 0 to 6.9130e-04.
training: bce: 0.020610, dice: 0.070486, loss: 0.045548
training IoU in current batch 5800 is 0.9234249912982945
training IoU uptillnow 5800 is 0.000676113954071556
testing: bce: 4.598494, dice: 15.726617, loss: 10.162556
IoU in current test batch is 0.8771626633382894
Epoch 38386: reducing learning rate of group 0 to 6.9061e-04.
training: bce: 0.020557, dice: 0.070377, loss: 0.045467
training IoU in current batch 5900 is 0.9577081029853104
training IoU uptillnow 5900 is 0.000686809610870357
testing: bce: 4.665735, dice: 15.972897, loss: 10.319316
IoU in current test batch is 0.9050098882714436
Epoch 38487: reducing learning rate of group 0 to 6.8992e-04.
training: bce: 0.020532, dice: 0.070331, loss: 0.045431
training IoU in current batch 6000 is 0.8177393531860391
training IoU uptillnow 6000 is 0.0006956343353286765
testing: bce: 4.738914, dice: 16.232853, loss: 10.485884
IoU in current test batch is 0.8723012235950774
Epoch 38588: reducing learning rate of group 0 to 6.8923e-04.
training: bce: 0.020427, dice: 0.070058, loss: 0.045242
training IoU in current batch 6100 is 0.9584481262029664
training IoU uptillnow 6100 is 0.0007062337125410237
testing: bce: 4.793340, dice: 16.439278, loss: 10.616309
IoU in current test batch is 0.8962780524029743
Epoch 38689: reducing learning rate of group 0 to 6.8854e-04.
training: bce: 0.020318, dice: 0.069819, loss: 0.045069
training IoU in current batch 6200 is 0.9350628054179922
training IoU uptillnow 6200 is 0.0007164766326162478
testing: bce: 4.845912, dice: 16.651780, loss: 10.748846
IoU in current test batch is 0.8739140461269025
Epoch 38790: reducing learning rate of group 0 to 6.8785e-04.
training: bce: 0.020248, dice: 0.069729, loss: 0.044988
training IoU in current batch 6300 is 0.8511049333270508
training IoU uptillnow 6300 is 0.0007255862686064639
testing: bce: 4.906963, dice: 16.898600, loss: 10.902782
IoU in current test batch is 0.8661766316035926
Epoch 38891: reducing learning rate of group 0 to 6.8716e-04.
training: bce: 0.020388, dice: 0.069721, loss: 0.045055
training IoU in current batch 6400 is 0.9345703504881939
training IoU uptillnow 6400 is 0.0007357205847949502
testing: bce: 5.019427, dice: 17.164728, loss: 11.092078
IoU in current test batch is 0.8801473219407825
Epoch 38992: reducing learning rate of group 0 to 6.8647e-04.
training: bce: 0.020375, dice: 0.069605, loss: 0.044990
training IoU in current batch 6500 is 0.8997821350762527
training IoU uptillnow 6500 is 0.0007453575586117375
testing: bce: 5.094457, dice: 17.403963, loss: 11.249210
IoU in current test batch is 0.8847721910084734
Epoch 39093: reducing learning rate of group 0 to 6.8579e-04.
training: bce: 0.020349, dice: 0.069474, loss: 0.044911
training IoU in current batch 6600 is 0.87200717587801
training IoU uptillnow 6600 is 0.000754590571346966
testing: bce: 5.166270, dice: 17.638258, loss: 11.402264
IoU in current test batch is 0.899712125289176
Epoch 39194: reducing learning rate of group 0 to 6.8510e-04.
training: bce: 0.020308, dice: 0.069463, loss: 0.044885
training IoU in current batch 6700 is 0.923239496153521
training IoU uptillnow 6700 is 0.000764429185620831
testing: bce: 5.233872, dice: 17.902720, loss: 11.568296
IoU in current test batch is 0.8876638388023932
Epoch 39295: reducing learning rate of group 0 to 6.8442e-04.
training: bce: 0.020225, dice: 0.069303, loss: 0.044764
training IoU in current batch 6800 is 0.9465233661126099
training IoU uptillnow 6800 is 0.0007745136533902873
testing: bce: 5.290364, dice: 18.127962, loss: 11.709163
IoU in current test batch is 0.8713664623758752
Epoch 39396: reducing learning rate of group 0 to 6.8373e-04.
training: bce: 0.020177, dice: 0.069263, loss: 0.044720
training IoU in current batch 6900 is 0.9191098838625843
training IoU uptillnow 6900 is 0.0007841995448868148
testing: bce: 5.355456, dice: 18.384098, loss: 11.869777
IoU in current test batch is 0.8952427563781611
Epoch 39497: reducing learning rate of group 0 to 6.8305e-04.
training: bce: 0.020247, dice: 0.069258, loss: 0.044753
training IoU in current batch 7000 is 0.9531511433351925
training IoU uptillnow 7000 is 0.000794266817979494
testing: bce: 5.451923, dice: 18.649130, loss: 12.050526
IoU in current test batch is 0.8932943396082605
Epoch 39598: reducing learning rate of group 0 to 6.8236e-04.
training: bce: 0.020252, dice: 0.069067, loss: 0.044660
training IoU in current batch 7100 is 0.9005109281862049
training IoU uptillnow 7100 is 0.0008036194903283295
testing: bce: 5.531116, dice: 18.863355, loss: 12.197235
IoU in current test batch is 0.9164133263902815
Epoch 39699: reducing learning rate of group 0 to 6.8168e-04.
training: bce: 0.020156, dice: 0.068862, loss: 0.044509
training IoU in current batch 7200 is 0.9394822139252451
training IoU uptillnow 7200 is 0.0008134153156325418
testing: bce: 5.582509, dice: 19.072064, loss: 12.327286
IoU in current test batch is 0.906655901899801
Epoch 39800: reducing learning rate of group 0 to 6.8100e-04.
training: bce: 0.020106, dice: 0.068756, loss: 0.044431
training IoU in current batch 7300 is 0.9325399547294053
training IoU uptillnow 7300 is 0.0008230748708540991
testing: bce: 5.645996, dice: 19.307350, loss: 12.476673
IoU in current test batch is 0.8841213084872577
Epoch 39901: reducing learning rate of group 0 to 6.8032e-04.
training: bce: 0.020018, dice: 0.068550, loss: 0.044284
training IoU in current batch 7400 is 0.8676205362510521
training IoU uptillnow 7400 is 0.0008318735487108963
testing: bce: 5.698243, dice: 19.513156, loss: 12.605699
IoU in current test batch is 0.8714825437432666
Epoch 40002: reducing learning rate of group 0 to 6.7964e-04.
training: bce: 0.020106, dice: 0.068524, loss: 0.044315
training IoU in current batch 7500 is 0.9353879521444064
training IoU uptillnow 7500 is 0.0008414743332700322
testing: bce: 5.800618, dice: 19.769060, loss: 12.784839
IoU in current test batch is 0.8744824693027184
Epoch 40103: reducing learning rate of group 0 to 6.7896e-04.
training: bce: 0.020035, dice: 0.068329, loss: 0.044182
training IoU in current batch 7600 is 0.912540657752078
training IoU uptillnow 7600 is 0.0008507427648955576
testing: bce: 5.857112, dice: 19.975588, loss: 12.916350
IoU in current test batch is 0.900018897155237
Epoch 40204: reducing learning rate of group 0 to 6.7828e-04.
training: bce: 0.019953, dice: 0.068164, loss: 0.044058
training IoU in current batch 7700 is 0.9416286149162861
training IoU uptillnow 7700 is 0.0008603264872018866
testing: bce: 5.909969, dice: 20.189561, loss: 13.049765
IoU in current test batch is 0.8909536162292326
Epoch 40305: reducing learning rate of group 0 to 6.7760e-04.
training: bce: 0.019863, dice: 0.067926, loss: 0.043895
training IoU in current batch 7800 is 0.941054945054945
training IoU uptillnow 7800 is 0.0008698555971984735
testing: bce: 5.959794, dice: 20.380405, loss: 13.170099
IoU in current test batch is 0.8956271915913502
Epoch 40406: reducing learning rate of group 0 to 6.7692e-04.
training: bce: 0.019799, dice: 0.067789, loss: 0.043794
training IoU in current batch 7900 is 0.9144252068170516
training IoU uptillnow 7900 is 0.0008790084184617446
testing: bce: 6.016654, dice: 20.599948, loss: 13.308301
IoU in current test batch is 0.8966251175415346
Epoch 40507: reducing learning rate of group 0 to 6.7625e-04.
training: bce: 0.019833, dice: 0.067763, loss: 0.043798
training IoU in current batch 8000 is 0.927169761622547
training IoU uptillnow 8000 is 0.0008882732438964626
testing: bce: 6.103283, dice: 20.852679, loss: 13.477981
IoU in current test batch is 0.9076906933229948
Epoch 40608: reducing learning rate of group 0 to 6.7557e-04.
training: bce: 0.019792, dice: 0.067663, loss: 0.043727
training IoU in current batch 8100 is 0.9261554902484638
training IoU uptillnow 8100 is 0.0008974800095574076
testing: bce: 6.166735, dice: 21.082140, loss: 13.624438
IoU in current test batch is 0.8894540774266193
Epoch 40709: reducing learning rate of group 0 to 6.7490e-04.
training: bce: 0.019715, dice: 0.067496, loss: 0.043606
training IoU in current batch 8200 is 0.9530712791975331
training IoU uptillnow 8200 is 0.0009069718472153672
testing: bce: 6.218656, dice: 21.289739, loss: 13.754198
IoU in current test batch is 0.9018968193032442
Epoch 40810: reducing learning rate of group 0 to 6.7422e-04.
training: bce: 0.019657, dice: 0.067418, loss: 0.043537
training IoU in current batch 8300 is 0.9026352664647652
training IoU uptillnow 8300 is 0.000915799873225743
testing: bce: 6.275834, dice: 21.524499, loss: 13.900167
IoU in current test batch is 0.8719670187028451
Epoch 40911: reducing learning rate of group 0 to 6.7355e-04.
training: bce: 0.019590, dice: 0.067229, loss: 0.043409
training IoU in current batch 8400 is 0.9521135671635889
training IoU uptillnow 8400 is 0.0009251889206197093
testing: bce: 6.329779, dice: 21.722646, loss: 14.026212
IoU in current test batch is 0.8934735795862708
Epoch 41012: reducing learning rate of group 0 to 6.7287e-04.
training: bce: 0.019520, dice: 0.067035, loss: 0.043277
training IoU in current batch 8500 is 0.9360469995803609
training IoU uptillnow 8500 is 0.0009343365255291043
testing: bce: 6.382344, dice: 21.917777, loss: 14.150060
IoU in current test batch is 0.9120070746722821
Epoch 41113: reducing learning rate of group 0 to 6.7220e-04.
training: bce: 0.019528, dice: 0.066985, loss: 0.043257
training IoU in current batch 8600 is 0.9206042296072507
training IoU uptillnow 8600 is 0.0009432520278377767
testing: bce: 6.460135, dice: 22.159115, loss: 14.309625
IoU in current test batch is 0.880449746170365
Epoch 41214: reducing learning rate of group 0 to 6.7153e-04.
training: bce: 0.019473, dice: 0.066874, loss: 0.043174
training IoU in current batch 8700 is 0.8501519756838906
training IoU uptillnow 8700 is 0.000951270325879163
testing: bce: 6.516763, dice: 22.379666, loss: 14.448215
IoU in current test batch is 0.9084108994164611
Epoch 41315: reducing learning rate of group 0 to 6.7086e-04.
training: bce: 0.019472, dice: 0.066794, loss: 0.043133
training IoU in current batch 8800 is 0.9083893774788757
training IoU uptillnow 8800 is 0.0009599540501358413
testing: bce: 6.591241, dice: 22.609740, loss: 14.600490
IoU in current test batch is 0.9069907968984118
Epoch 41416: reducing learning rate of group 0 to 6.7019e-04.
training: bce: 0.019500, dice: 0.066782, loss: 0.043141
training IoU in current batch 8900 is 0.900656340246784
training IoU uptillnow 8900 is 0.000968502591496046
testing: bce: 6.675668, dice: 22.862526, loss: 14.769097
IoU in current test batch is 0.887988020719769
Epoch 41517: reducing learning rate of group 0 to 6.6952e-04.
training: bce: 0.019465, dice: 0.066732, loss: 0.043099
training IoU in current batch 9000 is 0.8900710766144857
training IoU uptillnow 9000 is 0.0009768826027875812
testing: bce: 6.738800, dice: 23.102175, loss: 14.920487
IoU in current test batch is 0.8596542437403396
Epoch 41618: reducing learning rate of group 0 to 6.6885e-04.
training: bce: 0.019446, dice: 0.066618, loss: 0.043032
training IoU in current batch 9100 is 0.9365157682661075
training IoU uptillnow 9100 is 0.0009857799394627944
testing: bce: 6.806773, dice: 23.318879, loss: 15.062826
IoU in current test batch is 0.8968163947207155
Epoch 41719: reducing learning rate of group 0 to 6.6818e-04.
training: bce: 0.019388, dice: 0.066498, loss: 0.042943
training IoU in current batch 9200 is 0.9103430792631401
training IoU uptillnow 9200 is 0.0009943212045242992
testing: bce: 6.861020, dice: 23.532707, loss: 15.196863
IoU in current test batch is 0.8972195519424999
Epoch 41820: reducing learning rate of group 0 to 6.6751e-04.
training: bce: 0.019382, dice: 0.066454, loss: 0.042918
training IoU in current batch 9300 is 0.8891698436162764
training IoU uptillnow 9300 is 0.0010025686818264344
testing: bce: 6.933506, dice: 23.772545, loss: 15.353026
IoU in current test batch is 0.8917944481012561
Epoch 41921: reducing learning rate of group 0 to 6.6684e-04.
training: bce: 0.019343, dice: 0.066433, loss: 0.042888
training IoU in current batch 9400 is 0.9295504938520459
training IoU uptillnow 9400 is 0.0010112581388817838
testing: bce: 6.993816, dice: 24.020564, loss: 15.507190
IoU in current test batch is 0.9042939268902171
Epoch 42022: reducing learning rate of group 0 to 6.6617e-04.
training: bce: 0.019290, dice: 0.066358, loss: 0.042824
training IoU in current batch 9500 is 0.8930747034305867
training IoU uptillnow 9500 is 0.0010194725418551155
testing: bce: 7.049181, dice: 24.248713, loss: 15.648947
IoU in current test batch is 0.8850274660105877
Epoch 42123: reducing learning rate of group 0 to 6.6551e-04.
training: bce: 0.019281, dice: 0.066369, loss: 0.042825
training IoU in current batch 9600 is 0.9133763440860215
training IoU uptillnow 9600 is 0.0010278887963268766
testing: bce: 7.119994, dice: 24.507856, loss: 15.813925
IoU in current test batch is 0.9009917871291241
Epoch 42224: reducing learning rate of group 0 to 6.6484e-04.
training: bce: 0.019338, dice: 0.066341, loss: 0.042840
training IoU in current batch 9700 is 0.9555851385695268
training IoU uptillnow 9700 is 0.001036764728341506
testing: bce: 7.215417, dice: 24.752769, loss: 15.984093
IoU in current test batch is 0.9133540509125028
Epoch 42325: reducing learning rate of group 0 to 6.6418e-04.
training: bce: 0.019326, dice: 0.066253, loss: 0.042789
training IoU in current batch 9800 is 0.7968413496051687
training IoU uptillnow 9800 is 0.0010437245319275799
testing: bce: 7.285188, dice: 24.974691, loss: 16.129940
IoU in current test batch is 0.9010341930643597
Epoch 42426: reducing learning rate of group 0 to 6.6351e-04.
training: bce: 0.019469, dice: 0.066380, loss: 0.042924
training IoU in current batch 9900 is 0.9219842577338458
training IoU uptillnow 9900 is 0.00105212556552454
testing: bce: 7.413784, dice: 25.277846, loss: 16.345815
IoU in current test batch is 0.8845784403068306
Epoch 42527: reducing learning rate of group 0 to 6.6285e-04.
training: bce: 0.019423, dice: 0.066249, loss: 0.042836
training IoU in current batch 10000 is 0.8720160235448006
training IoU uptillnow 10000 is 0.0010598999331486003
testing: bce: 7.471257, dice: 25.482864, loss: 16.477061
IoU in current test batch is 0.8790554164957997
Epoch 42628: reducing learning rate of group 0 to 6.6219e-04.
training: bce: 0.019377, dice: 0.066141, loss: 0.042759
training IoU in current batch 10100 is 0.9048958333333333
training IoU uptillnow 10100 is 0.0010680233091167077
testing: bce: 7.528111, dice: 25.695888, loss: 16.612000
IoU in current test batch is 0.8849680566601071
Epoch 42729: reducing learning rate of group 0 to 6.6153e-04.
training: bce: 0.019311, dice: 0.065993, loss: 0.042652
training IoU in current batch 10200 is 0.9328057879267465
training IoU uptillnow 10200 is 0.0010764351165776537
testing: bce: 7.576734, dice: 25.892045, loss: 16.734390
IoU in current test batch is 0.8800364577674149
Epoch 42830: reducing learning rate of group 0 to 6.6086e-04.
training: bce: 0.019264, dice: 0.065922, loss: 0.042593
training IoU in current batch 10300 is 0.8796210088175694
training IoU uptillnow 10300 is 0.0010841870621721417
testing: bce: 7.632322, dice: 26.117711, loss: 16.875016
IoU in current test batch is 0.9045568756549713
Epoch 42931: reducing learning rate of group 0 to 6.6020e-04.
training: bce: 0.019233, dice: 0.065876, loss: 0.042554
training IoU in current batch 10400 is 0.9150700332599428
training IoU uptillnow 10400 is 0.0010923155924323952
testing: bce: 7.693781, dice: 26.353043, loss: 17.023412
IoU in current test batch is 0.8922148140434145
Epoch 43032: reducing learning rate of group 0 to 6.5954e-04.
training: bce: 0.019216, dice: 0.065833, loss: 0.042525
training IoU in current batch 10500 is 0.9457965674391544
training IoU uptillnow 10500 is 0.0011007632335078162
testing: bce: 7.760959, dice: 26.589027, loss: 17.174993
IoU in current test batch is 0.8635586621297086
Epoch 43133: reducing learning rate of group 0 to 6.5888e-04.
training: bce: 0.019315, dice: 0.065877, loss: 0.042596
training IoU in current batch 10600 is 0.9536369593709043
training IoU uptillnow 10600 is 0.001109262570842772
testing: bce: 7.875459, dice: 26.860001, loss: 17.367730
IoU in current test batch is 0.9063495262510889
Epoch 43234: reducing learning rate of group 0 to 6.5822e-04.
training: bce: 0.019335, dice: 0.065857, loss: 0.042596
training IoU in current batch 10700 is 0.866842018740753
training IoU uptillnow 10700 is 0.0011167191831108006
testing: bce: 7.957930, dice: 27.105094, loss: 17.531512
IoU in current test batch is 0.9168151877529731
Epoch 43335: reducing learning rate of group 0 to 6.5757e-04.
training: bce: 0.019343, dice: 0.065795, loss: 0.042569
training IoU in current batch 10800 is 0.9570234943745863
training IoU uptillnow 10800 is 0.0011251815605056081
testing: bce: 8.035514, dice: 27.332951, loss: 17.684232
IoU in current test batch is 0.8761214566650288
Epoch 43436: reducing learning rate of group 0 to 6.5691e-04.
training: bce: 0.019308, dice: 0.065774, loss: 0.042541
training IoU in current batch 10900 is 0.9295576878071612
training IoU uptillnow 10900 is 0.0011332889193440992
testing: bce: 8.095346, dice: 27.577205, loss: 17.836275
IoU in current test batch is 0.9041645826829832
Epoch 43537: reducing learning rate of group 0 to 6.5625e-04.
training: bce: 0.019283, dice: 0.065753, loss: 0.042518
training IoU in current batch 11000 is 0.911644104926136
training IoU uptillnow 11000 is 0.0011411533761238248
testing: bce: 8.159092, dice: 27.821018, loss: 17.990055
IoU in current test batch is 0.8894800634689353
Epoch 43638: reducing learning rate of group 0 to 6.5560e-04.
training: bce: 0.019303, dice: 0.065772, loss: 0.042537
training IoU in current batch 11100 is 0.8902548237401287
training IoU uptillnow 11100 is 0.0011487367865697104
testing: bce: 8.241444, dice: 28.082281, loss: 18.161863
IoU in current test batch is 0.8534523177050205
Epoch 43739: reducing learning rate of group 0 to 6.5494e-04.
training: bce: 0.019263, dice: 0.065699, loss: 0.042481
training IoU in current batch 11200 is 0.9283234036331737
training IoU uptillnow 11200 is 0.0011567206040569871
testing: bce: 8.298509, dice: 28.303686, loss: 18.301098
IoU in current test batch is 0.899709772202235
Epoch 43840: reducing learning rate of group 0 to 6.5429e-04.
training: bce: 0.019300, dice: 0.065713, loss: 0.042506
training IoU in current batch 11300 is 0.9530789655548685
training IoU uptillnow 11300 is 0.0011649502856353798
testing: bce: 8.388873, dice: 28.562204, loss: 18.475539
IoU in current test batch is 0.8713319326765364
Epoch 43941: reducing learning rate of group 0 to 6.5363e-04.
training: bce: 0.019263, dice: 0.065626, loss: 0.042445
training IoU in current batch 11400 is 0.834775967413442
training IoU uptillnow 11400 is 0.001171796619612858
testing: bce: 8.446728, dice: 28.777168, loss: 18.611948
IoU in current test batch is 0.8981881469917771
Epoch 44042: reducing learning rate of group 0 to 6.5298e-04.
training: bce: 0.019332, dice: 0.065617, loss: 0.042474
training IoU in current batch 11500 is 0.9396591012669766
training IoU uptillnow 11500 is 0.0011798023833257767
testing: bce: 8.551361, dice: 29.025343, loss: 18.788352
IoU in current test batch is 0.9057395834325513
Epoch 44143: reducing learning rate of group 0 to 6.5232e-04.
training: bce: 0.019291, dice: 0.065543, loss: 0.042417
training IoU in current batch 11600 is 0.9260357648725213
training IoU uptillnow 11600 is 0.0011876175940100128
testing: bce: 8.607544, dice: 29.244976, loss: 18.926260
IoU in current test batch is 0.8786540099253385
Epoch 44244: reducing learning rate of group 0 to 6.5167e-04.
training: bce: 0.019336, dice: 0.065548, loss: 0.042442
training IoU in current batch 11700 is 0.93209828086977
training IoU uptillnow 11700 is 0.0011954659850886438
testing: bce: 8.701987, dice: 29.499135, loss: 19.100561
IoU in current test batch is 0.8658885720708004
Epoch 44345: reducing learning rate of group 0 to 6.5102e-04.
training: bce: 0.019616, dice: 0.065573, loss: 0.042595
training IoU in current batch 11800 is 0.8748856358645929
training IoU uptillnow 11800 is 0.0012026339626176673
testing: bce: 8.903492, dice: 29.762618, loss: 19.333055
IoU in current test batch is 0.8909643291190689
Epoch 44446: reducing learning rate of group 0 to 6.5037e-04.
training: bce: 0.019573, dice: 0.065479, loss: 0.042526
training IoU in current batch 11900 is 0.9512992541634029
training IoU uptillnow 11900 is 0.0012106292433479327
testing: bce: 8.959323, dice: 29.971873, loss: 19.465598
IoU in current test batch is 0.9013530054427713
Epoch 44547: reducing learning rate of group 0 to 6.4972e-04.
training: bce: 0.019683, dice: 0.065482, loss: 0.042582
training IoU in current batch 12000 is 0.9230288160869774
training IoU uptillnow 12000 is 0.001218271337730781
testing: bce: 9.085019, dice: 30.225076, loss: 19.655048
IoU in current test batch is 0.906844671122275
Epoch 44648: reducing learning rate of group 0 to 6.4907e-04.
training: bce: 0.019719, dice: 0.065441, loss: 0.042580
training IoU in current batch 12100 is 0.9347332313540073
training IoU uptillnow 12100 is 0.0012260102705722222
testing: bce: 9.177848, dice: 30.457922, loss: 19.817885
IoU in current test batch is 0.8853428176328726
Epoch 44749: reducing learning rate of group 0 to 6.4842e-04.
training: bce: 0.019753, dice: 0.065518, loss: 0.042635
training IoU in current batch 12200 is 0.909755069545263
training IoU uptillnow 12200 is 0.0012334355268927488
testing: bce: 9.269625, dice: 30.745351, loss: 20.007488
IoU in current test batch is 0.8737839421385181
training: bce: 0.019707, dice: 0.065466, loss: 0.042587
training IoU in current batch 12300 is 0.8692479442356913
training IoU uptillnow 12300 is 0.0012403760818471758
testing: bce: 9.323713, dice: 30.973114, loss: 20.148413
IoU in current test batch is 0.8861666765607489
Epoch 44850: reducing learning rate of group 0 to 6.4777e-04.
training: bce: 0.019692, dice: 0.065431, loss: 0.042562
training IoU in current batch 12400 is 0.860450735193074
training IoU uptillnow 12400 is 0.0012471878986507403
testing: bce: 9.392438, dice: 31.208161, loss: 20.300300
IoU in current test batch is 0.8859547255171497
Epoch 44951: reducing learning rate of group 0 to 6.4712e-04.
training: bce: 0.019640, dice: 0.065310, loss: 0.042475
training IoU in current batch 12500 is 0.9241239638281838
training IoU uptillnow 12500 is 0.0012546761769235072
testing: bce: 9.443187, dice: 31.401495, loss: 20.422341
IoU in current test batch is 0.9074226547754448
Epoch 45052: reducing learning rate of group 0 to 6.4648e-04.
training: bce: 0.019658, dice: 0.065337, loss: 0.042497
training IoU in current batch 12600 is 0.9253587196467992
training IoU uptillnow 12600 is 0.0012621449582418175
testing: bce: 9.527111, dice: 31.666022, loss: 20.596566
IoU in current test batch is 0.905874915519
Epoch 45153: reducing learning rate of group 0 to 6.4583e-04.
training: bce: 0.019647, dice: 0.065306, loss: 0.042477
training IoU in current batch 12700 is 0.8359045755568935
training IoU uptillnow 12700 is 0.0012685922735039589
testing: bce: 9.597623, dice: 31.902036, loss: 20.749830
IoU in current test batch is 0.8898816374465857
Epoch 45254: reducing learning rate of group 0 to 6.4518e-04.
training: bce: 0.019610, dice: 0.065244, loss: 0.042427
training IoU in current batch 12800 is 0.9113298006560686
training IoU uptillnow 12800 is 0.0012758427541702867
testing: bce: 9.654761, dice: 32.122699, loss: 20.888730
IoU in current test batch is 0.9001875376584838
Epoch 45355: reducing learning rate of group 0 to 6.4454e-04.
training: bce: 0.019578, dice: 0.065217, loss: 0.042397
training IoU in current batch 12900 is 0.9637324645967689
training IoU uptillnow 12900 is 0.0012836378224742582
testing: bce: 9.714499, dice: 32.359943, loss: 21.037221
IoU in current test batch is 0.8863215109368379
Epoch 45456: reducing learning rate of group 0 to 6.4389e-04.
training: bce: 0.019543, dice: 0.065142, loss: 0.042343
training IoU in current batch 13000 is 0.9458578963698417
training IoU uptillnow 13000 is 0.0012912024536104398
testing: bce: 9.772457, dice: 32.573579, loss: 21.173018
IoU in current test batch is 0.899465626638584
Epoch 45557: reducing learning rate of group 0 to 6.4325e-04.
training: bce: 0.019504, dice: 0.065107, loss: 0.042305
training IoU in current batch 13100 is 0.8841833440929632
training IoU uptillnow 13100 is 0.0012980584197587093
testing: bce: 9.827523, dice: 32.806525, loss: 21.317024
IoU in current test batch is 0.8876052359619998
Epoch 45658: reducing learning rate of group 0 to 6.4261e-04.
training: bce: 0.019465, dice: 0.065049, loss: 0.042257
training IoU in current batch 13200 is 0.9272442452460729
training IoU uptillnow 13200 is 0.0013053550302276143
testing: bce: 9.882794, dice: 33.027479, loss: 21.455136
IoU in current test batch is 0.892244297053629
Epoch 45759: reducing learning rate of group 0 to 6.4196e-04.
training: bce: 0.019432, dice: 0.065036, loss: 0.042234
training IoU in current batch 13300 is 0.9468833108674268
training IoU uptillnow 13300 is 0.0013128339809775887
testing: bce: 9.941133, dice: 33.270995, loss: 21.606064
IoU in current test batch is 0.8806604486692431
Epoch 45860: reducing learning rate of group 0 to 6.4132e-04.
training: bce: 0.019395, dice: 0.064984, loss: 0.042190
training IoU in current batch 13400 is 0.9209025780983479
training IoU uptillnow 13400 is 0.0013199976691777087
testing: bce: 9.996481, dice: 33.494408, loss: 21.745445
IoU in current test batch is 0.8829125343246246
Epoch 45961: reducing learning rate of group 0 to 6.4068e-04.
training: bce: 0.019418, dice: 0.064980, loss: 0.042199
training IoU in current batch 13500 is 0.9395091825125955
training IoU uptillnow 13500 is 0.0013273322726878125
testing: bce: 10.083361, dice: 33.742365, loss: 21.912863
IoU in current test batch is 0.8595960754588785
Epoch 46062: reducing learning rate of group 0 to 6.4004e-04.
training: bce: 0.019377, dice: 0.064913, loss: 0.042145
training IoU in current batch 13600 is 0.9305066121026108
training IoU uptillnow 13600 is 0.0013345375528919863
testing: bce: 10.136650, dice: 33.956804, loss: 22.046727
IoU in current test batch is 0.9067458725054176
Epoch 46163: reducing learning rate of group 0 to 6.3940e-04.
training: bce: 0.019353, dice: 0.064848, loss: 0.042100
training IoU in current batch 13700 is 0.9186908343729361
training IoU uptillnow 13700 is 0.0013415839352722773
testing: bce: 10.198366, dice: 34.172321, loss: 22.185344
IoU in current test batch is 0.9044185747999698
Epoch 46264: reducing learning rate of group 0 to 6.3876e-04.
training: bce: 0.019350, dice: 0.064768, loss: 0.042059
training IoU in current batch 13800 is 0.9263491248341679
training IoU uptillnow 13800 is 0.001348682526818893
testing: bce: 10.271390, dice: 34.379613, loss: 22.325501
IoU in current test batch is 0.8635634628030991
Epoch 46365: reducing learning rate of group 0 to 6.3812e-04.
training: bce: 0.019305, dice: 0.064665, loss: 0.041985
training IoU in current batch 13900 is 0.9383328978311993
training IoU uptillnow 13900 is 0.0013558795514635868
testing: bce: 10.321609, dice: 34.573335, loss: 22.447472
IoU in current test batch is 0.8814378389656885
Epoch 46466: reducing learning rate of group 0 to 6.3748e-04.
training: bce: 0.019257, dice: 0.064553, loss: 0.041905
training IoU in current batch 14000 is 0.9310081141934662
training IoU uptillnow 14000 is 0.0013629669767195051
testing: bce: 10.369886, dice: 34.761626, loss: 22.565756
IoU in current test batch is 0.9102446224721763
Epoch 46567: reducing learning rate of group 0 to 6.3685e-04.
training: bce: 0.019242, dice: 0.064511, loss: 0.041876
training IoU in current batch 14100 is 0.9323699646505846
training IoU uptillnow 14100 is 0.001370038612742471
testing: bce: 10.435625, dice: 34.987454, loss: 22.711540
IoU in current test batch is 0.8783042637162695
Epoch 46668: reducing learning rate of group 0 to 6.3621e-04.
training: bce: 0.019203, dice: 0.064451, loss: 0.041827
training IoU in current batch 14200 is 0.9138727238595833
training IoU uptillnow 14200 is 0.001376882161885361
testing: bce: 10.488347, dice: 35.202461, loss: 22.845404
IoU in current test batch is 0.8923522816094409
Epoch 46769: reducing learning rate of group 0 to 6.3557e-04.
training: bce: 0.019173, dice: 0.064397, loss: 0.041785
training IoU in current batch 14300 is 0.8637966101694915
training IoU uptillnow 14300 is 0.001383162060046413
testing: bce: 10.545697, dice: 35.421002, loss: 22.983350
IoU in current test batch is 0.8780520166591251
Epoch 46870: reducing learning rate of group 0 to 6.3494e-04.
training: bce: 0.019130, dice: 0.064296, loss: 0.041713
training IoU in current batch 14400 is 0.9522462217928369
training IoU uptillnow 14400 is 0.001390357171919634
testing: bce: 10.595980, dice: 35.612516, loss: 23.104248
IoU in current test batch is 0.8847428479715324
Epoch 46971: reducing learning rate of group 0 to 6.3430e-04.
training: bce: 0.019111, dice: 0.064242, loss: 0.041676
training IoU in current batch 14500 is 0.7975650822432327
training IoU uptillnow 14500 is 0.0013958778857195606
testing: bce: 10.658604, dice: 35.829589, loss: 23.244096
IoU in current test batch is 0.9010225094209431
Epoch 47072: reducing learning rate of group 0 to 6.3367e-04.
training: bce: 0.019125, dice: 0.064273, loss: 0.041699
training IoU in current batch 14600 is 0.9497489439706703
training IoU uptillnow 14600 is 0.0014029890254646986
testing: bce: 10.740235, dice: 36.094322, loss: 23.417278
IoU in current test batch is 0.8999875880079335
Epoch 47173: reducing learning rate of group 0 to 6.3304e-04.
training: bce: 0.019097, dice: 0.064258, loss: 0.041678
training IoU in current batch 14700 is 0.819530780028073
training IoU uptillnow 14700 is 0.0014086920802582426
testing: bce: 10.797906, dice: 36.333108, loss: 23.565507
IoU in current test batch is 0.8910686534959597
Epoch 47274: reducing learning rate of group 0 to 6.3240e-04.
training: bce: 0.019075, dice: 0.064260, loss: 0.041668
training IoU in current batch 14800 is 0.8387212161860049
training IoU uptillnow 14800 is 0.0014145736925258944
testing: bce: 10.859058, dice: 36.581363, loss: 23.720211
IoU in current test batch is 0.9020456094338928
Epoch 47375: reducing learning rate of group 0 to 6.3177e-04.
training: bce: 0.019047, dice: 0.064245, loss: 0.041646
training IoU in current batch 14900 is 0.915620410290425
training IoU uptillnow 14900 is 0.0014212408404598582
testing: bce: 10.916073, dice: 36.819876, loss: 23.867974
IoU in current test batch is 0.8962417251980573
Epoch 47476: reducing learning rate of group 0 to 6.3114e-04.
training: bce: 0.019013, dice: 0.064181, loss: 0.041597
training IoU in current batch 15000 is 0.7992258211419138
training IoU uptillnow 15000 is 0.0014266560146788292
testing: bce: 10.969994, dice: 37.030169, loss: 24.000081
IoU in current test batch is 0.9055709737740473
Epoch 47577: reducing learning rate of group 0 to 6.3051e-04.
training: bce: 0.018988, dice: 0.064117, loss: 0.041552
training IoU in current batch 15100 is 0.9130227104954622
training IoU uptillnow 15100 is 0.0014332425634102923
testing: bce: 11.028263, dice: 37.239748, loss: 24.134005
IoU in current test batch is 0.8941880759704589
Epoch 47678: reducing learning rate of group 0 to 6.2988e-04.
training: bce: 0.018957, dice: 0.064060, loss: 0.041508
training IoU in current batch 15200 is 0.853071226892547
training IoU uptillnow 15200 is 0.001439173753414486
testing: bce: 11.083134, dice: 37.452981, loss: 24.268058
IoU in current test batch is 0.8677741520744877
Epoch 47779: reducing learning rate of group 0 to 6.2925e-04.
training: bce: 0.018930, dice: 0.063983, loss: 0.041457
training IoU in current batch 15300 is 0.9501471206389239
training IoU uptillnow 15300 is 0.0014460945401516098
testing: bce: 11.140589, dice: 37.654225, loss: 24.397407
IoU in current test batch is 0.9133181067376117
Epoch 47880: reducing learning rate of group 0 to 6.2862e-04.
training: bce: 0.018951, dice: 0.063965, loss: 0.041458
training IoU in current batch 15400 is 0.9179754182314783
training IoU uptillnow 15400 is 0.0014526509850592852
testing: bce: 11.225792, dice: 37.889278, loss: 24.557535
IoU in current test batch is 0.8852515897088499
Epoch 47981: reducing learning rate of group 0 to 6.2799e-04.
training: bce: 0.018931, dice: 0.063891, loss: 0.041411
training IoU in current batch 15500 is 0.9183354533890605
training IoU uptillnow 15500 is 0.0014591838860923574
testing: bce: 11.286662, dice: 38.091473, loss: 24.689068
IoU in current test batch is 0.8877013805105278
Epoch 48082: reducing learning rate of group 0 to 6.2736e-04.
training: bce: 0.018936, dice: 0.063908, loss: 0.041422
training IoU in current batch 15600 is 0.9441225494086832
training IoU uptillnow 15600 is 0.0014659574327770607
testing: bce: 11.362263, dice: 38.347278, loss: 24.854770
IoU in current test batch is 0.8734660564441643
Epoch 48183: reducing learning rate of group 0 to 6.2673e-04.
training: bce: 0.018900, dice: 0.063830, loss: 0.041365
training IoU in current batch 15700 is 0.9556789069171648
training IoU uptillnow 15700 is 0.0014728226585344441
testing: bce: 11.413267, dice: 38.545666, loss: 24.979466
IoU in current test batch is 0.9026149986188005
Epoch 48284: reducing learning rate of group 0 to 6.2611e-04.
training: bce: 0.018870, dice: 0.063774, loss: 0.041322
training IoU in current batch 15800 is 0.7958982857734735
training IoU uptillnow 15800 is 0.001478007135665196
testing: bce: 11.467917, dice: 38.757136, loss: 25.112526
IoU in current test batch is 0.8735274006837985
Epoch 48385: reducing learning rate of group 0 to 6.2548e-04.
training: bce: 0.018878, dice: 0.063753, loss: 0.041316
training IoU in current batch 15900 is 0.9387386695344907
training IoU uptillnow 15900 is 0.0014846443274050638
testing: bce: 11.545261, dice: 38.990012, loss: 25.267636
IoU in current test batch is 0.9081089329715453
Epoch 48486: reducing learning rate of group 0 to 6.2486e-04.
training: bce: 0.018863, dice: 0.063776, loss: 0.041320
training IoU in current batch 16000 is 0.948314708002884
training IoU uptillnow 16000 is 0.0014913527985790397
testing: bce: 11.608779, dice: 39.249466, loss: 25.429122
IoU in current test batch is 0.9087389949032628
Epoch 48587: reducing learning rate of group 0 to 6.2423e-04.
training: bce: 0.018835, dice: 0.063727, loss: 0.041281
training IoU in current batch 16100 is 0.9302877628136809
training IoU uptillnow 16100 is 0.0014978484172708854
testing: bce: 11.664104, dice: 39.464053, loss: 25.564078
IoU in current test batch is 0.895603709515127
Epoch 48721: reducing learning rate of group 0 to 6.2361e-04.
training: bce: 0.018814, dice: 0.063702, loss: 0.041258
training IoU in current batch 16200 is 0.8947794607983234
training IoU uptillnow 16200 is 0.0015039531955490643
testing: bce: 11.723603, dice: 39.693586, loss: 25.708594
IoU in current test batch is 0.8999512778934148
Epoch 48822: reducing learning rate of group 0 to 6.2298e-04.
training: bce: 0.018828, dice: 0.063755, loss: 0.041291
training IoU in current batch 16300 is 0.9016232971694266
training IoU uptillnow 16300 is 0.001510103029816146
testing: bce: 11.804208, dice: 39.971869, loss: 25.888039
IoU in current test batch is 0.9044071526911888
Epoch 48923: reducing learning rate of group 0 to 6.2236e-04.
training: bce: 0.018799, dice: 0.063681, loss: 0.041240
training IoU in current batch 16400 is 0.9522656378263011
training IoU uptillnow 16400 is 0.0015167450285277068
testing: bce: 11.858821, dice: 40.170315, loss: 26.014568
IoU in current test batch is 0.9023668574548099
Epoch 49024: reducing learning rate of group 0 to 6.2174e-04.
training: bce: 0.018777, dice: 0.063629, loss: 0.041203
training IoU in current batch 16500 is 0.9091788561256868
training IoU uptillnow 16500 is 0.0015229207270610267
testing: bce: 11.916619, dice: 40.382390, loss: 26.149505
IoU in current test batch is 0.8991352088036345
Epoch 49125: reducing learning rate of group 0 to 6.2112e-04.
training: bce: 0.018748, dice: 0.063592, loss: 0.041170
training IoU in current batch 16600 is 0.9401640984590754
training IoU uptillnow 16600 is 0.0015293865095516608
testing: bce: 11.970770, dice: 40.603274, loss: 26.287022
IoU in current test batch is 0.8984024062875716
Epoch 49226: reducing learning rate of group 0 to 6.2049e-04.
training: bce: 0.018824, dice: 0.063579, loss: 0.041201
training IoU in current batch 16700 is 0.9239720034995625
training IoU uptillnow 16700 is 0.0015356616463712146
testing: bce: 12.091411, dice: 40.839744, loss: 26.465577
IoU in current test batch is 0.9091709987345037
Epoch 49327: reducing learning rate of group 0 to 6.1987e-04.
training: bce: 0.018796, dice: 0.063533, loss: 0.041164
training IoU in current batch 16800 is 0.9286113699906804
training IoU uptillnow 16800 is 0.001541958356983444
testing: bce: 12.145602, dice: 41.054623, loss: 26.600112
IoU in current test batch is 0.9032716824644447
Epoch 49428: reducing learning rate of group 0 to 6.1925e-04.
training: bce: 0.018777, dice: 0.063495, loss: 0.041136
training IoU in current batch 16900 is 0.951877546525713
training IoU uptillnow 16900 is 0.0015484648522475926
testing: bce: 12.205581, dice: 41.274251, loss: 26.739916
IoU in current test batch is 0.8626120231961263
Epoch 49529: reducing learning rate of group 0 to 6.1863e-04.
training: bce: 0.018802, dice: 0.063485, loss: 0.041144
training IoU in current batch 17000 is 0.945532866669428
training IoU uptillnow 17000 is 0.0015548810612529297
testing: bce: 12.294450, dice: 41.512146, loss: 26.903298
IoU in current test batch is 0.9006179705760609
Epoch 49630: reducing learning rate of group 0 to 6.1802e-04.
training: bce: 0.018816, dice: 0.063523, loss: 0.041170
training IoU in current batch 17100 is 0.8008114889754951
training IoU uptillnow 17100 is 0.0015598139938778797
testing: bce: 12.375627, dice: 41.781279, loss: 27.078453
IoU in current test batch is 0.8799424545787691
Epoch 49731: reducing learning rate of group 0 to 6.1740e-04.
training: bce: 0.018787, dice: 0.063462, loss: 0.041125
training IoU in current batch 17200 is 0.8508732091986293
training IoU uptillnow 17200 is 0.0015652302333418245
testing: bce: 12.429278, dice: 41.985266, loss: 27.207272
IoU in current test batch is 0.8825423272322369
Epoch 49832: reducing learning rate of group 0 to 6.1678e-04.
training: bce: 0.018766, dice: 0.063432, loss: 0.041099
training IoU in current batch 17300 is 0.934785549993235
training IoU uptillnow 17300 is 0.0015714663992344097
testing: bce: 12.487250, dice: 42.209115, loss: 27.348183
IoU in current test batch is 0.9108897460249599
Epoch 49933: reducing learning rate of group 0 to 6.1616e-04.
training: bce: 0.018778, dice: 0.063421, loss: 0.041099
training IoU in current batch 17400 is 0.9537506650115268
training IoU uptillnow 17400 is 0.0015778674381353463
testing: bce: 12.567620, dice: 42.445560, loss: 27.506590
IoU in current test batch is 0.8879714688075058
Maximum training samples requirement meet, I have been training for more than  100001  samples.
Making network now
ResNetUNet(
  (base_model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer0_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer1_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2_1x1): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3_1x1): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4_1x1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (upsample): Upsample(scale_factor=2.0, mode=bilinear)
  (conv_up3): Sequential(
    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up2): Sequential(
    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up1): Sequential(
    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up0): Sequential(
    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size2): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 512, 512]           1,792
              ReLU-2         [-1, 64, 512, 512]               0
            Conv2d-3         [-1, 64, 512, 512]          36,928
              ReLU-4         [-1, 64, 512, 512]               0
            Conv2d-5         [-1, 64, 256, 256]           9,408
            Conv2d-6         [-1, 64, 256, 256]           9,408
       BatchNorm2d-7         [-1, 64, 256, 256]             128
       BatchNorm2d-8         [-1, 64, 256, 256]             128
              ReLU-9         [-1, 64, 256, 256]               0
             ReLU-10         [-1, 64, 256, 256]               0
        MaxPool2d-11         [-1, 64, 128, 128]               0
        MaxPool2d-12         [-1, 64, 128, 128]               0
           Conv2d-13         [-1, 64, 128, 128]          36,864
           Conv2d-14         [-1, 64, 128, 128]          36,864
      BatchNorm2d-15         [-1, 64, 128, 128]             128
      BatchNorm2d-16         [-1, 64, 128, 128]             128
             ReLU-17         [-1, 64, 128, 128]               0
             ReLU-18         [-1, 64, 128, 128]               0
           Conv2d-19         [-1, 64, 128, 128]          36,864
           Conv2d-20         [-1, 64, 128, 128]          36,864
      BatchNorm2d-21         [-1, 64, 128, 128]             128
      BatchNorm2d-22         [-1, 64, 128, 128]             128
             ReLU-23         [-1, 64, 128, 128]               0
             ReLU-24         [-1, 64, 128, 128]               0
       BasicBlock-25         [-1, 64, 128, 128]               0
       BasicBlock-26         [-1, 64, 128, 128]               0
           Conv2d-27         [-1, 64, 128, 128]          36,864
           Conv2d-28         [-1, 64, 128, 128]          36,864
      BatchNorm2d-29         [-1, 64, 128, 128]             128
      BatchNorm2d-30         [-1, 64, 128, 128]             128
             ReLU-31         [-1, 64, 128, 128]               0
             ReLU-32         [-1, 64, 128, 128]               0
           Conv2d-33         [-1, 64, 128, 128]          36,864
           Conv2d-34         [-1, 64, 128, 128]          36,864
      BatchNorm2d-35         [-1, 64, 128, 128]             128
      BatchNorm2d-36         [-1, 64, 128, 128]             128
             ReLU-37         [-1, 64, 128, 128]               0
             ReLU-38         [-1, 64, 128, 128]               0
       BasicBlock-39         [-1, 64, 128, 128]               0
       BasicBlock-40         [-1, 64, 128, 128]               0
           Conv2d-41         [-1, 64, 128, 128]          36,864
           Conv2d-42         [-1, 64, 128, 128]          36,864
      BatchNorm2d-43         [-1, 64, 128, 128]             128
      BatchNorm2d-44         [-1, 64, 128, 128]             128
             ReLU-45         [-1, 64, 128, 128]               0
             ReLU-46         [-1, 64, 128, 128]               0
           Conv2d-47         [-1, 64, 128, 128]          36,864
           Conv2d-48         [-1, 64, 128, 128]          36,864
      BatchNorm2d-49         [-1, 64, 128, 128]             128
      BatchNorm2d-50         [-1, 64, 128, 128]             128
             ReLU-51         [-1, 64, 128, 128]               0
             ReLU-52         [-1, 64, 128, 128]               0
       BasicBlock-53         [-1, 64, 128, 128]               0
       BasicBlock-54         [-1, 64, 128, 128]               0
           Conv2d-55          [-1, 128, 64, 64]          73,728
           Conv2d-56          [-1, 128, 64, 64]          73,728
      BatchNorm2d-57          [-1, 128, 64, 64]             256
      BatchNorm2d-58          [-1, 128, 64, 64]             256
             ReLU-59          [-1, 128, 64, 64]               0
             ReLU-60          [-1, 128, 64, 64]               0
           Conv2d-61          [-1, 128, 64, 64]         147,456
           Conv2d-62          [-1, 128, 64, 64]         147,456
      BatchNorm2d-63          [-1, 128, 64, 64]             256
      BatchNorm2d-64          [-1, 128, 64, 64]             256
           Conv2d-65          [-1, 128, 64, 64]           8,192
           Conv2d-66          [-1, 128, 64, 64]           8,192
      BatchNorm2d-67          [-1, 128, 64, 64]             256
      BatchNorm2d-68          [-1, 128, 64, 64]             256
             ReLU-69          [-1, 128, 64, 64]               0
             ReLU-70          [-1, 128, 64, 64]               0
       BasicBlock-71          [-1, 128, 64, 64]               0
       BasicBlock-72          [-1, 128, 64, 64]               0
           Conv2d-73          [-1, 128, 64, 64]         147,456
           Conv2d-74          [-1, 128, 64, 64]         147,456
      BatchNorm2d-75          [-1, 128, 64, 64]             256
      BatchNorm2d-76          [-1, 128, 64, 64]             256
             ReLU-77          [-1, 128, 64, 64]               0
             ReLU-78          [-1, 128, 64, 64]               0
           Conv2d-79          [-1, 128, 64, 64]         147,456
           Conv2d-80          [-1, 128, 64, 64]         147,456
      BatchNorm2d-81          [-1, 128, 64, 64]             256
      BatchNorm2d-82          [-1, 128, 64, 64]             256
             ReLU-83          [-1, 128, 64, 64]               0
             ReLU-84          [-1, 128, 64, 64]               0
       BasicBlock-85          [-1, 128, 64, 64]               0
       BasicBlock-86          [-1, 128, 64, 64]               0
           Conv2d-87          [-1, 128, 64, 64]         147,456
           Conv2d-88          [-1, 128, 64, 64]         147,456
      BatchNorm2d-89          [-1, 128, 64, 64]             256
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
             ReLU-92          [-1, 128, 64, 64]               0
           Conv2d-93          [-1, 128, 64, 64]         147,456
           Conv2d-94          [-1, 128, 64, 64]         147,456
      BatchNorm2d-95          [-1, 128, 64, 64]             256
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
             ReLU-98          [-1, 128, 64, 64]               0
       BasicBlock-99          [-1, 128, 64, 64]               0
      BasicBlock-100          [-1, 128, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]         147,456
          Conv2d-102          [-1, 128, 64, 64]         147,456
     BatchNorm2d-103          [-1, 128, 64, 64]             256
     BatchNorm2d-104          [-1, 128, 64, 64]             256
            ReLU-105          [-1, 128, 64, 64]               0
            ReLU-106          [-1, 128, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]         147,456
          Conv2d-108          [-1, 128, 64, 64]         147,456
     BatchNorm2d-109          [-1, 128, 64, 64]             256
     BatchNorm2d-110          [-1, 128, 64, 64]             256
            ReLU-111          [-1, 128, 64, 64]               0
            ReLU-112          [-1, 128, 64, 64]               0
      BasicBlock-113          [-1, 128, 64, 64]               0
      BasicBlock-114          [-1, 128, 64, 64]               0
          Conv2d-115          [-1, 256, 32, 32]         294,912
          Conv2d-116          [-1, 256, 32, 32]         294,912
     BatchNorm2d-117          [-1, 256, 32, 32]             512
     BatchNorm2d-118          [-1, 256, 32, 32]             512
            ReLU-119          [-1, 256, 32, 32]               0
            ReLU-120          [-1, 256, 32, 32]               0
          Conv2d-121          [-1, 256, 32, 32]         589,824
          Conv2d-122          [-1, 256, 32, 32]         589,824
     BatchNorm2d-123          [-1, 256, 32, 32]             512
     BatchNorm2d-124          [-1, 256, 32, 32]             512
          Conv2d-125          [-1, 256, 32, 32]          32,768
          Conv2d-126          [-1, 256, 32, 32]          32,768
     BatchNorm2d-127          [-1, 256, 32, 32]             512
     BatchNorm2d-128          [-1, 256, 32, 32]             512
            ReLU-129          [-1, 256, 32, 32]               0
            ReLU-130          [-1, 256, 32, 32]               0
      BasicBlock-131          [-1, 256, 32, 32]               0
      BasicBlock-132          [-1, 256, 32, 32]               0
          Conv2d-133          [-1, 256, 32, 32]         589,824
          Conv2d-134          [-1, 256, 32, 32]         589,824
     BatchNorm2d-135          [-1, 256, 32, 32]             512
     BatchNorm2d-136          [-1, 256, 32, 32]             512
            ReLU-137          [-1, 256, 32, 32]               0
            ReLU-138          [-1, 256, 32, 32]               0
          Conv2d-139          [-1, 256, 32, 32]         589,824
          Conv2d-140          [-1, 256, 32, 32]         589,824
     BatchNorm2d-141          [-1, 256, 32, 32]             512
     BatchNorm2d-142          [-1, 256, 32, 32]             512
            ReLU-143          [-1, 256, 32, 32]               0
            ReLU-144          [-1, 256, 32, 32]               0
      BasicBlock-145          [-1, 256, 32, 32]               0
      BasicBlock-146          [-1, 256, 32, 32]               0
          Conv2d-147          [-1, 256, 32, 32]         589,824
          Conv2d-148          [-1, 256, 32, 32]         589,824
     BatchNorm2d-149          [-1, 256, 32, 32]             512
     BatchNorm2d-150          [-1, 256, 32, 32]             512
            ReLU-151          [-1, 256, 32, 32]               0
            ReLU-152          [-1, 256, 32, 32]               0
          Conv2d-153          [-1, 256, 32, 32]         589,824
          Conv2d-154          [-1, 256, 32, 32]         589,824
     BatchNorm2d-155          [-1, 256, 32, 32]             512
     BatchNorm2d-156          [-1, 256, 32, 32]             512
            ReLU-157          [-1, 256, 32, 32]               0
            ReLU-158          [-1, 256, 32, 32]               0
      BasicBlock-159          [-1, 256, 32, 32]               0
      BasicBlock-160          [-1, 256, 32, 32]               0
          Conv2d-161          [-1, 256, 32, 32]         589,824
          Conv2d-162          [-1, 256, 32, 32]         589,824
     BatchNorm2d-163          [-1, 256, 32, 32]             512
     BatchNorm2d-164          [-1, 256, 32, 32]             512
            ReLU-165          [-1, 256, 32, 32]               0
            ReLU-166          [-1, 256, 32, 32]               0
          Conv2d-167          [-1, 256, 32, 32]         589,824
          Conv2d-168          [-1, 256, 32, 32]         589,824
     BatchNorm2d-169          [-1, 256, 32, 32]             512
     BatchNorm2d-170          [-1, 256, 32, 32]             512
            ReLU-171          [-1, 256, 32, 32]               0
            ReLU-172          [-1, 256, 32, 32]               0
      BasicBlock-173          [-1, 256, 32, 32]               0
      BasicBlock-174          [-1, 256, 32, 32]               0
          Conv2d-175          [-1, 256, 32, 32]         589,824
          Conv2d-176          [-1, 256, 32, 32]         589,824
     BatchNorm2d-177          [-1, 256, 32, 32]             512
     BatchNorm2d-178          [-1, 256, 32, 32]             512
            ReLU-179          [-1, 256, 32, 32]               0
            ReLU-180          [-1, 256, 32, 32]               0
          Conv2d-181          [-1, 256, 32, 32]         589,824
          Conv2d-182          [-1, 256, 32, 32]         589,824
     BatchNorm2d-183          [-1, 256, 32, 32]             512
     BatchNorm2d-184          [-1, 256, 32, 32]             512
            ReLU-185          [-1, 256, 32, 32]               0
            ReLU-186          [-1, 256, 32, 32]               0
      BasicBlock-187          [-1, 256, 32, 32]               0
      BasicBlock-188          [-1, 256, 32, 32]               0
          Conv2d-189          [-1, 256, 32, 32]         589,824
          Conv2d-190          [-1, 256, 32, 32]         589,824
     BatchNorm2d-191          [-1, 256, 32, 32]             512
     BatchNorm2d-192          [-1, 256, 32, 32]             512
            ReLU-193          [-1, 256, 32, 32]               0
            ReLU-194          [-1, 256, 32, 32]               0
          Conv2d-195          [-1, 256, 32, 32]         589,824
          Conv2d-196          [-1, 256, 32, 32]         589,824
     BatchNorm2d-197          [-1, 256, 32, 32]             512
     BatchNorm2d-198          [-1, 256, 32, 32]             512
            ReLU-199          [-1, 256, 32, 32]               0
            ReLU-200          [-1, 256, 32, 32]               0
      BasicBlock-201          [-1, 256, 32, 32]               0
      BasicBlock-202          [-1, 256, 32, 32]               0
          Conv2d-203          [-1, 512, 16, 16]       1,179,648
          Conv2d-204          [-1, 512, 16, 16]       1,179,648
     BatchNorm2d-205          [-1, 512, 16, 16]           1,024
     BatchNorm2d-206          [-1, 512, 16, 16]           1,024
            ReLU-207          [-1, 512, 16, 16]               0
            ReLU-208          [-1, 512, 16, 16]               0
          Conv2d-209          [-1, 512, 16, 16]       2,359,296
          Conv2d-210          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-211          [-1, 512, 16, 16]           1,024
     BatchNorm2d-212          [-1, 512, 16, 16]           1,024
          Conv2d-213          [-1, 512, 16, 16]         131,072
          Conv2d-214          [-1, 512, 16, 16]         131,072
     BatchNorm2d-215          [-1, 512, 16, 16]           1,024
     BatchNorm2d-216          [-1, 512, 16, 16]           1,024
            ReLU-217          [-1, 512, 16, 16]               0
            ReLU-218          [-1, 512, 16, 16]               0
      BasicBlock-219          [-1, 512, 16, 16]               0
      BasicBlock-220          [-1, 512, 16, 16]               0
          Conv2d-221          [-1, 512, 16, 16]       2,359,296
          Conv2d-222          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-223          [-1, 512, 16, 16]           1,024
     BatchNorm2d-224          [-1, 512, 16, 16]           1,024
            ReLU-225          [-1, 512, 16, 16]               0
            ReLU-226          [-1, 512, 16, 16]               0
          Conv2d-227          [-1, 512, 16, 16]       2,359,296
          Conv2d-228          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-229          [-1, 512, 16, 16]           1,024
     BatchNorm2d-230          [-1, 512, 16, 16]           1,024
            ReLU-231          [-1, 512, 16, 16]               0
            ReLU-232          [-1, 512, 16, 16]               0
      BasicBlock-233          [-1, 512, 16, 16]               0
      BasicBlock-234          [-1, 512, 16, 16]               0
          Conv2d-235          [-1, 512, 16, 16]       2,359,296
          Conv2d-236          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-237          [-1, 512, 16, 16]           1,024
     BatchNorm2d-238          [-1, 512, 16, 16]           1,024
            ReLU-239          [-1, 512, 16, 16]               0
            ReLU-240          [-1, 512, 16, 16]               0
          Conv2d-241          [-1, 512, 16, 16]       2,359,296
          Conv2d-242          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-243          [-1, 512, 16, 16]           1,024
     BatchNorm2d-244          [-1, 512, 16, 16]           1,024
            ReLU-245          [-1, 512, 16, 16]               0
            ReLU-246          [-1, 512, 16, 16]               0
      BasicBlock-247          [-1, 512, 16, 16]               0
      BasicBlock-248          [-1, 512, 16, 16]               0
          Conv2d-249          [-1, 512, 16, 16]         262,656
            ReLU-250          [-1, 512, 16, 16]               0
        Upsample-251          [-1, 512, 32, 32]               0
          Conv2d-252          [-1, 256, 32, 32]          65,792
            ReLU-253          [-1, 256, 32, 32]               0
          Conv2d-254          [-1, 512, 32, 32]       3,539,456
            ReLU-255          [-1, 512, 32, 32]               0
        Upsample-256          [-1, 512, 64, 64]               0
          Conv2d-257          [-1, 128, 64, 64]          16,512
            ReLU-258          [-1, 128, 64, 64]               0
          Conv2d-259          [-1, 256, 64, 64]       1,474,816
            ReLU-260          [-1, 256, 64, 64]               0
        Upsample-261        [-1, 256, 128, 128]               0
          Conv2d-262         [-1, 64, 128, 128]           4,160
            ReLU-263         [-1, 64, 128, 128]               0
          Conv2d-264        [-1, 256, 128, 128]         737,536
            ReLU-265        [-1, 256, 128, 128]               0
        Upsample-266        [-1, 256, 256, 256]               0
          Conv2d-267         [-1, 64, 256, 256]           4,160
            ReLU-268         [-1, 64, 256, 256]               0
          Conv2d-269        [-1, 128, 256, 256]         368,768
            ReLU-270        [-1, 128, 256, 256]               0
        Upsample-271        [-1, 128, 512, 512]               0
          Conv2d-272         [-1, 64, 512, 512]         110,656
            ReLU-273         [-1, 64, 512, 512]               0
          Conv2d-274          [-1, 1, 512, 512]              65
================================================================
Total params: 49,192,641
Trainable params: 49,192,641
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 2522.00
Params size (MB): 187.66
Estimated Total Size (MB): 2712.66
----------------------------------------------------------------
Start training now...
training: bce: 0.627379, dice: 0.931226, loss: 0.627379
training IoU in current batch 0 is 0.0547941712668443
training IoU uptillnow 0 is 0.02739708563342215
testing: bce: 0.024130, dice: 0.035816, loss: 0.024130
IoU in current test batch is 0.0
training: bce: 2.196059, dice: 0.621260, loss: 2.196059
training IoU in current batch 100 is 0.21735452562218957
training IoU uptillnow 100 is 0.0013472707766783855
testing: bce: 8.530843, dice: 2.413356, loss: 8.530843
IoU in current test batch is 0.6721182617474801
training: bce: 1.127724, dice: 0.451381, loss: 1.127724
training IoU in current batch 200 is 0.8025237817899437
training IoU uptillnow 200 is 0.0026733146235795456
testing: bce: 8.718174, dice: 3.489521, loss: 8.718174
IoU in current test batch is 0.802371804489523
training: bce: 0.759706, dice: 0.348390, loss: 0.759706
training IoU in current batch 300 is 0.8939182994329179
training IoU uptillnow 300 is 0.0032700843490230816
testing: bce: 8.795061, dice: 4.033287, loss: 8.795061
IoU in current test batch is 0.812888715902707
training: bce: 0.574697, dice: 0.291411, loss: 0.574697
training IoU in current batch 400 is 0.8931220807766279
training IoU uptillnow 400 is 0.0035682205223048916
testing: bce: 8.863596, dice: 4.494452, loss: 8.863596
IoU in current test batch is 0.8208710299687172
Epoch   426: reducing learning rate of group 0 to 9.9900e-04.
training: bce: 0.463411, dice: 0.258458, loss: 0.463411
training IoU in current batch 500 is 0.8111716507923304
training IoU uptillnow 500 is 0.0036655534028751037
testing: bce: 8.929565, dice: 4.980290, loss: 8.929565
IoU in current test batch is 0.8034386391549672
Epoch   527: reducing learning rate of group 0 to 9.9800e-04.
training: bce: 0.458672, dice: 0.267507, loss: 0.458672
training IoU in current batch 600 is 0.0
training IoU uptillnow 600 is 0.0030556443508160183
testing: bce: 10.602385, dice: 6.183519, loss: 10.602385
IoU in current test batch is 0.0
Epoch   628: reducing learning rate of group 0 to 9.9700e-04.
training: bce: 0.448939, dice: 0.358826, loss: 0.448939
training IoU in current batch 700 is 0.0
training IoU uptillnow 700 is 0.0026197464405712224
testing: bce: 12.104080, dice: 9.674490, loss: 12.104080
IoU in current test batch is 0.0
Epoch   729: reducing learning rate of group 0 to 9.9601e-04.
training: bce: 0.434727, dice: 0.424650, loss: 0.434727
training IoU in current batch 800 is 0.0
training IoU uptillnow 800 is 0.00229268695985072
testing: bce: 13.392942, dice: 13.082488, loss: 13.392942
IoU in current test batch is 0.0
Epoch   830: reducing learning rate of group 0 to 9.9501e-04.
training: bce: 0.427471, dice: 0.476655, loss: 0.427471
training IoU in current batch 900 is 0.0
training IoU uptillnow 900 is 0.002038226697936101
testing: bce: 14.813526, dice: 16.517928, loss: 14.813526
IoU in current test batch is 0.0
Epoch   931: reducing learning rate of group 0 to 9.9401e-04.
training: bce: 0.431683, dice: 0.514119, loss: 0.431683
training IoU in current batch 1000 is 0.0
training IoU uptillnow 1000 is 0.0018346076471932337
testing: bce: 16.619784, dice: 19.793584, loss: 16.619784
IoU in current test batch is 0.0
Epoch  1032: reducing learning rate of group 0 to 9.9302e-04.
training: bce: 0.417122, dice: 0.541879, loss: 0.417122
training IoU in current batch 1100 is 0.0
training IoU uptillnow 1100 is 0.0016679766165671453
testing: bce: 17.663529, dice: 22.946481, loss: 17.663529
IoU in current test batch is 0.0
Epoch  1133: reducing learning rate of group 0 to 9.9203e-04.
training: bce: 0.396528, dice: 0.562692, loss: 0.396528
training IoU in current batch 1200 is 0.0
training IoU uptillnow 1200 is 0.0015290943004499808
testing: bce: 18.316537, dice: 25.992049, loss: 18.316537
IoU in current test batch is 0.0
Epoch  1234: reducing learning rate of group 0 to 9.9104e-04.
training: bce: 0.378080, dice: 0.580626, loss: 0.378080
training IoU in current batch 1300 is 0.0
training IoU uptillnow 1300 is 0.0014115620713608201
testing: bce: 18.918526, dice: 29.053625, loss: 18.918526
IoU in current test batch is 0.0
Epoch  1335: reducing learning rate of group 0 to 9.9004e-04.
training: bce: 0.361604, dice: 0.594980, loss: 0.361604
training IoU in current batch 1400 is 0.0
training IoU uptillnow 1400 is 0.0013108081761887414
testing: bce: 19.484881, dice: 32.060270, loss: 19.484881
IoU in current test batch is 0.0
Epoch  1436: reducing learning rate of group 0 to 9.8905e-04.
training: bce: 0.346736, dice: 0.607472, loss: 0.346736
training IoU in current batch 1500 is 0.0
training IoU uptillnow 1500 is 0.0012234791837711038
testing: bce: 20.017344, dice: 35.069803, loss: 20.017344
IoU in current test batch is 0.0
Epoch  1537: reducing learning rate of group 0 to 9.8807e-04.
training: bce: 0.334439, dice: 0.619324, loss: 0.334439
training IoU in current batch 1600 is 0.0
training IoU uptillnow 1600 is 0.0011470594970895858
testing: bce: 20.593706, dice: 38.136051, loss: 20.593706
IoU in current test batch is 0.0
Epoch  1638: reducing learning rate of group 0 to 9.8708e-04.
training: bce: 0.326923, dice: 0.630932, loss: 0.326923
training IoU in current batch 1700 is 0.0
training IoU uptillnow 1700 is 0.0010796250763318207
testing: bce: 21.388282, dice: 41.277516, loss: 21.388282
IoU in current test batch is 0.0
Epoch  1739: reducing learning rate of group 0 to 9.8609e-04.
training: bce: 0.323250, dice: 0.642789, loss: 0.323250
training IoU in current batch 1800 is 0.0
training IoU uptillnow 1800 is 0.0010196792086843015
testing: bce: 22.391275, dice: 44.525517, loss: 22.391275
IoU in current test batch is 0.0
Epoch  1840: reducing learning rate of group 0 to 9.8510e-04.
training: bce: 0.311320, dice: 0.648535, loss: 0.311320
training IoU in current batch 1900 is 0.0
training IoU uptillnow 1900 is 0.0009660401130144276
testing: bce: 22.762283, dice: 47.417878, loss: 22.762283
IoU in current test batch is 0.0
Epoch  1941: reducing learning rate of group 0 to 9.8412e-04.
training: bce: 0.299734, dice: 0.650500, loss: 0.299734
training IoU in current batch 2000 is 0.0
training IoU uptillnow 2000 is 0.0009177622462970649
testing: bce: 23.067957, dice: 50.063482, loss: 23.067957
IoU in current test batch is 5.300029976866585e-06
Epoch  2042: reducing learning rate of group 0 to 9.8314e-04.
training: bce: 0.290850, dice: 0.653745, loss: 0.290850
training IoU in current batch 2100 is 0.0
training IoU uptillnow 2100 is 0.0008740800832177187
testing: bce: 23.502938, dice: 52.827602, loss: 23.502938
IoU in current test batch is 0.0
Epoch  2143: reducing learning rate of group 0 to 9.8215e-04.
training: bce: 0.282344, dice: 0.657677, loss: 0.282344
training IoU in current batch 2200 is 0.0
training IoU uptillnow 2200 is 0.0008343672216449009
testing: bce: 23.901489, dice: 55.674889, loss: 23.901489
IoU in current test batch is 0.0
Epoch  2244: reducing learning rate of group 0 to 9.8117e-04.
training: bce: 0.273896, dice: 0.660569, loss: 0.273896
training IoU in current batch 2300 is 0.0
training IoU uptillnow 2300 is 0.0007981061516038361
testing: bce: 24.239771, dice: 58.460396, loss: 24.239771
IoU in current test batch is 0.0
Epoch  2345: reducing learning rate of group 0 to 9.8019e-04.
training: bce: 0.265966, dice: 0.662272, loss: 0.265966
training IoU in current batch 2400 is 0.0
training IoU uptillnow 2400 is 0.0007648655788589867
testing: bce: 24.560921, dice: 61.158296, loss: 24.560921
IoU in current test batch is 0.0
Epoch  2446: reducing learning rate of group 0 to 9.7921e-04.
training: bce: 0.261290, dice: 0.665825, loss: 0.261290
training IoU in current batch 2500 is 0.0
training IoU uptillnow 2500 is 0.0007342831886607065
testing: bce: 25.134048, dice: 64.047291, loss: 25.134048
IoU in current test batch is 0.0
Epoch  2547: reducing learning rate of group 0 to 9.7823e-04.
training: bce: 0.255161, dice: 0.668941, loss: 0.255161
training IoU in current batch 2600 is 0.0
training IoU uptillnow 2600 is 0.0007060523855595644
testing: bce: 25.525882, dice: 66.919864, loss: 25.525882
IoU in current test batch is 0.0
Epoch  2648: reducing learning rate of group 0 to 9.7725e-04.
training: bce: 0.250838, dice: 0.673075, loss: 0.250838
training IoU in current batch 2700 is 0.0
training IoU uptillnow 2700 is 0.0006799119788376257
testing: bce: 26.058191, dice: 69.922167, loss: 26.058191
IoU in current test batch is 0.0
Epoch  2749: reducing learning rate of group 0 to 9.7627e-04.
training: bce: 0.244838, dice: 0.673415, loss: 0.244838
training IoU in current batch 2800 is 0.0
training IoU uptillnow 2800 is 0.0006556380774153613
testing: bce: 26.376627, dice: 72.547536, loss: 26.376627
IoU in current test batch is 0.0
Epoch  2850: reducing learning rate of group 0 to 9.7530e-04.
training: bce: 0.239350, dice: 0.672739, loss: 0.239350
training IoU in current batch 2900 is 0.0
training IoU uptillnow 2900 is 0.0006330376610963208
testing: bce: 26.705959, dice: 75.062159, loss: 26.705959
IoU in current test batch is 0.0
Epoch  2951: reducing learning rate of group 0 to 9.7432e-04.
training: bce: 0.233758, dice: 0.671762, loss: 0.233758
training IoU in current batch 3000 is 0.0
training IoU uptillnow 3000 is 0.0006119434371344309
testing: bce: 26.981109, dice: 77.536891, loss: 26.981109
IoU in current test batch is 0.0
Epoch  3052: reducing learning rate of group 0 to 9.7335e-04.
training: bce: 0.228444, dice: 0.670212, loss: 0.228444
training IoU in current batch 3100 is 0.0
training IoU uptillnow 3100 is 0.0005922096919833688
testing: bce: 27.246366, dice: 79.935660, loss: 27.246366
IoU in current test batch is 3.0410388188605226e-06
Epoch  3153: reducing learning rate of group 0 to 9.7237e-04.
training: bce: 0.223986, dice: 0.667331, loss: 0.223986
training IoU in current batch 3200 is 0.0
training IoU uptillnow 3200 is 0.0005737089205999459
testing: bce: 27.576092, dice: 82.158713, loss: 27.576092
IoU in current test batch is 0.0
Epoch  3254: reducing learning rate of group 0 to 9.7140e-04.
training: bce: 0.219278, dice: 0.665730, loss: 0.219278
training IoU in current batch 3300 is 0.0
training IoU uptillnow 3300 is 0.000556329068415761
testing: bce: 27.839909, dice: 84.522045, loss: 27.839909
IoU in current test batch is 0.0
Epoch  3355: reducing learning rate of group 0 to 9.7043e-04.
training: bce: 0.214995, dice: 0.663132, loss: 0.214995
training IoU in current batch 3400 is 0.0
training IoU uptillnow 3400 is 0.0005399712598766325
testing: bce: 28.123028, dice: 86.742818, loss: 28.123028
IoU in current test batch is 0.0
Epoch  3456: reducing learning rate of group 0 to 9.6946e-04.
training: bce: 0.210567, dice: 0.660463, loss: 0.210567
training IoU in current batch 3500 is 0.0
training IoU uptillnow 3500 is 0.0005245479162640465
testing: bce: 28.353715, dice: 88.933854, loss: 28.353715
IoU in current test batch is 8.640974166908496e-06
Epoch  3557: reducing learning rate of group 0 to 9.6849e-04.
training: bce: 0.206208, dice: 0.657398, loss: 0.206208
training IoU in current batch 3600 is 0.0
training IoU uptillnow 3600 is 0.0005099811871259169
testing: bce: 28.559798, dice: 91.049599, loss: 28.559798
IoU in current test batch is 0.0
Epoch  3658: reducing learning rate of group 0 to 9.6752e-04.
training: bce: 0.202092, dice: 0.653959, loss: 0.202092
training IoU in current batch 3700 is 0.0
training IoU uptillnow 3700 is 0.0004962016360011961
testing: bce: 28.767017, dice: 93.088490, loss: 28.767017
IoU in current test batch is 0.0
Epoch  3759: reducing learning rate of group 0 to 9.6656e-04.
training: bce: 0.200209, dice: 0.653921, loss: 0.200209
training IoU in current batch 3800 is 0.0
training IoU uptillnow 3800 is 0.00048314713360705783
testing: bce: 29.268967, dice: 95.598253, loss: 29.268967
IoU in current test batch is 0.0
Epoch  3860: reducing learning rate of group 0 to 9.6559e-04.
training: bce: 0.197414, dice: 0.654798, loss: 0.197414
training IoU in current batch 3900 is 0.0
training IoU uptillnow 3900 is 0.0004707619212613245
testing: bce: 29.619642, dice: 98.244854, loss: 29.619642
IoU in current test batch is 0.0
Epoch  3961: reducing learning rate of group 0 to 9.6462e-04.
training: bce: 0.194201, dice: 0.653664, loss: 0.194201
training IoU in current batch 4000 is 0.0
training IoU uptillnow 4000 is 0.0004589958147564176
testing: bce: 29.884477, dice: 100.588825, loss: 29.884477
IoU in current test batch is 0.0
Epoch  4062: reducing learning rate of group 0 to 9.6366e-04.
training: bce: 0.190741, dice: 0.650764, loss: 0.190741
training IoU in current batch 4100 is 0.0
training IoU uptillnow 4100 is 0.00044780352471115016
testing: bce: 30.085729, dice: 102.645485, loss: 30.085729
IoU in current test batch is 0.0
Epoch  4163: reducing learning rate of group 0 to 9.6269e-04.
training: bce: 0.188162, dice: 0.649982, loss: 0.188162
training IoU in current batch 4200 is 0.0
training IoU uptillnow 4200 is 0.0004371440739920083
testing: bce: 30.402689, dice: 105.022063, loss: 30.402689
IoU in current test batch is 0.0
Epoch  4264: reducing learning rate of group 0 to 9.6173e-04.
training: bce: 0.195209, dice: 0.651778, loss: 0.195209
training IoU in current batch 4300 is 0.0
training IoU uptillnow 4300 is 0.00042698029640558636
testing: bce: 32.292115, dice: 107.819063, loss: 32.292115
IoU in current test batch is 0.0
Epoch  4365: reducing learning rate of group 0 to 9.6077e-04.
training: bce: 0.193296, dice: 0.652459, loss: 0.193296
training IoU in current batch 4400 is 0.0
training IoU uptillnow 4400 is 0.00041727840373561166
testing: bce: 32.719100, dice: 110.441177, loss: 32.719100
IoU in current test batch is 0.0
Epoch  4466: reducing learning rate of group 0 to 9.5981e-04.
training: bce: 0.190882, dice: 0.652232, loss: 0.190882
training IoU in current batch 4500 is 0.0
training IoU uptillnow 4500 is 0.0004080076104955403
testing: bce: 33.044688, dice: 112.911458, loss: 33.044688
IoU in current test batch is 0.0
Epoch  4567: reducing learning rate of group 0 to 9.5885e-04.
training: bce: 0.188078, dice: 0.650609, loss: 0.188078
training IoU in current batch 4600 is 0.0
training IoU uptillnow 4600 is 0.0003991398076158285
testing: bce: 33.282497, dice: 115.132810, loss: 33.282497
IoU in current test batch is 0.0
Epoch  4668: reducing learning rate of group 0 to 9.5789e-04.
training: bce: 0.185327, dice: 0.648822, loss: 0.185327
training IoU in current batch 4700 is 0.0
training IoU uptillnow 4700 is 0.00039064927777928673
testing: bce: 33.508611, dice: 117.312059, loss: 33.508611
IoU in current test batch is 0.0
Epoch  4769: reducing learning rate of group 0 to 9.5693e-04.
training: bce: 0.183190, dice: 0.648150, loss: 0.183190
training IoU in current batch 4800 is 0.0
training IoU uptillnow 4800 is 0.00038251244633210306
testing: bce: 33.826677, dice: 119.683397, loss: 33.826677
IoU in current test batch is 0.0
Epoch  4870: reducing learning rate of group 0 to 9.5598e-04.
training: bce: 0.182087, dice: 0.648174, loss: 0.182087
training IoU in current batch 4900 is 0.0
training IoU uptillnow 4900 is 0.0003747076626893342
testing: bce: 34.323323, dice: 122.180848, loss: 34.323323
IoU in current test batch is 0.0
Epoch  4971: reducing learning rate of group 0 to 9.5502e-04.
training: bce: 0.179862, dice: 0.647008, loss: 0.179862
training IoU in current batch 5000 is 0.0
training IoU uptillnow 5000 is 0.0003672150079664921
testing: bce: 34.595720, dice: 124.449453, loss: 34.595720
IoU in current test batch is 0.0
Epoch  5072: reducing learning rate of group 0 to 9.5406e-04.
training: bce: 0.177544, dice: 0.645449, loss: 0.177544
training IoU in current batch 5100 is 0.0
training IoU uptillnow 5100 is 0.0003600161252382723
testing: bce: 34.832846, dice: 126.632126, loss: 34.832846
IoU in current test batch is 0.0
Epoch  5173: reducing learning rate of group 0 to 9.5311e-04.
training: bce: 0.175258, dice: 0.643760, loss: 0.175258
training IoU in current batch 5200 is 0.0
training IoU uptillnow 5200 is 0.0003530940693790477
testing: bce: 35.058355, dice: 128.776770, loss: 35.058355
IoU in current test batch is 0.0
Epoch  5274: reducing learning rate of group 0 to 9.5216e-04.
training: bce: 0.173416, dice: 0.642997, loss: 0.173416
training IoU in current batch 5300 is 0.0
training IoU uptillnow 5300 is 0.00034643317389934484
testing: bce: 35.356946, dice: 131.097232, loss: 35.356946
IoU in current test batch is 0.0
Epoch  5375: reducing learning rate of group 0 to 9.5121e-04.
training: bce: 0.171407, dice: 0.641651, loss: 0.171407
training IoU in current batch 5400 is 0.0
training IoU uptillnow 5400 is 0.000340018932575528
testing: bce: 35.606418, dice: 133.290654, loss: 35.606418
IoU in current test batch is 0.0
Epoch  5476: reducing learning rate of group 0 to 9.5025e-04.
training: bce: 0.169253, dice: 0.639390, loss: 0.169253
training IoU in current batch 5500 is 0.0
training IoU uptillnow 5500 is 0.0003338378939902612
testing: bce: 35.810045, dice: 135.280257, loss: 35.810045
IoU in current test batch is 0.0
Epoch  5577: reducing learning rate of group 0 to 9.4930e-04.
training: bce: 0.167100, dice: 0.636805, loss: 0.167100
training IoU in current batch 5600 is 0.0
training IoU uptillnow 5600 is 0.0003278775673701887
testing: bce: 35.997146, dice: 137.182416, loss: 35.997146
IoU in current test batch is 0.0
Epoch  5678: reducing learning rate of group 0 to 9.4835e-04.
training: bce: 0.165009, dice: 0.634034, loss: 0.165009
training IoU in current batch 5700 is 0.0
training IoU uptillnow 5700 is 0.00032212633833370057
testing: bce: 36.181449, dice: 139.024203, loss: 36.181449
IoU in current test batch is 0.0
Epoch  5779: reducing learning rate of group 0 to 9.4741e-04.
training: bce: 0.162940, dice: 0.631155, loss: 0.162940
training IoU in current batch 5800 is 0.0
training IoU uptillnow 5800 is 0.0003165733933529438
testing: bce: 36.354512, dice: 140.820411, loss: 36.354512
IoU in current test batch is 0.0
Epoch  5880: reducing learning rate of group 0 to 9.4646e-04.
training: bce: 0.161481, dice: 0.629639, loss: 0.161481
training IoU in current batch 5900 is 0.0
training IoU uptillnow 5900 is 0.0003112086518963611
testing: bce: 36.650087, dice: 142.903778, loss: 36.650087
IoU in current test batch is 0.0
Epoch  5981: reducing learning rate of group 0 to 9.4551e-04.
training: bce: 0.159660, dice: 0.627146, loss: 0.159660
training IoU in current batch 6000 is 0.0
training IoU uptillnow 6000 is 0.00030602270535584516
testing: bce: 36.850805, dice: 144.750044, loss: 36.850805
IoU in current test batch is 0.0
Epoch  6082: reducing learning rate of group 0 to 9.4457e-04.
training: bce: 0.157745, dice: 0.624186, loss: 0.157745
training IoU in current batch 6100 is 0.0
training IoU uptillnow 6100 is 0.00030100676198007326
testing: bce: 37.015498, dice: 146.467542, loss: 37.015498
IoU in current test batch is 0.0
Epoch  6183: reducing learning rate of group 0 to 9.4362e-04.
training: bce: 0.156021, dice: 0.621508, loss: 0.156021
training IoU in current batch 6200 is 0.0
training IoU uptillnow 6200 is 0.0002961525971360147
testing: bce: 37.211094, dice: 148.229668, loss: 37.211094
IoU in current test batch is 0.0
Epoch  6284: reducing learning rate of group 0 to 9.4268e-04.
training: bce: 0.154182, dice: 0.618259, loss: 0.154182
training IoU in current batch 6300 is 0.0
training IoU uptillnow 6300 is 0.00029145250830668574
testing: bce: 37.365327, dice: 149.832640, loss: 37.365327
IoU in current test batch is 0.0
Epoch  6385: reducing learning rate of group 0 to 9.4174e-04.
training: bce: 0.153322, dice: 0.617796, loss: 0.153322
training IoU in current batch 6400 is 0.0
training IoU uptillnow 6400 is 0.0002868992743072062
testing: bce: 37.746626, dice: 152.096658, loss: 37.746626
IoU in current test batch is 0.0
Epoch  6486: reducing learning rate of group 0 to 9.4079e-04.
training: bce: 0.151934, dice: 0.616528, loss: 0.151934
training IoU in current batch 6500 is 0.0
training IoU uptillnow 6500 is 0.000282486118264948
testing: bce: 37.989277, dice: 154.155652, loss: 37.989277
IoU in current test batch is 0.0
Epoch  6587: reducing learning rate of group 0 to 9.3985e-04.
training: bce: 0.150401, dice: 0.614064, loss: 0.150401
training IoU in current batch 6600 is 0.0
training IoU uptillnow 6600 is 0.0002782066739646155
testing: bce: 38.184592, dice: 155.901469, loss: 38.184592
IoU in current test batch is 0.0
Epoch  6688: reducing learning rate of group 0 to 9.3891e-04.
training: bce: 0.148737, dice: 0.610991, loss: 0.148737
training IoU in current batch 6700 is 0.8430040153554251
training IoU uptillnow 6700 is 0.00033695631435877324
testing: bce: 38.333993, dice: 157.471267, loss: 38.333993
IoU in current test batch is 0.7628938823394557
Epoch  6789: reducing learning rate of group 0 to 9.3797e-04.
training: bce: 0.147118, dice: 0.607740, loss: 0.147118
training IoU in current batch 6800 is 0.8379068749087725
training IoU uptillnow 6800 is 0.00039360354359249016
testing: bce: 38.482780, dice: 158.970791, loss: 38.482780
IoU in current test batch is 0.8115854088647984
Epoch  6890: reducing learning rate of group 0 to 9.3704e-04.
training: bce: 0.145553, dice: 0.604547, loss: 0.145553
training IoU in current batch 6900 is 0.8923753665689149
training IoU uptillnow 6900 is 0.000452555482286188
testing: bce: 38.633242, dice: 160.460791, loss: 38.633242
IoU in current test batch is 0.8334877640640934
Epoch  6991: reducing learning rate of group 0 to 9.3610e-04.
training: bce: 0.144077, dice: 0.601546, loss: 0.144077
training IoU in current batch 7000 is 0.7982854261220373
training IoU uptillnow 7000 is 0.0005031035703925156
testing: bce: 38.795406, dice: 161.977863, loss: 38.795406
IoU in current test batch is 0.8049236710266159
Epoch  7092: reducing learning rate of group 0 to 9.3516e-04.
training: bce: 0.142757, dice: 0.598930, loss: 0.142757
training IoU in current batch 7100 is 0.7894366718483974
training IoU uptillnow 7100 is 0.0005516049052587242
testing: bce: 38.989151, dice: 163.576902, loss: 38.989151
IoU in current test batch is 0.7291850504474885
Epoch  7193: reducing learning rate of group 0 to 9.3423e-04.
training: bce: 0.141295, dice: 0.595680, loss: 0.141295
training IoU in current batch 7200 is 0.8841684631158315
training IoU uptillnow 7200 is 0.0006053368509651598
testing: bce: 39.133244, dice: 164.980522, loss: 39.133244
IoU in current test batch is 0.7970040867250827
Epoch  7294: reducing learning rate of group 0 to 9.3329e-04.
training: bce: 0.139814, dice: 0.592288, loss: 0.139814
training IoU in current batch 7300 is 0.8322071432627335
training IoU uptillnow 7300 is 0.0006540383831573048
testing: bce: 39.260942, dice: 166.318890, loss: 39.260942
IoU in current test batch is 0.8380821027025931
Epoch  7395: reducing learning rate of group 0 to 9.3236e-04.
training: bce: 0.138393, dice: 0.588907, loss: 0.138393
training IoU in current batch 7400 is 0.6528033771330043
training IoU uptillnow 7400 is 0.0006893035973514369
testing: bce: 39.394095, dice: 167.634700, loss: 39.394095
IoU in current test batch is 0.8443050165961948
Epoch  7496: reducing learning rate of group 0 to 9.3143e-04.
training: bce: 0.136956, dice: 0.585284, loss: 0.136956
training IoU in current batch 7500 is 0.8671828144594079
training IoU uptillnow 7500 is 0.0007379185883519116
testing: bce: 39.511882, dice: 168.854522, loss: 39.511882
IoU in current test batch is 0.8368080294486242
Epoch  7597: reducing learning rate of group 0 to 9.3050e-04.
training: bce: 0.135581, dice: 0.581734, loss: 0.135581
training IoU in current batch 7600 is 0.6188046112049342
training IoU uptillnow 7600 is 0.0007689158843349765
testing: bce: 39.636572, dice: 170.067670, loss: 39.636572
IoU in current test batch is 0.7567217367112262
Epoch  7698: reducing learning rate of group 0 to 9.2957e-04.
training: bce: 0.134268, dice: 0.577071, loss: 0.134268
training IoU in current batch 7700 is 0.8590958019375673
training IoU uptillnow 7700 is 0.0008147094582260667
testing: bce: 39.769221, dice: 170.924093, loss: 39.769221
IoU in current test batch is 0.8011927715353218
Epoch  7799: reducing learning rate of group 0 to 9.2864e-04.
training: bce: 0.132841, dice: 0.571552, loss: 0.132841
training IoU in current batch 7800 is 0.8565869368311584
training IoU uptillnow 7800 is 0.0008591681843628405
testing: bce: 39.857483, dice: 171.487624, loss: 39.857483
IoU in current test batch is 0.8266129455634051
Epoch  7900: reducing learning rate of group 0 to 9.2771e-04.
training: bce: 0.131393, dice: 0.565995, loss: 0.131393
training IoU in current batch 7900 is 0.913104705102717
training IoU uptillnow 7900 is 0.0009060781367884923
testing: bce: 39.928211, dice: 171.997096, loss: 39.928211
IoU in current test batch is 0.8254343022978253
training: bce: 0.129973, dice: 0.560405, loss: 0.129973
training IoU in current batch 8000 is 0.8810935316995808
training IoU uptillnow 8000 is 0.0009498150386971213
testing: bce: 39.996584, dice: 172.453740, loss: 39.996584
IoU in current test batch is 0.8616568868126251
Epoch  8001: reducing learning rate of group 0 to 9.2678e-04.
training: bce: 0.128595, dice: 0.555100, loss: 0.128595
training IoU in current batch 8100 is 0.9135699221874772
training IoU uptillnow 8100 is 0.0009944766184062962
testing: bce: 40.067271, dice: 172.956287, loss: 40.067271
IoU in current test batch is 0.8756894858132753
Epoch  8102: reducing learning rate of group 0 to 9.2585e-04.
training: bce: 0.127270, dice: 0.549986, loss: 0.127270
training IoU in current batch 8200 is 0.8594858156028369
training IoU uptillnow 8200 is 0.0010347516148653609
testing: bce: 40.144032, dice: 173.478269, loss: 40.144032
IoU in current test batch is 0.8213814782175599
Epoch  8203: reducing learning rate of group 0 to 9.2493e-04.
training: bce: 0.125927, dice: 0.544791, loss: 0.125927
training IoU in current batch 8300 is 0.9316729192719189
training IoU uptillnow 8300 is 0.0010784043432293439
testing: bce: 40.204740, dice: 173.935110, loss: 40.204740
IoU in current test batch is 0.8381929624357749
Epoch  8304: reducing learning rate of group 0 to 9.2400e-04.
training: bce: 0.124636, dice: 0.539864, loss: 0.124636
training IoU in current batch 8400 is 0.9251054146308815
training IoU uptillnow 8400 is 0.0011206269682730895
testing: bce: 40.271742, dice: 174.438326, loss: 40.271742
IoU in current test batch is 0.8094876317550332
Epoch  8405: reducing learning rate of group 0 to 9.2308e-04.
training: bce: 0.123337, dice: 0.534833, loss: 0.123337
training IoU in current batch 8500 is 0.8933928571428571
training IoU uptillnow 8500 is 0.0011599910115320142
testing: bce: 40.326513, dice: 174.869918, loss: 40.326513
IoU in current test batch is 0.8320336600063848
Epoch  8553: reducing learning rate of group 0 to 9.2216e-04.
training: bce: 0.122121, dice: 0.530151, loss: 0.122121
training IoU in current batch 8600 is 0.9091115054940906
training IoU uptillnow 8600 is 0.0011993534870108939
testing: bce: 40.398410, dice: 175.378005, loss: 40.398410
IoU in current test batch is 0.8458153188782755
Epoch  8654: reducing learning rate of group 0 to 9.2123e-04.
training: bce: 0.121086, dice: 0.526244, loss: 0.121086
training IoU in current batch 8700 is 0.6864871377793843
training IoU uptillnow 8700 is 0.001225018148565727
testing: bce: 40.521894, dice: 176.109545, loss: 40.521894
IoU in current test batch is 0.5979430643031265
Epoch  8755: reducing learning rate of group 0 to 9.2031e-04.
training: bce: 0.119919, dice: 0.521780, loss: 0.119919
training IoU in current batch 8800 is 0.8303667367437515
training IoU uptillnow 8800 is 0.0012582736369778737
testing: bce: 40.592451, dice: 176.622569, loss: 40.592451
IoU in current test batch is 0.8431610094921826
Epoch  8856: reducing learning rate of group 0 to 9.1939e-04.
training: bce: 0.118737, dice: 0.517167, loss: 0.118737
training IoU in current batch 8900 is 0.8808098089535216
training IoU uptillnow 8900 is 0.0012936154570856115
testing: bce: 40.649242, dice: 177.050253, loss: 40.649242
IoU in current test batch is 0.8558533933329202
training: bce: 0.117575, dice: 0.512565, loss: 0.117575
training IoU in current batch 9000 is 0.8502846790890269
training IoU uptillnow 9000 is 0.0013264763385250017
testing: bce: 40.703705, dice: 177.446022, loss: 40.703705
IoU in current test batch is 0.863290563872744
Epoch  9026: reducing learning rate of group 0 to 9.1847e-04.
training: bce: 0.116447, dice: 0.508229, loss: 0.116447
training IoU in current batch 9100 is 0.9149014280994163
training IoU uptillnow 9100 is 0.0013621650628626797
testing: bce: 40.760774, dice: 177.899680, loss: 40.760774
IoU in current test batch is 0.8169686473696559
Epoch  9127: reducing learning rate of group 0 to 9.1755e-04.
training: bce: 0.115347, dice: 0.503883, loss: 0.115347
training IoU in current batch 9200 is 0.9078026008669556
training IoU uptillnow 9200 is 0.001396692265791406
testing: bce: 40.819391, dice: 178.316579, loss: 40.819391
IoU in current test batch is 0.8075791232421559
Epoch  9228: reducing learning rate of group 0 to 9.1664e-04.
training: bce: 0.114257, dice: 0.499596, loss: 0.114257
training IoU in current batch 9300 is 0.920880607107972
training IoU uptillnow 9300 is 0.0014311800710784552
testing: bce: 40.873423, dice: 178.721018, loss: 40.873423
IoU in current test batch is 0.8567388418205392
Epoch  9329: reducing learning rate of group 0 to 9.1572e-04.
training: bce: 0.113531, dice: 0.496847, loss: 0.113531
training IoU in current batch 9400 is 0.7788018433179723
training IoU uptillnow 9400 is 0.0014573775941665458
testing: bce: 41.050094, dice: 179.648299, loss: 41.050094
IoU in current test batch is 0.6677622038118769
Epoch  9430: reducing learning rate of group 0 to 9.1480e-04.
training: bce: 0.112635, dice: 0.493620, loss: 0.112635
training IoU in current batch 9500 is 0.8556478325572464
training IoU uptillnow 9500 is 0.0014870677485568171
testing: bce: 41.159564, dice: 180.379991, loss: 41.159564
IoU in current test batch is 0.8412333829036439
Epoch  9531: reducing learning rate of group 0 to 9.1389e-04.
training: bce: 0.111630, dice: 0.489652, loss: 0.111630
training IoU in current batch 9600 is 0.7899959497772377
training IoU uptillnow 9600 is 0.001512720409741375
testing: bce: 41.221363, dice: 180.813462, loss: 41.221363
IoU in current test batch is 0.8397086498156618
Epoch  9632: reducing learning rate of group 0 to 9.1298e-04.
training: bce: 0.110709, dice: 0.486066, loss: 0.110709
training IoU in current batch 9700 is 0.8196269048870205
training IoU uptillnow 9700 is 0.0015393714159746882
testing: bce: 41.307302, dice: 181.358749, loss: 41.307302
IoU in current test batch is 0.8512162948105562
Epoch  9733: reducing learning rate of group 0 to 9.1206e-04.
training: bce: 0.109737, dice: 0.482192, loss: 0.109737
training IoU in current batch 9800 is 0.8954616840494504
training IoU uptillnow 9800 is 0.0015693473062335654
testing: bce: 41.366680, dice: 181.768000, loss: 41.366680
IoU in current test batch is 0.8394179162079196
Epoch  9834: reducing learning rate of group 0 to 9.1115e-04.
training: bce: 0.108775, dice: 0.478407, loss: 0.108775
training IoU in current batch 9900 is 0.800330446102238
training IoU uptillnow 9900 is 0.0015939135614025143
testing: bce: 41.422520, dice: 182.181255, loss: 41.422520
IoU in current test batch is 0.8723171309548625
Epoch  9935: reducing learning rate of group 0 to 9.1024e-04.
training: bce: 0.107824, dice: 0.474630, loss: 0.107824
training IoU in current batch 10000 is 0.8802101753145052
training IoU uptillnow 10000 is 0.0016219821276975846
testing: bce: 41.474780, dice: 182.568425, loss: 41.474780
IoU in current test batch is 0.8635931274294626
Epoch 10036: reducing learning rate of group 0 to 9.0933e-04.
training: bce: 0.106900, dice: 0.470958, loss: 0.106900
training IoU in current batch 10100 is 0.868786313750807
training IoU uptillnow 10100 is 0.0016489294541113702
testing: bce: 41.530715, dice: 182.967096, loss: 41.530715
IoU in current test batch is 0.8394062816462549
Epoch 10137: reducing learning rate of group 0 to 9.0842e-04.
training: bce: 0.105987, dice: 0.467404, loss: 0.105987
training IoU in current batch 10200 is 0.8439984101748808
training IoU uptillnow 10200 is 0.0016741334791752173
testing: bce: 41.583739, dice: 183.384230, loss: 41.583739
IoU in current test batch is 0.8603341106512061
Epoch 10238: reducing learning rate of group 0 to 9.0751e-04.
training: bce: 0.105097, dice: 0.463916, loss: 0.105097
training IoU in current batch 10300 is 0.861048381578047
training IoU uptillnow 10300 is 0.0016996757413702955
testing: bce: 41.638571, dice: 183.800122, loss: 41.638571
IoU in current test batch is 0.8744353556767273
Epoch 10339: reducing learning rate of group 0 to 9.0660e-04.
training: bce: 0.104248, dice: 0.460613, loss: 0.104248
training IoU in current batch 10400 is 0.9057348603158077
training IoU uptillnow 10400 is 0.0017268750352863493
testing: bce: 41.703251, dice: 184.262742, loss: 41.703251
IoU in current test batch is 0.8503194951748431
Epoch 10440: reducing learning rate of group 0 to 9.0570e-04.
training: bce: 0.103380, dice: 0.457196, loss: 0.103380
training IoU in current batch 10500 is 0.7982775012583189
training IoU uptillnow 10500 is 0.0017484397669405274
testing: bce: 41.753575, dice: 184.654585, loss: 41.753575
IoU in current test batch is 0.8869006463746023
Epoch 10541: reducing learning rate of group 0 to 9.0479e-04.
training: bce: 0.102603, dice: 0.454119, loss: 0.102603
training IoU in current batch 10600 is 0.8401065022421524
training IoU uptillnow 10600 is 0.001771570535210221
testing: bce: 41.834450, dice: 185.158439, loss: 41.834450
IoU in current test batch is 0.7909966942144186
Epoch 10642: reducing learning rate of group 0 to 9.0389e-04.
training: bce: 0.101779, dice: 0.450916, loss: 0.101779
training IoU in current batch 10700 is 0.8846137329840683
training IoU uptillnow 10700 is 0.0017963485758579188
testing: bce: 41.890027, dice: 185.586464, loss: 41.890027
IoU in current test batch is 0.8666385409777971
Epoch 10743: reducing learning rate of group 0 to 9.0298e-04.
training: bce: 0.101000, dice: 0.447874, loss: 0.101000
training IoU in current batch 10800 is 0.848526176858777
training IoU uptillnow 10800 is 0.001818997240874454
testing: bce: 41.957869, dice: 186.057157, loss: 41.957869
IoU in current test batch is 0.8371052181889952
Epoch 10844: reducing learning rate of group 0 to 9.0208e-04.
training: bce: 0.100200, dice: 0.444729, loss: 0.100200
training IoU in current batch 10900 is 0.8267236674549754
training IoU uptillnow 10900 is 0.0018402303488131793
testing: bce: 42.010610, dice: 186.461283, loss: 42.010610
IoU in current test batch is 0.8479429401036256
Epoch 10945: reducing learning rate of group 0 to 9.0118e-04.
training: bce: 0.099417, dice: 0.441691, loss: 0.099417
training IoU in current batch 11000 is 0.9061756056574709
training IoU uptillnow 11000 is 0.0018646885587893104
testing: bce: 42.064751, dice: 186.886235, loss: 42.064751
IoU in current test batch is 0.8461691525541627
Epoch 11046: reducing learning rate of group 0 to 9.0028e-04.
training: bce: 0.098647, dice: 0.438648, loss: 0.098647
training IoU in current batch 11100 is 0.8740061162079511
training IoU uptillnow 11100 is 0.0018872571744297973
testing: bce: 42.118568, dice: 187.286005, loss: 42.118568
IoU in current test batch is 0.8522397446121587
training: bce: 0.097968, dice: 0.435905, loss: 0.097968
training IoU in current batch 11200 is 0.8861722797927462
training IoU uptillnow 11200 is 0.0019099658988698824
testing: bce: 42.205211, dice: 187.791343, loss: 42.205211
IoU in current test batch is 0.8660701877998052
Epoch 11215: reducing learning rate of group 0 to 8.9938e-04.
training: bce: 0.097312, dice: 0.433207, loss: 0.097312
training IoU in current batch 11300 is 0.767999339852292
training IoU uptillnow 11300 is 0.0019270443060939474
testing: bce: 42.296886, dice: 188.295104, loss: 42.296886
IoU in current test batch is 0.8196808367551534
Epoch 11316: reducing learning rate of group 0 to 8.9848e-04.
training: bce: 0.096631, dice: 0.430653, loss: 0.096631
training IoU in current batch 11400 is 0.8348363926576217
training IoU uptillnow 11400 is 0.0019467543109811867
testing: bce: 42.372676, dice: 188.841236, loss: 42.372676
IoU in current test batch is 0.8800900686639628
Epoch 11417: reducing learning rate of group 0 to 8.9758e-04.
training: bce: 0.095985, dice: 0.428071, loss: 0.095985
training IoU in current batch 11500 is 0.7629257186225216
training IoU uptillnow 11500 is 0.0019629952837846945
testing: bce: 42.458509, dice: 189.355382, loss: 42.458509
IoU in current test batch is 0.8694869611133099
Epoch 11518: reducing learning rate of group 0 to 8.9668e-04.
training: bce: 0.095318, dice: 0.425537, loss: 0.095318
training IoU in current batch 11600 is 0.9235987590486039
training IoU uptillnow 11600 is 0.0019858812290606044
testing: bce: 42.530132, dice: 189.871212, loss: 42.530132
IoU in current test batch is 0.839802507010136
Epoch 11619: reducing learning rate of group 0 to 8.9578e-04.
training: bce: 0.094629, dice: 0.422829, loss: 0.094629
training IoU in current batch 11700 is 0.8997694790225911
training IoU uptillnow 11700 is 0.002007357736761248
testing: bce: 42.586635, dice: 190.289142, loss: 42.586635
IoU in current test batch is 0.8551029005603129
Epoch 11720: reducing learning rate of group 0 to 8.9489e-04.
training: bce: 0.093936, dice: 0.420060, loss: 0.093936
training IoU in current batch 11800 is 0.8687287966473758
training IoU uptillnow 11800 is 0.0020271550950061055
testing: bce: 42.635929, dice: 190.658955, loss: 42.635929
IoU in current test batch is 0.8709610815347222
Epoch 11821: reducing learning rate of group 0 to 8.9399e-04.
training: bce: 0.093265, dice: 0.417428, loss: 0.093265
training IoU in current batch 11900 is 0.7969390274006418
training IoU uptillnow 11900 is 0.0020436036290956533
testing: bce: 42.690429, dice: 191.069583, loss: 42.690429
IoU in current test batch is 0.8848104973150862
Epoch 11922: reducing learning rate of group 0 to 8.9310e-04.
training: bce: 0.092597, dice: 0.414769, loss: 0.092597
training IoU in current batch 12000 is 0.7968260386131608
training IoU uptillnow 12000 is 0.0020597733363198027
testing: bce: 42.740548, dice: 191.447790, loss: 42.740548
IoU in current test batch is 0.8419240261928961
Epoch 12023: reducing learning rate of group 0 to 8.9221e-04.
training: bce: 0.091955, dice: 0.412257, loss: 0.091955
training IoU in current batch 12100 is 0.7447985568250151
training IoU uptillnow 12100 is 0.002073526079463388
testing: bce: 42.797797, dice: 191.873705, loss: 42.797797
IoU in current test batch is 0.8744037867645704
Epoch 12124: reducing learning rate of group 0 to 8.9131e-04.
training: bce: 0.091399, dice: 0.410063, loss: 0.091399
training IoU in current batch 12200 is 0.8933842426022469
training IoU uptillnow 12200 is 0.0020931424644609117
testing: bce: 42.890923, dice: 192.430177, loss: 42.890923
IoU in current test batch is 0.8679662883485543
Epoch 12225: reducing learning rate of group 0 to 8.9042e-04.
training: bce: 0.090802, dice: 0.407754, loss: 0.090802
training IoU in current batch 12300 is 0.7572612998127841
training IoU uptillnow 12300 is 0.0021069069066575056
testing: bce: 42.959819, dice: 192.914824, loss: 42.959819
IoU in current test batch is 0.8790723550092113
Epoch 12326: reducing learning rate of group 0 to 8.8953e-04.
training: bce: 0.090209, dice: 0.405316, loss: 0.090209
training IoU in current batch 12400 is 0.6324250204224531
training IoU uptillnow 12400 is 0.002115416044593597
testing: bce: 43.026453, dice: 193.320332, loss: 43.026453
IoU in current test batch is 0.6845808342094457
Epoch 12427: reducing learning rate of group 0 to 8.8864e-04.
training: bce: 0.089859, dice: 0.404090, loss: 0.089859
training IoU in current batch 12500 is 0.7224007463460143
training IoU uptillnow 12500 is 0.0021273877883511883
testing: bce: 43.205073, dice: 194.289501, loss: 43.205073
IoU in current test batch is 0.8445494499958534
Epoch 12528: reducing learning rate of group 0 to 8.8775e-04.
training: bce: 0.089280, dice: 0.401868, loss: 0.089280
training IoU in current batch 12600 is 0.8744061501252484
training IoU uptillnow 12600 is 0.0021452010012888525
testing: bce: 43.269991, dice: 194.766722, loss: 43.269991
IoU in current test batch is 0.8146077295861563
Epoch 12629: reducing learning rate of group 0 to 8.8687e-04.
training: bce: 0.088696, dice: 0.399563, loss: 0.088696
training IoU in current batch 12700 is 0.9322358474718629
training IoU uptillnow 12700 is 0.0021650102937545677
testing: bce: 43.328055, dice: 195.186732, loss: 43.328055
IoU in current test batch is 0.8496964074338518
Epoch 12730: reducing learning rate of group 0 to 8.8598e-04.
training: bce: 0.088126, dice: 0.397280, loss: 0.088126
training IoU in current batch 12800 is 0.842913688190465
training IoU uptillnow 12800 is 0.002181021215926255
testing: bce: 43.388364, dice: 195.599469, loss: 43.388364
IoU in current test batch is 0.8713160703673807
Epoch 12831: reducing learning rate of group 0 to 8.8509e-04.
training: bce: 0.087551, dice: 0.395027, loss: 0.087551
training IoU in current batch 12900 is 0.6517151470708058
training IoU uptillnow 12900 is 0.0021893737042560572
testing: bce: 43.442011, dice: 196.009350, loss: 43.442011
IoU in current test batch is 0.8706862498873265
Epoch 12932: reducing learning rate of group 0 to 8.8421e-04.
training: bce: 0.086989, dice: 0.392813, loss: 0.086989
training IoU in current batch 13000 is 0.9096267190569745
training IoU uptillnow 13000 is 0.0022075166155015678
testing: bce: 43.497997, dice: 196.421658, loss: 43.497997
IoU in current test batch is 0.8691354835855252
Epoch 13059: reducing learning rate of group 0 to 8.8333e-04.
training: bce: 0.086429, dice: 0.390597, loss: 0.086429
training IoU in current batch 13100 is 0.8238063214525891
training IoU uptillnow 13100 is 0.002222107219209387
testing: bce: 43.550165, dice: 196.815995, loss: 43.550165
IoU in current test batch is 0.8731254020500062
Epoch 13160: reducing learning rate of group 0 to 8.8244e-04.
training: bce: 0.085966, dice: 0.388848, loss: 0.085966
training IoU in current batch 13200 is 0.8947878059367591
training IoU uptillnow 13200 is 0.0022391652588311913
testing: bce: 43.647798, dice: 197.430265, loss: 43.647798
IoU in current test batch is 0.8490828930709916
Epoch 13261: reducing learning rate of group 0 to 8.8156e-04.
training: bce: 0.085517, dice: 0.386902, loss: 0.085517
training IoU in current batch 13300 is 0.21052183898269042
training IoU uptillnow 13300 is 0.0022302444554034963
testing: bce: 43.748604, dice: 197.930368, loss: 43.748604
IoU in current test batch is 0.13984260556373795
Epoch 13362: reducing learning rate of group 0 to 8.8068e-04.
training: bce: 0.085050, dice: 0.385461, loss: 0.085050
training IoU in current batch 13400 is 0.8068166387543566
training IoU uptillnow 13400 is 0.0022437049340123185
testing: bce: 43.836853, dice: 198.675657, loss: 43.836853
IoU in current test batch is 0.8747302169288774
Epoch 13463: reducing learning rate of group 0 to 8.7980e-04.
training: bce: 0.084533, dice: 0.383442, loss: 0.084533
training IoU in current batch 13500 is 0.8733336541700756
training IoU uptillnow 13500 is 0.002259429423582262
testing: bce: 43.895350, dice: 199.109625, loss: 43.895350
IoU in current test batch is 0.8208469020537414
Epoch 13564: reducing learning rate of group 0 to 8.7892e-04.
training: bce: 0.084008, dice: 0.381378, loss: 0.084008
training IoU in current batch 13600 is 0.8166443918988383
training IoU uptillnow 13600 is 0.002272838676842404
testing: bce: 43.945858, dice: 199.504953, loss: 43.945858
IoU in current test batch is 0.8591844239791104
Epoch 13665: reducing learning rate of group 0 to 8.7804e-04.
training: bce: 0.083500, dice: 0.379329, loss: 0.083500
training IoU in current batch 13700 is 0.8740713632204941
training IoU uptillnow 13700 is 0.0022881479107615347
testing: bce: 44.001548, dice: 199.891728, loss: 44.001548
IoU in current test batch is 0.8778198896552782
Epoch 13766: reducing learning rate of group 0 to 8.7716e-04.
training: bce: 0.083064, dice: 0.377504, loss: 0.083064
training IoU in current batch 13800 is 0.880693022819044
training IoU uptillnow 13800 is 0.002303475185620847
testing: bce: 44.090838, dice: 200.382220, loss: 44.090838
IoU in current test batch is 0.8452203557407189
Epoch 13867: reducing learning rate of group 0 to 8.7628e-04.
training: bce: 0.082590, dice: 0.375652, loss: 0.082590
training IoU in current batch 13900 is 0.8760401721664276
training IoU uptillnow 13900 is 0.0023184145833275677
testing: bce: 44.157082, dice: 200.843631, loss: 44.157082
IoU in current test batch is 0.8651535209726227
Epoch 13968: reducing learning rate of group 0 to 8.7541e-04.
training: bce: 0.082105, dice: 0.373706, loss: 0.082105
training IoU in current batch 14000 is 0.8939536263410294
training IoU uptillnow 14000 is 0.0023337802968364427
testing: bce: 44.213808, dice: 201.240943, loss: 44.213808
IoU in current test batch is 0.8671578458046127
Epoch 14069: reducing learning rate of group 0 to 8.7453e-04.
training: bce: 0.081613, dice: 0.371701, loss: 0.081613
training IoU in current batch 14100 is 0.8750578257517347
training IoU uptillnow 14100 is 0.0023482580560870085
testing: bce: 44.262695, dice: 201.590813, loss: 44.262695
IoU in current test batch is 0.8671972389411926
Epoch 14170: reducing learning rate of group 0 to 8.7366e-04.
training: bce: 0.081171, dice: 0.369925, loss: 0.081171
training IoU in current batch 14200 is 0.9217914914843583
training IoU uptillnow 14200 is 0.0023641773533289966
testing: bce: 44.335122, dice: 202.049979, loss: 44.335122
IoU in current test batch is 0.8526751613369499
Epoch 14271: reducing learning rate of group 0 to 8.7278e-04.
training: bce: 0.080800, dice: 0.368393, loss: 0.080800
training IoU in current batch 14300 is 0.883356070941337
training IoU uptillnow 14300 is 0.0023785302167747533
testing: bce: 44.443009, dice: 202.630045, loss: 44.443009
IoU in current test batch is 0.8549191859433963
Epoch 14372: reducing learning rate of group 0 to 8.7191e-04.
training: bce: 0.080339, dice: 0.366540, loss: 0.080339
training IoU in current batch 14400 is 0.9200194995125122
training IoU uptillnow 14400 is 0.0023939566960524965
testing: bce: 44.498565, dice: 203.021101, loss: 44.498565
IoU in current test batch is 0.8578266888608697
Epoch 14473: reducing learning rate of group 0 to 8.7104e-04.
training: bce: 0.079880, dice: 0.364712, loss: 0.079880
training IoU in current batch 14500 is 0.9088044485634847
training IoU uptillnow 14500 is 0.002408783711753241
testing: bce: 44.551382, dice: 203.411178, loss: 44.551382
IoU in current test batch is 0.8635559414969008
Epoch 14574: reducing learning rate of group 0 to 8.7017e-04.
training: bce: 0.079464, dice: 0.363067, loss: 0.079464
training IoU in current batch 14600 is 0.8820988710229216
training IoU uptillnow 14600 is 0.00242249311962504
testing: bce: 44.624895, dice: 203.889946, loss: 44.624895
IoU in current test batch is 0.8597726851864202
Epoch 14675: reducing learning rate of group 0 to 8.6930e-04.
training: bce: 0.079013, dice: 0.361268, loss: 0.079013
training IoU in current batch 14700 is 0.9081117021276596
training IoU uptillnow 14700 is 0.002436900747616423
testing: bce: 44.675543, dice: 204.269239, loss: 44.675543
IoU in current test batch is 0.8811188103105662
Epoch 14776: reducing learning rate of group 0 to 8.6843e-04.
training: bce: 0.078647, dice: 0.359857, loss: 0.078647
training IoU in current batch 14800 is 0.8795025488768137
training IoU uptillnow 14800 is 0.002450147230940304
testing: bce: 44.771400, dice: 204.855490, loss: 44.771400
IoU in current test batch is 0.8450006942031176
Epoch 14877: reducing learning rate of group 0 to 8.6756e-04.
training: bce: 0.078209, dice: 0.358132, loss: 0.078209
training IoU in current batch 14900 is 0.8032568382012054
training IoU uptillnow 14900 is 0.0024606575118614887
testing: bce: 44.823056, dice: 205.250897, loss: 44.823056
IoU in current test batch is 0.8526265575087199
Epoch 14978: reducing learning rate of group 0 to 8.6669e-04.
training: bce: 0.077777, dice: 0.356405, loss: 0.077777
training IoU in current batch 15000 is 0.8171844278411325
training IoU uptillnow 15000 is 0.0024714918870854348
testing: bce: 44.874328, dice: 205.631818, loss: 44.874328
IoU in current test batch is 0.8545554445657899
Epoch 15079: reducing learning rate of group 0 to 8.6583e-04.
training: bce: 0.077356, dice: 0.354718, loss: 0.077356
training IoU in current batch 15100 is 0.8894602801449701
training IoU uptillnow 15100 is 0.0024845758518138596
testing: bce: 44.928922, dice: 206.022875, loss: 44.928922
IoU in current test batch is 0.8379184906325903
Epoch 15180: reducing learning rate of group 0 to 8.6496e-04.
training: bce: 0.076931, dice: 0.353014, loss: 0.076931
training IoU in current batch 15200 is 0.8056460369163952
training IoU uptillnow 15200 is 0.0024947308043351945
testing: bce: 44.977881, dice: 206.391127, loss: 44.977881
IoU in current test batch is 0.8718042080048338
Epoch 15281: reducing learning rate of group 0 to 8.6409e-04.
training: bce: 0.076568, dice: 0.351564, loss: 0.076568
training IoU in current batch 15300 is 0.7906116191785166
training IoU uptillnow 15300 is 0.0025042617323239365
testing: bce: 45.060126, dice: 206.895376, loss: 45.060126
IoU in current test batch is 0.8585586239170168
Epoch 15382: reducing learning rate of group 0 to 8.6323e-04.
training: bce: 0.076202, dice: 0.350070, loss: 0.076202
training IoU in current batch 15400 is 0.8276795824922456
training IoU uptillnow 15400 is 0.002514872317221913
testing: bce: 45.137815, dice: 207.362894, loss: 45.137815
IoU in current test batch is 0.8687988308252171
Epoch 15483: reducing learning rate of group 0 to 8.6237e-04.
training: bce: 0.075798, dice: 0.348442, loss: 0.075798
training IoU in current batch 15500 is 0.8703573003970004
training IoU uptillnow 15500 is 0.002526722611943306
testing: bce: 45.190270, dice: 207.738251, loss: 45.190270
IoU in current test batch is 0.8114218213315861
Epoch 15584: reducing learning rate of group 0 to 8.6150e-04.
training: bce: 0.075404, dice: 0.346978, loss: 0.075404
training IoU in current batch 15600 is 0.902573463617166
training IoU uptillnow 15600 is 0.002539453492695453
testing: bce: 45.245310, dice: 208.200342, loss: 45.245310
IoU in current test batch is 0.8307694504239667
Epoch 15685: reducing learning rate of group 0 to 8.6064e-04.
training: bce: 0.075025, dice: 0.345475, loss: 0.075025
training IoU in current batch 15700 is 0.9341964891863059
training IoU uptillnow 15700 is 0.0025530292455343555
testing: bce: 45.306221, dice: 208.627061, loss: 45.306221
IoU in current test batch is 0.8810637958693636
Epoch 15786: reducing learning rate of group 0 to 8.5978e-04.
training: bce: 0.074636, dice: 0.343945, loss: 0.074636
training IoU in current batch 15800 is 0.9076344936708861
training IoU uptillnow 15800 is 0.002565592647995086
testing: bce: 45.358780, dice: 209.026043, loss: 45.358780
IoU in current test batch is 0.8518162342244495
Epoch 15887: reducing learning rate of group 0 to 8.5892e-04.
training: bce: 0.074317, dice: 0.342590, loss: 0.074317
training IoU in current batch 15900 is 0.8709121572078017
training IoU uptillnow 15900 is 0.0025768433123435166
testing: bce: 45.450279, dice: 209.520207, loss: 45.450279
IoU in current test batch is 0.8146876109234606
Epoch 15988: reducing learning rate of group 0 to 8.5806e-04.
training: bce: 0.073976, dice: 0.341356, loss: 0.073976
training IoU in current batch 16000 is 0.9368956235669424
training IoU uptillnow 16000 is 0.0025900152066344433
testing: bce: 45.526394, dice: 210.078614, loss: 45.526394
IoU in current test batch is 0.8373830514105557
Epoch 16089: reducing learning rate of group 0 to 8.5721e-04.
training: bce: 0.073660, dice: 0.340055, loss: 0.073660
training IoU in current batch 16100 is 0.867817234660198
training IoU uptillnow 16100 is 0.0026008783267305025
testing: bce: 45.615438, dice: 210.585369, loss: 45.615438
IoU in current test batch is 0.8385126183343589
Epoch 16190: reducing learning rate of group 0 to 8.5635e-04.
training: bce: 0.073290, dice: 0.338609, loss: 0.073290
training IoU in current batch 16200 is 0.8333675634969535
training IoU uptillnow 16200 is 0.002610544146684544
testing: bce: 45.668384, dice: 210.992258, loss: 45.668384
IoU in current test batch is 0.8669462906447621
Epoch 16291: reducing learning rate of group 0 to 8.5549e-04.
training: bce: 0.072923, dice: 0.337163, loss: 0.072923
training IoU in current batch 16300 is 0.8653227042771731
training IoU uptillnow 16300 is 0.002621071533806201
testing: bce: 45.720160, dice: 211.387947, loss: 45.720160
IoU in current test batch is 0.8464091055838563
Epoch 16392: reducing learning rate of group 0 to 8.5464e-04.
training: bce: 0.072574, dice: 0.335835, loss: 0.072574
training IoU in current batch 16400 is 0.8632197316890259
training IoU uptillnow 16400 is 0.002631406434877105
testing: bce: 45.780528, dice: 211.847486, loss: 45.780528
IoU in current test batch is 0.8239868879572967
training: bce: 0.072218, dice: 0.334431, loss: 0.072218
training IoU in current batch 16500 is 0.8677368632756792
training IoU uptillnow 16500 is 0.0026417529464915603
testing: bce: 45.833262, dice: 212.247723, loss: 45.833262
IoU in current test batch is 0.8763016895724957
Epoch 16547: reducing learning rate of group 0 to 8.5378e-04.
training: bce: 0.072037, dice: 0.333719, loss: 0.072037
training IoU in current batch 16600 is 0.8979255068363979
training IoU uptillnow 16600 is 0.0026528840505677634
testing: bce: 45.995498, dice: 213.079370, loss: 45.995498
IoU in current test batch is 0.8539093127124892
Epoch 16648: reducing learning rate of group 0 to 8.5293e-04.
training: bce: 0.071697, dice: 0.332463, loss: 0.071697
training IoU in current batch 16700 is 0.9172961373390558
training IoU uptillnow 16700 is 0.0026644617802613593
testing: bce: 46.054172, dice: 213.556434, loss: 46.054172
IoU in current test batch is 0.888897812007962
Epoch 16749: reducing learning rate of group 0 to 8.5208e-04.
training: bce: 0.071399, dice: 0.331325, loss: 0.071399
training IoU in current batch 16800 is 0.8755747243357611
training IoU uptillnow 16800 is 0.002674660053229739
testing: bce: 46.137375, dice: 214.099568, loss: 46.137375
IoU in current test batch is 0.8572939998477197
Epoch 16850: reducing learning rate of group 0 to 8.5122e-04.
training: bce: 0.071053, dice: 0.329940, loss: 0.071053
training IoU in current batch 16900 is 0.8956551488197058
training IoU uptillnow 16900 is 0.0026853317039656054
testing: bce: 46.187259, dice: 214.473688, loss: 46.187259
IoU in current test batch is 0.8825553760162239
Epoch 16951: reducing learning rate of group 0 to 8.5037e-04.
training: bce: 0.070718, dice: 0.328567, loss: 0.070718
training IoU in current batch 17000 is 0.8770996640537514
training IoU uptillnow 17000 is 0.0026953320958031626
testing: bce: 46.241614, dice: 214.845072, loss: 46.241614
IoU in current test batch is 0.8616126401152511
Epoch 17052: reducing learning rate of group 0 to 8.4952e-04.
training: bce: 0.070405, dice: 0.327326, loss: 0.070405
training IoU in current batch 17100 is 0.868639167309175
training IoU uptillnow 17100 is 0.0027049681623533216
testing: bce: 46.307404, dice: 215.292371, loss: 46.307404
IoU in current test batch is 0.8516087484554153
Epoch 17153: reducing learning rate of group 0 to 8.4867e-04.
training: bce: 0.070073, dice: 0.326023, loss: 0.070073
training IoU in current batch 17200 is 0.8161555149091837
training IoU uptillnow 17200 is 0.002712966589259854
testing: bce: 46.358470, dice: 215.689387, loss: 46.358470
IoU in current test batch is 0.8775201891139814
Epoch 17254: reducing learning rate of group 0 to 8.4782e-04.
training: bce: 0.069749, dice: 0.324734, loss: 0.069749
training IoU in current batch 17300 is 0.9004386412322758
training IoU uptillnow 17300 is 0.0027233083418574003
testing: bce: 46.412756, dice: 216.085716, loss: 46.412756
IoU in current test batch is 0.8738973056583847
Epoch 17355: reducing learning rate of group 0 to 8.4698e-04.
training: bce: 0.069429, dice: 0.323497, loss: 0.069429
training IoU in current batch 17400 is 0.7301825040217451
training IoU uptillnow 17400 is 0.0027286390939880324
testing: bce: 46.466708, dice: 216.506551, loss: 46.466708
IoU in current test batch is 0.8802010620844724
Epoch 17456: reducing learning rate of group 0 to 8.4613e-04.
training: bce: 0.069109, dice: 0.322248, loss: 0.069109
training IoU in current batch 17500 is 0.9029347826086956
training IoU uptillnow 17500 is 0.0027388444240780585
testing: bce: 46.518218, dice: 216.909979, loss: 46.518218
IoU in current test batch is 0.8410874993970255
Epoch 17557: reducing learning rate of group 0 to 8.4528e-04.
training: bce: 0.068800, dice: 0.321023, loss: 0.068800
training IoU in current batch 17600 is 0.8935792234159546
training IoU uptillnow 17600 is 0.0027486680232656144
testing: bce: 46.574725, dice: 217.319886, loss: 46.574725
IoU in current test batch is 0.8545790256457416
Epoch 17658: reducing learning rate of group 0 to 8.4444e-04.
training: bce: 0.068506, dice: 0.319886, loss: 0.068506
training IoU in current batch 17700 is 0.91959663260253
training IoU uptillnow 17700 is 0.002759115541144531
testing: bce: 46.639420, dice: 217.780852, loss: 46.639420
IoU in current test batch is 0.8463939121810627
Epoch 17759: reducing learning rate of group 0 to 8.4359e-04.
training: bce: 0.068236, dice: 0.318788, loss: 0.068236
training IoU in current batch 17800 is 0.8853267570900123
training IoU uptillnow 17800 is 0.002768483094901654
testing: bce: 46.718168, dice: 218.259400, loss: 46.718168
IoU in current test batch is 0.8559588578773986
Epoch 17860: reducing learning rate of group 0 to 8.4275e-04.
training: bce: 0.067968, dice: 0.317654, loss: 0.067968
training IoU in current batch 17900 is 0.9031567474991242
training IoU uptillnow 17900 is 0.0027782440057032517
testing: bce: 46.796087, dice: 218.705015, loss: 46.796087
IoU in current test batch is 0.8583685134712352
Epoch 17961: reducing learning rate of group 0 to 8.4191e-04.
training: bce: 0.067673, dice: 0.316457, loss: 0.067673
training IoU in current batch 18000 is 0.8886131386861313
training IoU uptillnow 18000 is 0.0027874925012742057
testing: bce: 46.852848, dice: 219.097472, loss: 46.852848
IoU in current test batch is 0.8684503105643321
Epoch 18062: reducing learning rate of group 0 to 8.4106e-04.
training: bce: 0.067369, dice: 0.315249, loss: 0.067369
training IoU in current batch 18100 is 0.8756426916774959
training IoU uptillnow 18100 is 0.0027962805293230055
testing: bce: 46.901953, dice: 219.473721, loss: 46.901953
IoU in current test batch is 0.8434225853604455
Epoch 18163: reducing learning rate of group 0 to 8.4022e-04.
training: bce: 0.067087, dice: 0.314128, loss: 0.067087
training IoU in current batch 18200 is 0.8691390295703316
training IoU uptillnow 18200 is 0.0028047933287215477
testing: bce: 46.963526, dice: 219.901916, loss: 46.963526
IoU in current test batch is 0.8754055976191821
Epoch 18264: reducing learning rate of group 0 to 8.3938e-04.
training: bce: 0.066798, dice: 0.313007, loss: 0.066798
training IoU in current batch 18300 is 0.9135019250962548
training IoU uptillnow 18300 is 0.0028144251318839965
testing: bce: 47.018335, dice: 220.320822, loss: 47.018335
IoU in current test batch is 0.8834618574733637
Epoch 18365: reducing learning rate of group 0 to 8.3854e-04.
training: bce: 0.066505, dice: 0.311831, loss: 0.066505
training IoU in current batch 18400 is 0.9489093836919924
training IoU uptillnow 18400 is 0.0028249143541359176
testing: bce: 47.067927, dice: 220.692397, loss: 47.067927
IoU in current test batch is 0.8895129259677229
Epoch 18466: reducing learning rate of group 0 to 8.3771e-04.
training: bce: 0.066229, dice: 0.310749, loss: 0.066229
training IoU in current batch 18500 is 0.9260546957812169
training IoU uptillnow 18500 is 0.0028346725246389723
testing: bce: 47.127044, dice: 221.122087, loss: 47.127044
IoU in current test batch is 0.8871523612136613
Epoch 18567: reducing learning rate of group 0 to 8.3687e-04.
training: bce: 0.065979, dice: 0.309738, loss: 0.065979
training IoU in current batch 18600 is 0.8831051099766211
training IoU uptillnow 18600 is 0.0028431712775299147
testing: bce: 47.203027, dice: 221.593802, loss: 47.203027
IoU in current test batch is 0.8775710604485223
Epoch 18668: reducing learning rate of group 0 to 8.3603e-04.
training: bce: 0.065700, dice: 0.308614, loss: 0.065700
training IoU in current batch 18700 is 0.893159469858914
training IoU uptillnow 18700 is 0.0028518479583050853
testing: bce: 47.256319, dice: 221.976888, loss: 47.256319
IoU in current test batch is 0.8751953405265664
Epoch 18769: reducing learning rate of group 0 to 8.3519e-04.
training: bce: 0.065424, dice: 0.307514, loss: 0.065424
training IoU in current batch 18800 is 0.9068905461528156
training IoU uptillnow 18800 is 0.0028607975076506463
testing: bce: 47.308934, dice: 222.367822, loss: 47.308934
IoU in current test batch is 0.8743987525673186
Epoch 18870: reducing learning rate of group 0 to 8.3436e-04.
training: bce: 0.065273, dice: 0.306828, loss: 0.065273
training IoU in current batch 18900 is 0.8920833158710867
training IoU uptillnow 18900 is 0.00286926065283717
testing: bce: 47.450899, dice: 223.052363, loss: 47.450899
IoU in current test batch is 0.8442735551518242
Epoch 18971: reducing learning rate of group 0 to 8.3353e-04.
training: bce: 0.065026, dice: 0.305871, loss: 0.065026
training IoU in current batch 19000 is 0.8641714313169654
training IoU uptillnow 19000 is 0.002876900232352709
testing: bce: 47.521551, dice: 223.533022, loss: 47.521551
IoU in current test batch is 0.8476113228730807
Epoch 19072: reducing learning rate of group 0 to 8.3269e-04.
training: bce: 0.064853, dice: 0.304998, loss: 0.064853
training IoU in current batch 19100 is 0.8931550576685638
training IoU uptillnow 19100 is 0.0028852185144111883
testing: bce: 47.644264, dice: 224.067637, loss: 47.644264
IoU in current test batch is 0.8411545109130609
Epoch 19173: reducing learning rate of group 0 to 8.3186e-04.
training: bce: 0.064585, dice: 0.303947, loss: 0.064585
training IoU in current batch 19200 is 0.931230448383733
training IoU uptillnow 19200 is 0.0028944416472037903
testing: bce: 47.696261, dice: 224.465096, loss: 47.696261
IoU in current test batch is 0.8831552374966615
Epoch 19274: reducing learning rate of group 0 to 8.3103e-04.
training: bce: 0.064331, dice: 0.302918, loss: 0.064331
training IoU in current batch 19300 is 0.894705559619605
training IoU uptillnow 19300 is 0.002902623016826578
testing: bce: 47.756203, dice: 224.870049, loss: 47.756203
IoU in current test batch is 0.8470855806815328
Epoch 19375: reducing learning rate of group 0 to 8.3020e-04.
training: bce: 0.064072, dice: 0.301900, loss: 0.064072
training IoU in current batch 19400 is 0.9397117727216419
training IoU uptillnow 19400 is 0.0029118799409376113
testing: bce: 47.810365, dice: 225.275221, loss: 47.810365
IoU in current test batch is 0.8771621065368193
Epoch 19476: reducing learning rate of group 0 to 8.2937e-04.
training: bce: 0.063810, dice: 0.300862, loss: 0.063810
training IoU in current batch 19500 is 0.823331652378551
training IoU uptillnow 19500 is 0.002918057974479251
testing: bce: 47.859744, dice: 225.658060, loss: 47.859744
IoU in current test batch is 0.8632174280283019
Epoch 19577: reducing learning rate of group 0 to 8.2854e-04.
training: bce: 0.063551, dice: 0.299823, loss: 0.063551
training IoU in current batch 19600 is 0.8674054260603745
training IoU uptillnow 19600 is 0.0029252972436788973
testing: bce: 47.909757, dice: 226.032011, loss: 47.909757
IoU in current test batch is 0.8654183553608734
Epoch 19678: reducing learning rate of group 0 to 8.2771e-04.
training: bce: 0.063291, dice: 0.298746, loss: 0.063291
training IoU in current batch 19700 is 0.8646018047838594
training IoU uptillnow 19700 is 0.002932391867201766
testing: bce: 47.957425, dice: 226.369003, loss: 47.957425
IoU in current test batch is 0.8495910155856918
Epoch 19779: reducing learning rate of group 0 to 8.2688e-04.
training: bce: 0.063226, dice: 0.298291, loss: 0.063226
training IoU in current batch 19800 is 0.7004550898203593
training IoU uptillnow 19800 is 0.002935269921754062
testing: bce: 48.151833, dice: 227.171649, loss: 48.151833
IoU in current test batch is 0.8689946790269083
Epoch 19880: reducing learning rate of group 0 to 8.2605e-04.
training: bce: 0.063022, dice: 0.297516, loss: 0.063022
training IoU in current batch 19900 is 0.9106917651454995
training IoU uptillnow 19900 is 0.0029434011156838813
testing: bce: 48.238783, dice: 227.725811, loss: 48.238783
IoU in current test batch is 0.8386312092412762
Epoch 19981: reducing learning rate of group 0 to 8.2523e-04.
training: bce: 0.062786, dice: 0.296615, loss: 0.062786
training IoU in current batch 20000 is 0.8877760549805138
training IoU uptillnow 20000 is 0.002950878137628878
testing: bce: 48.299244, dice: 228.176499, loss: 48.299244
IoU in current test batch is 0.8757097704806532
Epoch 20082: reducing learning rate of group 0 to 8.2440e-04.
training: bce: 0.062572, dice: 0.295775, loss: 0.062572
training IoU in current batch 20100 is 0.9362551093232985
training IoU uptillnow 20100 is 0.0029594866516778687
testing: bce: 48.375725, dice: 228.667830, loss: 48.375725
IoU in current test batch is 0.8727500185420272
Epoch 20183: reducing learning rate of group 0 to 8.2358e-04.
training: bce: 0.062330, dice: 0.294778, loss: 0.062330
training IoU in current batch 20200 is 0.9056423672488156
training IoU uptillnow 20200 is 0.0029672522335033535
testing: bce: 48.427960, dice: 229.031246, loss: 48.427960
IoU in current test batch is 0.8774162942563435
Epoch 20284: reducing learning rate of group 0 to 8.2275e-04.
training: bce: 0.062105, dice: 0.293924, loss: 0.062105
training IoU in current batch 20300 is 0.9410741011451046
training IoU uptillnow 20300 is 0.0029758139707193633
testing: bce: 48.491832, dice: 229.498289, loss: 48.491832
IoU in current test batch is 0.850706220855531
Epoch 20385: reducing learning rate of group 0 to 8.2193e-04.
training: bce: 0.061865, dice: 0.292984, loss: 0.061865
training IoU in current batch 20400 is 0.920634268578236
training IoU uptillnow 20400 is 0.0029837908217177057
testing: bce: 48.542750, dice: 229.890711, loss: 48.542750
IoU in current test batch is 0.8925747641786285
Epoch 20486: reducing learning rate of group 0 to 8.2111e-04.
training: bce: 0.061634, dice: 0.292069, loss: 0.061634
training IoU in current batch 20500 is 0.8538389358061489
training IoU uptillnow 20500 is 0.0029900607785847515
testing: bce: 48.598095, dice: 230.296028, loss: 48.598095
IoU in current test batch is 0.8606347216656046
Epoch 20587: reducing learning rate of group 0 to 8.2029e-04.
training: bce: 0.061398, dice: 0.291143, loss: 0.061398
training IoU in current batch 20600 is 0.9040539372460887
training IoU uptillnow 20600 is 0.0029974886165908955
testing: bce: 48.648545, dice: 230.686048, loss: 48.648545
IoU in current test batch is 0.8461341655089794
Epoch 20688: reducing learning rate of group 0 to 8.1947e-04.
training: bce: 0.061174, dice: 0.290244, loss: 0.061174
training IoU in current batch 20700 is 0.8957464927614182
training IoU uptillnow 20700 is 0.0030046440382962055
testing: bce: 48.706266, dice: 231.090262, loss: 48.706266
IoU in current test batch is 0.8550034890968023
Epoch 20789: reducing learning rate of group 0 to 8.1865e-04.
training: bce: 0.060962, dice: 0.289455, loss: 0.060962
training IoU in current batch 20800 is 0.8907941373694799
training IoU uptillnow 20800 is 0.0030116116198958937
testing: bce: 48.771973, dice: 231.575006, loss: 48.771973
IoU in current test batch is 0.849172881563029
Epoch 20890: reducing learning rate of group 0 to 8.1783e-04.
training: bce: 0.060730, dice: 0.288545, loss: 0.060730
training IoU in current batch 20900 is 0.910157104721894
training IoU uptillnow 20900 is 0.0030189757359846627
testing: bce: 48.820264, dice: 231.956611, loss: 48.820264
IoU in current test batch is 0.8657571173813022
Epoch 20991: reducing learning rate of group 0 to 8.1701e-04.
training: bce: 0.060502, dice: 0.287634, loss: 0.060502
training IoU in current batch 21000 is 0.8213499157922011
training IoU uptillnow 21000 is 0.003024155364778417
testing: bce: 48.869693, dice: 232.330896, loss: 48.869693
IoU in current test batch is 0.8527707537048562
Epoch 21092: reducing learning rate of group 0 to 8.1620e-04.
training: bce: 0.060298, dice: 0.286851, loss: 0.060298
training IoU in current batch 21100 is 0.9387878402442127
training IoU uptillnow 21100 is 0.003032068657212153
testing: bce: 48.936369, dice: 232.801290, loss: 48.936369
IoU in current test batch is 0.8699973000278736
Epoch 21193: reducing learning rate of group 0 to 8.1538e-04.
training: bce: 0.060076, dice: 0.285971, loss: 0.060076
training IoU in current batch 21200 is 0.8469887794824823
training IoU uptillnow 21200 is 0.0030377423293983714
testing: bce: 48.987651, dice: 233.187594, loss: 48.987651
IoU in current test batch is 0.8703808105866258
Epoch 21294: reducing learning rate of group 0 to 8.1456e-04.
training: bce: 0.059875, dice: 0.285188, loss: 0.059875
training IoU in current batch 21300 is 0.7569349828328519
training IoU uptillnow 21300 is 0.0030412488905211636
testing: bce: 49.053537, dice: 233.646042, loss: 49.053537
IoU in current test batch is 0.8762438796031888
Epoch 21395: reducing learning rate of group 0 to 8.1375e-04.
training: bce: 0.059681, dice: 0.284409, loss: 0.059681
training IoU in current batch 21400 is 0.8963184650705149
training IoU uptillnow 21400 is 0.003047979152821203
testing: bce: 49.124747, dice: 234.101250, loss: 49.124747
IoU in current test batch is 0.8712024949661823
Epoch 21496: reducing learning rate of group 0 to 8.1294e-04.
training: bce: 0.059464, dice: 0.283522, loss: 0.059464
training IoU in current batch 21500 is 0.9001300390117035
training IoU uptillnow 21500 is 0.003054735448073691
testing: bce: 49.174685, dice: 234.461539, loss: 49.174685
IoU in current test batch is 0.869460767859024
Epoch 21597: reducing learning rate of group 0 to 8.1212e-04.
training: bce: 0.059280, dice: 0.282796, loss: 0.059280
training IoU in current batch 21600 is 0.8064842512757346
training IoU uptillnow 21600 is 0.0030592615617179896
testing: bce: 49.250134, dice: 234.948830, loss: 49.250134
IoU in current test batch is 0.8511920275237294
Epoch 21698: reducing learning rate of group 0 to 8.1131e-04.
training: bce: 0.059071, dice: 0.281972, loss: 0.059071
training IoU in current batch 21700 is 0.857894204510974
training IoU uptillnow 21700 is 0.0030649304685003357
testing: bce: 49.303973, dice: 235.348975, loss: 49.303973
IoU in current test batch is 0.8238326716391897
Epoch 21799: reducing learning rate of group 0 to 8.1050e-04.
training: bce: 0.058859, dice: 0.281144, loss: 0.058859
training IoU in current batch 21800 is 0.8855999614327725
training IoU uptillnow 21800 is 0.0030711827933416896
testing: bce: 49.353036, dice: 235.738846, loss: 49.353036
IoU in current test batch is 0.8610705364958379
Epoch 21900: reducing learning rate of group 0 to 8.0969e-04.
training: bce: 0.058651, dice: 0.280315, loss: 0.058651
training IoU in current batch 21900 is 0.8625324215986796
training IoU uptillnow 21900 is 0.003076851389819712
testing: bce: 49.404251, dice: 236.122214, loss: 49.404251
IoU in current test batch is 0.8679281631196397
training: bce: 0.058448, dice: 0.279508, loss: 0.058448
training IoU in current batch 22000 is 0.9191028826192016
training IoU uptillnow 22000 is 0.003083754089802787
testing: bce: 49.458076, dice: 236.517694, loss: 49.458076
IoU in current test batch is 0.8700109909756534
Epoch 22001: reducing learning rate of group 0 to 8.0888e-04.
training: bce: 0.058241, dice: 0.278696, loss: 0.058241
training IoU in current batch 22100 is 0.8629979035639413
training IoU uptillnow 22100 is 0.0030893250387553995
testing: bce: 49.507448, dice: 236.902095, loss: 49.507448
IoU in current test batch is 0.8570757048711759
Epoch 22102: reducing learning rate of group 0 to 8.0807e-04.
training: bce: 0.058037, dice: 0.277895, loss: 0.058037
training IoU in current batch 22200 is 0.8842658447921605
training IoU uptillnow 22200 is 0.003095324787348731
testing: bce: 49.556695, dice: 237.290625, loss: 49.556695
IoU in current test batch is 0.842113199766169
Epoch 22203: reducing learning rate of group 0 to 8.0726e-04.
training: bce: 0.057876, dice: 0.277257, loss: 0.057876
training IoU in current batch 22300 is 0.9150296638101516
training IoU uptillnow 22300 is 0.003101960469747287
testing: bce: 49.642198, dice: 237.812203, loss: 49.642198
IoU in current test batch is 0.8706783173653816
Epoch 22304: reducing learning rate of group 0 to 8.0645e-04.
training: bce: 0.057684, dice: 0.276549, loss: 0.057684
training IoU in current batch 22400 is 0.8561534363345764
training IoU uptillnow 22400 is 0.0031072227647873547
testing: bce: 49.699586, dice: 238.268477, loss: 49.699586
IoU in current test batch is 0.8916401998713595
Epoch 22405: reducing learning rate of group 0 to 8.0565e-04.
training: bce: 0.057484, dice: 0.275750, loss: 0.057484
training IoU in current batch 22500 is 0.9060952631348319
training IoU uptillnow 22500 is 0.0031135480550006198
testing: bce: 49.747581, dice: 238.640019, loss: 49.747581
IoU in current test batch is 0.8387346387809139
Epoch 22506: reducing learning rate of group 0 to 8.0484e-04.
training: bce: 0.057303, dice: 0.275015, loss: 0.057303
training IoU in current batch 22600 is 0.7930612770339855
training IoU uptillnow 22600 is 0.0031173167304139613
testing: bce: 49.811463, dice: 239.061766, loss: 49.811463
IoU in current test batch is 0.8198100323801659
Epoch 22607: reducing learning rate of group 0 to 8.0404e-04.
training: bce: 0.057106, dice: 0.274227, loss: 0.057106
training IoU in current batch 22700 is 0.8969724153397622
training IoU uptillnow 22700 is 0.003123340893870571
testing: bce: 49.860467, dice: 239.432130, loss: 49.860467
IoU in current test batch is 0.8325207376912217
Epoch 22708: reducing learning rate of group 0 to 8.0323e-04.
training: bce: 0.056909, dice: 0.273427, loss: 0.056909
training IoU in current batch 22800 is 0.9149909564847324
training IoU uptillnow 22800 is 0.0031297073422217533
testing: bce: 49.907358, dice: 239.784869, loss: 49.907358
IoU in current test batch is 0.842636018582888
Epoch 22809: reducing learning rate of group 0 to 8.0243e-04.
training: bce: 0.056727, dice: 0.272739, loss: 0.056727
training IoU in current batch 22900 is 0.9037106833580114
training IoU uptillnow 22900 is 0.003135771907413528
testing: bce: 49.965604, dice: 240.230211, loss: 49.965604
IoU in current test batch is 0.8638196801650377
Epoch 22910: reducing learning rate of group 0 to 8.0163e-04.
training: bce: 0.056544, dice: 0.272025, loss: 0.056544
training IoU in current batch 23000 is 0.7889070910367423
training IoU uptillnow 23000 is 0.0031392881177859907
testing: bce: 50.022267, dice: 240.648235, loss: 50.022267
IoU in current test batch is 0.8743712506111085
Epoch 23011: reducing learning rate of group 0 to 8.0083e-04.
training: bce: 0.056351, dice: 0.271248, loss: 0.056351
training IoU in current batch 23100 is 0.9385889150311225
training IoU uptillnow 23100 is 0.003146013612168786
testing: bce: 50.067806, dice: 241.003886, loss: 50.067806
IoU in current test batch is 0.8592112288237297
Epoch 23112: reducing learning rate of group 0 to 8.0003e-04.
training: bce: 0.056301, dice: 0.270930, loss: 0.056301
training IoU in current batch 23200 is 0.8810245241927848
training IoU uptillnow 23200 is 0.0031514405722515206
testing: bce: 50.239922, dice: 241.763043, loss: 50.239922
IoU in current test batch is 0.8687469403052663
Epoch 23213: reducing learning rate of group 0 to 7.9923e-04.
training: bce: 0.056116, dice: 0.270194, loss: 0.056116
training IoU in current batch 23300 is 0.9056309976462068
training IoU uptillnow 23300 is 0.003157348964234609
testing: bce: 50.290524, dice: 242.145380, loss: 50.290524
IoU in current test batch is 0.8518735221207974
Epoch 23314: reducing learning rate of group 0 to 7.9843e-04.
training: bce: 0.055928, dice: 0.269438, loss: 0.055928
training IoU in current batch 23400 is 0.9177570798295561
training IoU uptillnow 23400 is 0.0031634659525467033
testing: bce: 50.337490, dice: 242.504698, loss: 50.337490
IoU in current test batch is 0.8731717548375564
Epoch 23415: reducing learning rate of group 0 to 7.9763e-04.
training: bce: 0.055819, dice: 0.268930, loss: 0.055819
training IoU in current batch 23500 is 0.8847678125269234
training IoU uptillnow 23500 is 0.0031688290141614764
testing: bce: 50.453620, dice: 243.081919, loss: 50.453620
IoU in current test batch is 0.8408440610117225
Epoch 23516: reducing learning rate of group 0 to 7.9683e-04.
training: bce: 0.055645, dice: 0.268243, loss: 0.055645
training IoU in current batch 23600 is 0.8797240982899842
training IoU uptillnow 23600 is 0.0031740397742025276
testing: bce: 50.510932, dice: 243.492461, loss: 50.510932
IoU in current test batch is 0.8265546268265281
Epoch 23617: reducing learning rate of group 0 to 7.9603e-04.
training: bce: 0.055471, dice: 0.267544, loss: 0.055471
training IoU in current batch 23700 is 0.8085575139146568
training IoU uptillnow 23700 is 0.0031777052220543934
testing: bce: 50.565724, dice: 243.887242, loss: 50.565724
IoU in current test batch is 0.850829786610907
Epoch 23718: reducing learning rate of group 0 to 7.9524e-04.
training: bce: 0.055332, dice: 0.266990, loss: 0.055332
training IoU in current batch 23800 is 0.8706756357185098
training IoU uptillnow 23800 is 0.003182644816846789
testing: bce: 50.651909, dice: 244.408510, loss: 50.651909
IoU in current test batch is 0.8588321830104868
Epoch 23819: reducing learning rate of group 0 to 7.9444e-04.
training: bce: 0.055163, dice: 0.266311, loss: 0.055163
training IoU in current batch 23900 is 0.8779417144250701
training IoU uptillnow 23900 is 0.0031876950815021535
testing: bce: 50.709240, dice: 244.811605, loss: 50.709240
IoU in current test batch is 0.8553061310688052
Epoch 23920: reducing learning rate of group 0 to 7.9365e-04.
training: bce: 0.054988, dice: 0.265633, loss: 0.054988
training IoU in current batch 24000 is 0.8293564714389009
training IoU uptillnow 24000 is 0.0031916911119829346
testing: bce: 50.760003, dice: 245.209995, loss: 50.760003
IoU in current test batch is 0.8733576456395539
Epoch 24021: reducing learning rate of group 0 to 7.9285e-04.
training: bce: 0.054810, dice: 0.264921, loss: 0.054810
training IoU in current batch 24100 is 0.9358397195779924
training IoU uptillnow 24100 is 0.003197863086116402
testing: bce: 50.806833, dice: 245.571410, loss: 50.806833
IoU in current test batch is 0.855967104264004
Epoch 24122: reducing learning rate of group 0 to 7.9206e-04.
training: bce: 0.054644, dice: 0.264262, loss: 0.054644
training IoU in current batch 24200 is 0.9634830444066447
training IoU uptillnow 24200 is 0.0032045551737818574
testing: bce: 50.863029, dice: 245.977283, loss: 50.863029
IoU in current test batch is 0.8675496659913635
Epoch 24223: reducing learning rate of group 0 to 7.9127e-04.
training: bce: 0.054474, dice: 0.263584, loss: 0.054474
training IoU in current batch 24300 is 0.9050555328553918
training IoU uptillnow 24300 is 0.003209990022102894
testing: bce: 50.914312, dice: 246.359524, loss: 50.914312
IoU in current test batch is 0.8489441876873343
Epoch 24324: reducing learning rate of group 0 to 7.9048e-04.
training: bce: 0.054305, dice: 0.262935, loss: 0.054305
training IoU in current batch 24400 is 0.8980557636944434
training IoU uptillnow 24400 is 0.0032152368922982516
testing: bce: 50.964963, dice: 246.764398, loss: 50.964963
IoU in current test batch is 0.8714494942216658
Epoch 24425: reducing learning rate of group 0 to 7.8969e-04.
training: bce: 0.054155, dice: 0.262319, loss: 0.054155
training IoU in current batch 24500 is 0.529174166922773
training IoU uptillnow 24500 is 0.003212913044056611
testing: bce: 51.032517, dice: 247.195613, loss: 51.032517
IoU in current test batch is 0.6714555336120849
Epoch 24526: reducing learning rate of group 0 to 7.8890e-04.
training: bce: 0.054047, dice: 0.261903, loss: 0.054047
training IoU in current batch 24600 is 0.908874546187979
training IoU uptillnow 24600 is 0.0032183252617993176
testing: bce: 51.139106, dice: 247.810923, loss: 51.139106
IoU in current test batch is 0.8653682391446039
Epoch 24627: reducing learning rate of group 0 to 7.8811e-04.
training: bce: 0.053896, dice: 0.261310, loss: 0.053896
training IoU in current batch 24700 is 0.8217363884801345
training IoU uptillnow 24700 is 0.0032219297987840607
testing: bce: 51.202832, dice: 248.254238, loss: 51.202832
IoU in current test batch is 0.8774522031704796
Epoch 24728: reducing learning rate of group 0 to 7.8732e-04.
training: bce: 0.053772, dice: 0.260803, loss: 0.053772
training IoU in current batch 24800 is 0.9361868666062454
training IoU uptillnow 24800 is 0.0032278126443719287
testing: bce: 51.292274, dice: 248.775569, loss: 51.292274
IoU in current test batch is 0.8660925285385368
Epoch 24829: reducing learning rate of group 0 to 7.8653e-04.
training: bce: 0.053625, dice: 0.260220, loss: 0.053625
training IoU in current batch 24900 is 0.921915427688743
training IoU uptillnow 24900 is 0.0032333616765155043
testing: bce: 51.358757, dice: 249.220374, loss: 51.358757
IoU in current test batch is 0.8390091500451274
Epoch 24930: reducing learning rate of group 0 to 7.8575e-04.
training: bce: 0.053828, dice: 0.260360, loss: 0.053828
training IoU in current batch 25000 is 0.7507431411724574
training IoU uptillnow 25000 is 0.0032354430093795767
testing: bce: 51.759518, dice: 250.356103, loss: 51.759518
IoU in current test batch is 0.7810801893236702
Epoch 25031: reducing learning rate of group 0 to 7.8496e-04.
training: bce: 0.053681, dice: 0.259807, loss: 0.053681
training IoU in current batch 25100 is 0.9047511239043954
training IoU uptillnow 25100 is 0.0032405755244592246
testing: bce: 51.824625, dice: 250.823965, loss: 51.824625
IoU in current test batch is 0.8590057767506274
Epoch 25132: reducing learning rate of group 0 to 7.8418e-04.
training: bce: 0.053524, dice: 0.259203, loss: 0.053524
training IoU in current batch 25200 is 0.7732493772488237
training IoU uptillnow 25200 is 0.003243058248802643
testing: bce: 51.879062, dice: 251.237380, loss: 51.879062
IoU in current test batch is 0.8810621527717662
Epoch 25233: reducing learning rate of group 0 to 7.8339e-04.
training: bce: 0.053372, dice: 0.258641, loss: 0.053372
training IoU in current batch 25300 is 0.8957699751175007
training IoU uptillnow 25300 is 0.003247942607629507
testing: bce: 51.937128, dice: 251.687369, loss: 51.937128
IoU in current test batch is 0.8013978576070687
Epoch 25334: reducing learning rate of group 0 to 7.8261e-04.
training: bce: 0.053483, dice: 0.258888, loss: 0.053483
training IoU in current batch 25400 is 0.9004501547406921
training IoU uptillnow 25400 is 0.0032528806343452817
testing: bce: 52.250722, dice: 252.923474, loss: 52.250722
IoU in current test batch is 0.8406808604970142
Epoch 25435: reducing learning rate of group 0 to 7.8183e-04.
training: bce: 0.053335, dice: 0.258338, loss: 0.053335
training IoU in current batch 25500 is 0.8746762303247659
training IoU uptillnow 25500 is 0.003257274581709222
testing: bce: 52.311625, dice: 253.380213, loss: 52.311625
IoU in current test batch is 0.8910898898327658
Epoch 25536: reducing learning rate of group 0 to 7.8104e-04.
training: bce: 0.053200, dice: 0.257821, loss: 0.053200
training IoU in current batch 25600 is 0.9081462799495587
training IoU uptillnow 25600 is 0.0032622878890723665
testing: bce: 52.383812, dice: 253.864677, loss: 52.383812
IoU in current test batch is 0.8372720015887547
Epoch 25637: reducing learning rate of group 0 to 7.8026e-04.
training: bce: 0.053061, dice: 0.257293, loss: 0.053061
training IoU in current batch 25700 is 0.8883995784402289
training IoU uptillnow 25700 is 0.003266878021764203
testing: bce: 52.450905, dice: 254.333685, loss: 52.450905
IoU in current test batch is 0.8739547520547338
Epoch 25738: reducing learning rate of group 0 to 7.7948e-04.
training: bce: 0.052914, dice: 0.256741, loss: 0.052914
training IoU in current batch 25800 is 0.8977298709782786
training IoU uptillnow 25800 is 0.0032716133860257713
testing: bce: 52.509069, dice: 254.776432, loss: 52.509069
IoU in current test batch is 0.8406122802990098
Epoch 25839: reducing learning rate of group 0 to 7.7870e-04.
training: bce: 0.052765, dice: 0.256148, loss: 0.052765
training IoU in current batch 25900 is 0.8729896655048004
training IoU uptillnow 25900 is 0.0032758345934752837
testing: bce: 52.563651, dice: 255.172648, loss: 52.563651
IoU in current test batch is 0.858813932876878
Epoch 25940: reducing learning rate of group 0 to 7.7792e-04.
training: bce: 0.052612, dice: 0.255533, loss: 0.052612
training IoU in current batch 26000 is 0.936870096188569
training IoU uptillnow 26000 is 0.0032812517539209107
testing: bce: 52.613737, dice: 255.543046, loss: 52.613737
IoU in current test batch is 0.8721966492902384
Epoch 26041: reducing learning rate of group 0 to 7.7715e-04.
training: bce: 0.052470, dice: 0.255009, loss: 0.052470
training IoU in current batch 26100 is 0.9343387036343199
training IoU uptillnow 26100 is 0.0032865789128966234
testing: bce: 52.673646, dice: 255.999271, loss: 52.673646
IoU in current test batch is 0.8368422860757173
Epoch 26142: reducing learning rate of group 0 to 7.7637e-04.
training: bce: 0.052334, dice: 0.254460, loss: 0.052334
training IoU in current batch 26200 is 0.7660842586544742
training IoU uptillnow 26200 is 0.0032886545679493913
testing: bce: 52.738551, dice: 256.427580, loss: 52.738551
IoU in current test batch is 0.8499521020871019
Epoch 26243: reducing learning rate of group 0 to 7.7559e-04.
training: bce: 0.052189, dice: 0.253895, loss: 0.052189
training IoU in current batch 26300 is 0.928856433034706
training IoU uptillnow 26300 is 0.0032938088495250884
testing: bce: 52.792858, dice: 256.834012, loss: 52.792858
IoU in current test batch is 0.8441976920841354
Epoch 26344: reducing learning rate of group 0 to 7.7482e-04.
training: bce: 0.052094, dice: 0.253387, loss: 0.052094
training IoU in current batch 26400 is 0.674049421258799
training IoU uptillnow 26400 is 0.00329409837740952
testing: bce: 52.897504, dice: 257.294746, loss: 52.897504
IoU in current test batch is 0.7423438997867253
Epoch 26445: reducing learning rate of group 0 to 7.7404e-04.
training: bce: 0.051971, dice: 0.252982, loss: 0.051971
training IoU in current batch 26500 is 0.9132344865458539
training IoU uptillnow 26500 is 0.0032988984757277716
testing: bce: 52.972063, dice: 257.856310, loss: 52.972063
IoU in current test batch is 0.8580591372042353
Epoch 26546: reducing learning rate of group 0 to 7.7327e-04.
training: bce: 0.051831, dice: 0.252437, loss: 0.051831
training IoU in current batch 26600 is 0.9268901013250195
training IoU uptillnow 26600 is 0.003303919159276876
testing: bce: 53.028988, dice: 258.272657, loss: 53.028988
IoU in current test batch is 0.8598790167512096
Epoch 26647: reducing learning rate of group 0 to 7.7250e-04.
training: bce: 0.051694, dice: 0.251871, loss: 0.051694
training IoU in current batch 26700 is 0.9112484458421707
training IoU uptillnow 26700 is 0.003308609332191501
testing: bce: 53.088144, dice: 258.661567, loss: 53.088144
IoU in current test batch is 0.8067641916013584
Epoch 26748: reducing learning rate of group 0 to 7.7172e-04.
training: bce: 0.051554, dice: 0.251307, loss: 0.051554
training IoU in current batch 26800 is 0.7950100372813307
training IoU uptillnow 26800 is 0.003311095959012199
testing: bce: 53.142123, dice: 259.049392, loss: 53.142123
IoU in current test batch is 0.8322109350853655
Epoch 26849: reducing learning rate of group 0 to 7.7095e-04.
training: bce: 0.051406, dice: 0.250691, loss: 0.051406
training IoU in current batch 26900 is 0.908760730794629
training IoU uptillnow 26900 is 0.0033156783451501154
testing: bce: 53.187333, dice: 259.378343, loss: 53.187333
IoU in current test batch is 0.8733991948228421
Epoch 26950: reducing learning rate of group 0 to 7.7018e-04.
training: bce: 0.051262, dice: 0.250089, loss: 0.051262
training IoU in current batch 27000 is 0.7035532994923858
training IoU uptillnow 27000 is 0.0033164267920680513
testing: bce: 53.235071, dice: 259.717007, loss: 53.235071
IoU in current test batch is 0.8698194751398888
Epoch 27051: reducing learning rate of group 0 to 7.6941e-04.
training: bce: 0.051119, dice: 0.249522, loss: 0.051119
training IoU in current batch 27100 is 0.8779755428869838
training IoU uptillnow 27100 is 0.0033203877194226392
testing: bce: 53.283952, dice: 260.088767, loss: 53.283952
IoU in current test batch is 0.8288482623631184
Epoch 27152: reducing learning rate of group 0 to 7.6864e-04.
training: bce: 0.051001, dice: 0.249068, loss: 0.051001
training IoU in current batch 27200 is 0.8062071781841281
training IoU uptillnow 27200 is 0.003323000300472961
testing: bce: 53.356489, dice: 260.572609, loss: 53.356489
IoU in current test batch is 0.8384665888341923
Epoch 27253: reducing learning rate of group 0 to 7.6787e-04.
training: bce: 0.050861, dice: 0.248505, loss: 0.050861
training IoU in current batch 27300 is 0.8865593834995467
training IoU uptillnow 27300 is 0.003327065340643742
testing: bce: 53.406243, dice: 260.940271, loss: 53.406243
IoU in current test batch is 0.8838713805972943
Epoch 27354: reducing learning rate of group 0 to 7.6710e-04.
training: bce: 0.050726, dice: 0.247966, loss: 0.050726
training IoU in current batch 27400 is 0.917552681073197
training IoU uptillnow 27400 is 0.0033316662605544106
testing: bce: 53.459860, dice: 261.327613, loss: 53.459860
IoU in current test batch is 0.8742931234976967
Epoch 27455: reducing learning rate of group 0 to 7.6634e-04.
training: bce: 0.050590, dice: 0.247430, loss: 0.050590
training IoU in current batch 27500 is 0.9153268048280574
training IoU uptillnow 27500 is 0.0033361932514405085
testing: bce: 53.510686, dice: 261.714690, loss: 53.510686
IoU in current test batch is 0.8439470606354097
Epoch 27556: reducing learning rate of group 0 to 7.6557e-04.
training: bce: 0.050458, dice: 0.246915, loss: 0.050458
training IoU in current batch 27600 is 0.9307296043257463
training IoU uptillnow 27600 is 0.0033409664653464835
testing: bce: 53.565570, dice: 262.119170, loss: 53.565570
IoU in current test batch is 0.8514773360646472
Epoch 27657: reducing learning rate of group 0 to 7.6481e-04.
training: bce: 0.050330, dice: 0.246389, loss: 0.050330
training IoU in current batch 27700 is 0.8048306772908367
training IoU uptillnow 27700 is 0.0033434327550873146
testing: bce: 53.622876, dice: 262.508477, loss: 53.622876
IoU in current test batch is 0.8471173333882422
Epoch 27758: reducing learning rate of group 0 to 7.6404e-04.
training: bce: 0.050211, dice: 0.245957, loss: 0.050211
training IoU in current batch 27800 is 0.8823610790869264
training IoU uptillnow 27800 is 0.0033472756839040747
testing: bce: 53.689366, dice: 262.994325, loss: 53.689366
IoU in current test batch is 0.847377408315984
Epoch 27859: reducing learning rate of group 0 to 7.6328e-04.
training: bce: 0.050149, dice: 0.245670, loss: 0.050149
training IoU in current batch 27900 is 0.8861018770356939
training IoU uptillnow 27900 is 0.0033511581028183583
testing: bce: 53.815178, dice: 263.632689, loss: 53.815178
IoU in current test batch is 0.8695771742905407
Epoch 27960: reducing learning rate of group 0 to 7.6251e-04.
training: bce: 0.050018, dice: 0.245162, loss: 0.050018
training IoU in current batch 28000 is 0.829178252948509
training IoU uptillnow 28000 is 0.0033539963341741104
testing: bce: 53.867530, dice: 264.029682, loss: 53.867530
IoU in current test batch is 0.8728517333761799
Epoch 28061: reducing learning rate of group 0 to 7.6175e-04.
training: bce: 0.049889, dice: 0.244642, loss: 0.049889
training IoU in current batch 28100 is 0.8890887505845955
training IoU uptillnow 28100 is 0.0033578803504680107
testing: bce: 53.920039, dice: 264.410440, loss: 53.920039
IoU in current test batch is 0.8266815569446326
Epoch 28162: reducing learning rate of group 0 to 7.6099e-04.
training: bce: 0.049755, dice: 0.244109, loss: 0.049755
training IoU in current batch 28200 is 0.9221687251413021
training IoU uptillnow 28200 is 0.0033623233250974155
testing: bce: 53.967392, dice: 264.773905, loss: 53.967392
IoU in current test batch is 0.8698793905565665
Epoch 28263: reducing learning rate of group 0 to 7.6023e-04.
training: bce: 0.049625, dice: 0.243594, loss: 0.049625
training IoU in current batch 28300 is 0.9150872505635802
training IoU uptillnow 28300 is 0.003366609791751316
testing: bce: 54.016974, dice: 265.152004, loss: 54.016974
IoU in current test batch is 0.8809116440592101
Epoch 28364: reducing learning rate of group 0 to 7.5947e-04.
training: bce: 0.049499, dice: 0.243107, loss: 0.049499
training IoU in current batch 28400 is 0.903522027199814
training IoU uptillnow 28400 is 0.0033706624671650262
testing: bce: 54.069584, dice: 265.557395, loss: 54.069584
IoU in current test batch is 0.8786079834483136
Epoch 28465: reducing learning rate of group 0 to 7.5871e-04.
training: bce: 0.049364, dice: 0.242548, loss: 0.049364
training IoU in current batch 28500 is 0.934826736262143
training IoU uptillnow 28500 is 0.0033752358899015815
testing: bce: 54.112592, dice: 265.879502, loss: 54.112592
IoU in current test batch is 0.8519702665820351
Epoch 28566: reducing learning rate of group 0 to 7.5795e-04.
training: bce: 0.049243, dice: 0.242071, loss: 0.049243
training IoU in current batch 28600 is 0.9032912860664056
training IoU uptillnow 28600 is 0.0033792260319960203
testing: bce: 54.168714, dice: 266.287610, loss: 54.168714
IoU in current test batch is 0.8761253100628825
Epoch 28667: reducing learning rate of group 0 to 7.5719e-04.
training: bce: 0.049119, dice: 0.241583, loss: 0.049119
training IoU in current batch 28700 is 0.9216671902817049
training IoU uptillnow 28700 is 0.0033835084957408814
testing: bce: 54.221338, dice: 266.680156, loss: 54.221338
IoU in current test batch is 0.8685369520405334
Epoch 28768: reducing learning rate of group 0 to 7.5643e-04.
training: bce: 0.049019, dice: 0.241187, loss: 0.049019
training IoU in current batch 28800 is 0.8445067926757236
training IoU uptillnow 28800 is 0.0033864216774624808
testing: bce: 54.299488, dice: 267.170495, loss: 54.299488
IoU in current test batch is 0.8532507082055261
Epoch 28869: reducing learning rate of group 0 to 7.5568e-04.
training: bce: 0.048891, dice: 0.240689, loss: 0.048891
training IoU in current batch 28900 is 0.9283768003114052
training IoU uptillnow 28900 is 0.0033907656874417012
testing: bce: 54.345845, dice: 267.544067, loss: 54.345845
IoU in current test batch is 0.8462204524051464
Epoch 28970: reducing learning rate of group 0 to 7.5492e-04.
training: bce: 0.048765, dice: 0.240178, loss: 0.048765
training IoU in current batch 29000 is 0.8767406386852334
training IoU uptillnow 29000 is 0.0033941894918139105
testing: bce: 54.393125, dice: 267.899699, loss: 54.393125
IoU in current test batch is 0.872638356421614
Epoch 29071: reducing learning rate of group 0 to 7.5417e-04.
training: bce: 0.048654, dice: 0.239742, loss: 0.048654
training IoU in current batch 29100 is 0.8686359551439847
training IoU uptillnow 29100 is 0.0033974505147475074
testing: bce: 54.456896, dice: 268.335961, loss: 54.456896
IoU in current test batch is 0.8657545626671426
Epoch 29172: reducing learning rate of group 0 to 7.5341e-04.
training: bce: 0.048533, dice: 0.239260, loss: 0.048533
training IoU in current batch 29200 is 0.8550647191544947
training IoU uptillnow 29200 is 0.003400456826452671
testing: bce: 54.508526, dice: 268.716587, loss: 54.508526
IoU in current test batch is 0.8687860837770646
Epoch 29273: reducing learning rate of group 0 to 7.5266e-04.
training: bce: 0.048515, dice: 0.239138, loss: 0.048515
training IoU in current batch 29300 is 0.8774997186057855
training IoU uptillnow 29300 is 0.003403825454713059
testing: bce: 54.675092, dice: 269.499236, loss: 54.675092
IoU in current test batch is 0.7812753469058997
Epoch 29374: reducing learning rate of group 0 to 7.5191e-04.
training: bce: 0.048404, dice: 0.238729, loss: 0.048404
training IoU in current batch 29400 is 0.8577707454289732
training IoU uptillnow 29400 is 0.0034068356525717436
testing: bce: 54.735730, dice: 269.956675, loss: 54.735730
IoU in current test batch is 0.8368725909285193
Epoch 29475: reducing learning rate of group 0 to 7.5116e-04.
training: bce: 0.048286, dice: 0.238262, loss: 0.048286
training IoU in current batch 29500 is 0.8234892042941578
training IoU uptillnow 29500 is 0.0034092444196267553
testing: bce: 54.788274, dice: 270.344781, loss: 54.788274
IoU in current test batch is 0.8609224101737497
Epoch 29576: reducing learning rate of group 0 to 7.5040e-04.
training: bce: 0.048172, dice: 0.237807, loss: 0.048172
training IoU in current batch 29600 is 0.8659558918361205
training IoU uptillnow 29600 is 0.003412354230239754
testing: bce: 54.844192, dice: 270.742856, loss: 54.844192
IoU in current test batch is 0.8448994399641633
Epoch 29677: reducing learning rate of group 0 to 7.4965e-04.
training: bce: 0.048073, dice: 0.237427, loss: 0.048073
training IoU in current batch 29700 is 0.8910800757947926
training IoU uptillnow 29700 is 0.0034158660518913294
testing: bce: 54.916079, dice: 271.223816, loss: 54.916079
IoU in current test batch is 0.8801858689733764
Epoch 29778: reducing learning rate of group 0 to 7.4890e-04.
training: bce: 0.047979, dice: 0.237003, loss: 0.047979
training IoU in current batch 29800 is 0.8754429364177309
training IoU uptillnow 29800 is 0.0034190919457546135
testing: bce: 54.992845, dice: 271.651036, loss: 54.992845
IoU in current test batch is 0.769962241137615
Epoch 29879: reducing learning rate of group 0 to 7.4816e-04.
training: bce: 0.047878, dice: 0.236626, loss: 0.047878
training IoU in current batch 29900 is 0.9434865900383141
training IoU uptillnow 29900 is 0.0034234340781396067
testing: bce: 55.061462, dice: 272.129165, loss: 55.061462
IoU in current test batch is 0.8886470500754847
Epoch 29980: reducing learning rate of group 0 to 7.4741e-04.
training: bce: 0.047765, dice: 0.236154, loss: 0.047765
training IoU in current batch 30000 is 0.8906733046934596
training IoU uptillnow 30000 is 0.003426867071857575
testing: bce: 55.114802, dice: 272.494203, loss: 55.114802
IoU in current test batch is 0.8623272528178169
Epoch 30081: reducing learning rate of group 0 to 7.4666e-04.
training: bce: 0.047651, dice: 0.235689, loss: 0.047651
training IoU in current batch 30100 is 0.8890573557442384
training IoU uptillnow 30100 is 0.0034302504136298206
testing: bce: 55.167373, dice: 272.863916, loss: 55.167373
IoU in current test batch is 0.8701870340952222
Epoch 30182: reducing learning rate of group 0 to 7.4591e-04.
training: bce: 0.047551, dice: 0.235301, loss: 0.047551
training IoU in current batch 30200 is 0.9009048136083967
training IoU uptillnow 30200 is 0.003433807493376889
testing: bce: 55.233663, dice: 273.320086, loss: 55.233663
IoU in current test batch is 0.8813665948672408
Epoch 30283: reducing learning rate of group 0 to 7.4517e-04.
training: bce: 0.047495, dice: 0.235037, loss: 0.047495
training IoU in current batch 30300 is 0.891483165585245
training IoU uptillnow 30300 is 0.0034371856272158692
testing: bce: 55.351596, dice: 273.917236, loss: 55.351596
IoU in current test batch is 0.8608861200985932
Epoch 30384: reducing learning rate of group 0 to 7.4442e-04.
training: bce: 0.047393, dice: 0.234624, loss: 0.047393
training IoU in current batch 30400 is 0.8991326302072729
training IoU uptillnow 30400 is 0.0034406673466455605
testing: bce: 55.415421, dice: 274.339196, loss: 55.415421
IoU in current test batch is 0.8933781060754715
Epoch 30485: reducing learning rate of group 0 to 7.4368e-04.
training: bce: 0.047298, dice: 0.234262, loss: 0.047298
training IoU in current batch 30500 is 0.873845473148609
training IoU uptillnow 30500 is 0.003443711705909511
testing: bce: 55.485497, dice: 274.816440, loss: 55.485497
IoU in current test batch is 0.8387374031211547
Epoch 30586: reducing learning rate of group 0 to 7.4293e-04.
training: bce: 0.047188, dice: 0.233847, loss: 0.047188
training IoU in current batch 30600 is 0.9080893154441289
training IoU uptillnow 30600 is 0.003447295689672496
testing: bce: 55.538538, dice: 275.228387, loss: 55.538538
IoU in current test batch is 0.8708080576374637
Epoch 30687: reducing learning rate of group 0 to 7.4219e-04.
training: bce: 0.047073, dice: 0.233368, loss: 0.047073
training IoU in current batch 30700 is 0.9179756355517255
training IoU uptillnow 30700 is 0.0034510173355084173
testing: bce: 55.584319, dice: 275.562577, loss: 55.584319
IoU in current test batch is 0.8791813709019027
Epoch 30788: reducing learning rate of group 0 to 7.4145e-04.
training: bce: 0.047003, dice: 0.233056, loss: 0.047003
training IoU in current batch 30800 is 0.9151276840151984
training IoU uptillnow 30800 is 0.0034546685841190714
testing: bce: 55.681989, dice: 276.090334, loss: 55.681989
IoU in current test batch is 0.8759339205370318
Epoch 30889: reducing learning rate of group 0 to 7.4071e-04.
training: bce: 0.046955, dice: 0.232746, loss: 0.046955
training IoU in current batch 30900 is 0.626178154025431
training IoU uptillnow 30900 is 0.0034536207933874062
testing: bce: 55.805863, dice: 276.618287, loss: 55.805863
IoU in current test batch is 0.7120833786515198
Epoch 30990: reducing learning rate of group 0 to 7.3997e-04.
training: bce: 0.046858, dice: 0.232398, loss: 0.046858
training IoU in current batch 31000 is 0.8635484507598185
training IoU uptillnow 31000 is 0.0034564081920532936
testing: bce: 55.871406, dice: 277.099174, loss: 55.871406
IoU in current test batch is 0.8916997069295625
Epoch 31091: reducing learning rate of group 0 to 7.3923e-04.
training: bce: 0.046753, dice: 0.231964, loss: 0.046753
training IoU in current batch 31100 is 0.9027721433400947
training IoU uptillnow 31100 is 0.00345980825161616
testing: bce: 55.925509, dice: 277.473683, loss: 55.925509
IoU in current test batch is 0.8813640295898796
Epoch 31192: reducing learning rate of group 0 to 7.3849e-04.
training: bce: 0.046664, dice: 0.231564, loss: 0.046664
training IoU in current batch 31200 is 0.8486672081518127
training IoU uptillnow 31200 is 0.0034623194781446143
testing: bce: 55.998710, dice: 277.886258, loss: 55.998710
IoU in current test batch is 0.8533098793601993
Epoch 31293: reducing learning rate of group 0 to 7.3775e-04.
training: bce: 0.046561, dice: 0.231177, loss: 0.046561
training IoU in current batch 31300 is 0.9060924468736747
training IoU uptillnow 31300 is 0.003465731965784702
testing: bce: 56.053610, dice: 278.310193, loss: 56.053610
IoU in current test batch is 0.8705669932567598
Epoch 31394: reducing learning rate of group 0 to 7.3701e-04.
training: bce: 0.046456, dice: 0.230752, loss: 0.046456
training IoU in current batch 31400 is 0.878544061302682
training IoU uptillnow 31400 is 0.0034686840639367627
testing: bce: 56.106722, dice: 278.686122, loss: 56.106722
IoU in current test batch is 0.8533249298467986
Epoch 31495: reducing learning rate of group 0 to 7.3627e-04.
training: bce: 0.046365, dice: 0.230392, loss: 0.046365
training IoU in current batch 31500 is 0.8002800466744457
training IoU uptillnow 31500 is 0.003470375172693423
testing: bce: 56.174776, dice: 279.137512, loss: 56.174776
IoU in current test batch is 0.8654118445086201
Epoch 31596: reducing learning rate of group 0 to 7.3554e-04.
training: bce: 0.046275, dice: 0.230039, loss: 0.046275
training IoU in current batch 31600 is 0.9055738978442416
training IoU uptillnow 31600 is 0.003473721567796514
testing: bce: 56.243124, dice: 279.594333, loss: 56.243124
IoU in current test batch is 0.8467200693726794
Epoch 31697: reducing learning rate of group 0 to 7.3480e-04.
training: bce: 0.046174, dice: 0.229656, loss: 0.046174
training IoU in current batch 31700 is 0.7908775807504898
training IoU uptillnow 31700 is 0.003475237817555058
testing: bce: 56.298950, dice: 280.012907, loss: 56.298950
IoU in current test batch is 0.821879565608769
Epoch 31798: reducing learning rate of group 0 to 7.3407e-04.
training: bce: 0.046089, dice: 0.229315, loss: 0.046089
training IoU in current batch 31800 is 0.8673082261497178
training IoU uptillnow 31800 is 0.003477946233369635
testing: bce: 56.372258, dice: 280.479279, loss: 56.372258
IoU in current test batch is 0.8902425474534434
Epoch 31899: reducing learning rate of group 0 to 7.3333e-04.
training: bce: 0.046013, dice: 0.228980, loss: 0.046013
training IoU in current batch 31900 is 0.844954648526077
training IoU uptillnow 31900 is 0.003480287310480888
testing: bce: 56.456013, dice: 280.949402, loss: 56.456013
IoU in current test batch is 0.8553114726495884
Epoch 32000: reducing learning rate of group 0 to 7.3260e-04.
training: bce: 0.045988, dice: 0.228840, loss: 0.045988
training IoU in current batch 32000 is 0.8487750556792873
training IoU uptillnow 32000 is 0.0034826734483138166
testing: bce: 56.601785, dice: 281.658106, loss: 56.601785
IoU in current test batch is 0.8220154685210088
training: bce: 0.045898, dice: 0.228504, loss: 0.045898
training IoU in current batch 32100 is 0.7834859615578142
training IoU uptillnow 32100 is 0.0034840277873047364
testing: bce: 56.668454, dice: 282.122766, loss: 56.668454
IoU in current test batch is 0.8964358902043558
Epoch 32101: reducing learning rate of group 0 to 7.3187e-04.
training: bce: 0.045816, dice: 0.228111, loss: 0.045816
training IoU in current batch 32200 is 0.1714937916194132
training IoU uptillnow 32200 is 0.003475871025622777
testing: bce: 56.742870, dice: 282.515610, loss: 56.742870
IoU in current test batch is 0.8128643212481469
Epoch 32202: reducing learning rate of group 0 to 7.3114e-04.
training: bce: 0.045735, dice: 0.227841, loss: 0.045735
training IoU in current batch 32300 is 0.8144893750560387
training IoU uptillnow 32300 is 0.0034777179524970456
testing: bce: 56.818341, dice: 283.057948, loss: 56.818341
IoU in current test batch is 0.7859959626162596
Epoch 32303: reducing learning rate of group 0 to 7.3040e-04.
training: bce: 0.045642, dice: 0.227495, loss: 0.045642
training IoU in current batch 32400 is 0.8854533076137524
training IoU uptillnow 32400 is 0.0034806485675569873
testing: bce: 56.879012, dice: 283.502786, loss: 56.879012
IoU in current test batch is 0.8622485222703032
Epoch 32404: reducing learning rate of group 0 to 7.2967e-04.
training: bce: 0.045546, dice: 0.227124, loss: 0.045546
training IoU in current batch 32500 is 0.9092306641158786
training IoU uptillnow 32500 is 0.0034839269428470474
testing: bce: 56.934197, dice: 283.913656, loss: 56.934197
IoU in current test batch is 0.8980489438404718
Epoch 32505: reducing learning rate of group 0 to 7.2894e-04.
training: bce: 0.231837, dice: 0.993785, loss: 0.231837
training IoU in current batch 0 is 0.0
training IoU uptillnow 0 is 0.0
testing: bce: 0.008917, dice: 0.038223, loss: 0.008917
IoU in current test batch is 0.0
Epoch 32606: reducing learning rate of group 0 to 7.2822e-04.
training: bce: 0.074672, dice: 0.508590, loss: 0.074672
training IoU in current batch 100 is 0.8540723981900452
training IoU uptillnow 100 is 1.3079410070445875e-05
testing: bce: 0.290072, dice: 1.975677, loss: 0.290072
IoU in current test batch is 0.8466334366001858
Epoch 32707: reducing learning rate of group 0 to 7.2749e-04.
training: bce: 0.046383, dice: 0.312546, loss: 0.046383
training IoU in current batch 200 is 0.9446170367813447
training IoU uptillnow 200 is 2.7461326661038947e-05
testing: bce: 0.358576, dice: 2.416221, loss: 0.358576
IoU in current test batch is 0.8613046302662491
Epoch 32808: reducing learning rate of group 0 to 7.2676e-04.
training: bce: 0.035631, dice: 0.244519, loss: 0.035631
training IoU in current batch 300 is 0.9420560006345681
training IoU uptillnow 300 is 4.1716699426261555e-05
testing: bce: 0.412496, dice: 2.830778, loss: 0.412496
IoU in current test batch is 0.816262040773925
Epoch 32909: reducing learning rate of group 0 to 7.2603e-04.
training: bce: 0.030239, dice: 0.209787, loss: 0.030239
training IoU in current batch 400 is 0.8373081900536143
training IoU uptillnow 400 is 5.429602309078396e-05
testing: bce: 0.466372, dice: 3.235554, loss: 0.466372
IoU in current test batch is 0.8775314143364715
Epoch 33010: reducing learning rate of group 0 to 7.2531e-04.
training: bce: 0.026829, dice: 0.187779, loss: 0.026829
training IoU in current batch 500 is 0.9309877777419459
training IoU uptillnow 500 is 6.82164844158235e-05
testing: bce: 0.516966, dice: 3.618362, loss: 0.516966
IoU in current test batch is 0.865729261048837
Epoch 33111: reducing learning rate of group 0 to 7.2458e-04.
training: bce: 0.024688, dice: 0.173894, loss: 0.024688
training IoU in current batch 600 is 0.7212941894666741
training IoU uptillnow 600 is 7.889011286547598e-05
testing: bce: 0.570679, dice: 4.019625, loss: 0.570679
IoU in current test batch is 0.8618874492979406
Epoch 33212: reducing learning rate of group 0 to 7.2386e-04.
training: bce: 0.023657, dice: 0.166781, loss: 0.023657
training IoU in current batch 700 is 0.8773012898247161
training IoU uptillnow 700 is 9.184554478552923e-05
testing: bce: 0.637841, dice: 4.496665, loss: 0.637841
IoU in current test batch is 0.8746484690122073
Epoch 33313: reducing learning rate of group 0 to 7.2313e-04.
training: bce: 0.022353, dice: 0.158400, loss: 0.022353
training IoU in current batch 800 is 0.9037474228887901
training IoU uptillnow 800 is 0.00010511978148970297
testing: bce: 0.688645, dice: 4.879933, loss: 0.688645
IoU in current test batch is 0.828629126345679
Epoch 33414: reducing learning rate of group 0 to 7.2241e-04.
training: bce: 0.023452, dice: 0.155774, loss: 0.023452
training IoU in current batch 900 is 0.266524148161075
training IoU uptillnow 900 is 0.00010878949541462164
testing: bce: 0.812712, dice: 5.398158, loss: 0.812712
IoU in current test batch is 0.28964869084426115
Epoch 33515: reducing learning rate of group 0 to 7.2169e-04.
training: bce: 0.023846, dice: 0.156668, loss: 0.023846
training IoU in current batch 1000 is 0.8792889783905197
training IoU uptillnow 1000 is 0.00012156958273794382
testing: bce: 0.918070, dice: 6.031701, loss: 0.918070
IoU in current test batch is 0.8785133645677923
Epoch 33616: reducing learning rate of group 0 to 7.2097e-04.
training: bce: 0.023235, dice: 0.152889, loss: 0.023235
training IoU in current batch 1100 is 0.9222724703222159
training IoU uptillnow 1100 is 0.00013491240438127624
testing: bce: 0.983899, dice: 6.474263, loss: 0.983899
IoU in current test batch is 0.8781369181358616
Epoch 33717: reducing learning rate of group 0 to 7.2024e-04.
training: bce: 0.022608, dice: 0.149909, loss: 0.022608
training IoU in current batch 1200 is 0.885980198019802
training IoU uptillnow 1200 is 0.00014763848502163455
testing: bce: 1.044319, dice: 6.924641, loss: 1.044319
IoU in current test batch is 0.86523927085666
Epoch 33818: reducing learning rate of group 0 to 7.1952e-04.
training: bce: 0.022015, dice: 0.146976, loss: 0.022015
training IoU in current batch 1300 is 0.852291178062421
training IoU uptillnow 1300 is 0.00015979174402188704
testing: bce: 1.101587, dice: 7.354437, loss: 1.101587
IoU in current test batch is 0.8688869989554505
Epoch 33919: reducing learning rate of group 0 to 7.1881e-04.
training: bce: 0.021410, dice: 0.143135, loss: 0.021410
training IoU in current batch 1400 is 0.9143030303030303
training IoU uptillnow 1400 is 0.00017278670243804417
testing: bce: 1.153670, dice: 7.712777, loss: 1.153670
IoU in current test batch is 0.8801815405751635
Epoch 34020: reducing learning rate of group 0 to 7.1809e-04.
training: bce: 0.020836, dice: 0.139834, loss: 0.020836
training IoU in current batch 1500 is 0.9053379578715844
training IoU uptillnow 1500 is 0.0001855736834125662
testing: bce: 1.202879, dice: 8.072731, loss: 1.202879
IoU in current test batch is 0.8662078763694134
Epoch 34121: reducing learning rate of group 0 to 7.1737e-04.
training: bce: 0.020300, dice: 0.137098, loss: 0.020300
training IoU in current batch 1600 is 0.8459904186627786
training IoU uptillnow 1600 is 0.00019741683897824452
testing: bce: 1.249993, dice: 8.442086, loss: 1.249993
IoU in current test batch is 0.8342454903192992
Epoch 34222: reducing learning rate of group 0 to 7.1665e-04.
training: bce: 0.019908, dice: 0.134634, loss: 0.019908
training IoU in current batch 1700 is 0.887107011382389
training IoU uptillnow 1700 is 0.00020979108741379454
testing: bce: 1.302442, dice: 8.808202, loss: 1.302442
IoU in current test batch is 0.8887108701699655
Epoch 34323: reducing learning rate of group 0 to 7.1593e-04.
training: bce: 0.019664, dice: 0.133310, loss: 0.019664
training IoU in current batch 1800 is 0.8926074914270641
training IoU uptillnow 1800 is 0.000222173353151932
testing: bce: 1.362090, dice: 9.234275, loss: 1.362090
IoU in current test batch is 0.8569264702040132
Epoch 34424: reducing learning rate of group 0 to 7.1522e-04.
training: bce: 0.019359, dice: 0.131538, loss: 0.019359
training IoU in current batch 1900 is 0.8732219521026224
training IoU uptillnow 1900 is 0.00023420237072072452
testing: bce: 1.415471, dice: 9.617465, loss: 1.415471
IoU in current test batch is 0.8478361255685358
Epoch 34525: reducing learning rate of group 0 to 7.1450e-04.
training: bce: 0.019299, dice: 0.129628, loss: 0.019299
training IoU in current batch 2000 is 0.9220546654099906
training IoU uptillnow 2000 is 0.00024686846127580994
testing: bce: 1.485250, dice: 9.976379, loss: 1.485250
IoU in current test batch is 0.6063349679256782
Epoch 34626: reducing learning rate of group 0 to 7.1379e-04.
training: bce: 0.019431, dice: 0.131220, loss: 0.019431
training IoU in current batch 2100 is 0.9060331491712708
training IoU uptillnow 2100 is 0.0002592302479814783
testing: bce: 1.570173, dice: 10.603621, loss: 1.570173
IoU in current test batch is 0.8103662375312923
Epoch 34727: reducing learning rate of group 0 to 7.1307e-04.
training: bce: 0.019133, dice: 0.129675, loss: 0.019133
training IoU in current batch 2200 is 0.8929840142095915
training IoU uptillnow 2200 is 0.0002713331266504274
testing: bce: 1.619677, dice: 10.977502, loss: 1.619677
IoU in current test batch is 0.861857978319412
Epoch 34828: reducing learning rate of group 0 to 7.1236e-04.
training: bce: 0.018989, dice: 0.128628, loss: 0.018989
training IoU in current batch 2300 is 0.8950946169391879
training IoU uptillnow 2300 is 0.00028339682902218455
testing: bce: 1.680493, dice: 11.383552, loss: 1.680493
IoU in current test batch is 0.8464407163953145
Epoch 34929: reducing learning rate of group 0 to 7.1165e-04.
training: bce: 0.018734, dice: 0.127428, loss: 0.018734
training IoU in current batch 2400 is 0.8347067141255059
training IoU uptillnow 2400 is 0.0002945275654893882
testing: bce: 1.730040, dice: 11.767459, loss: 1.730040
IoU in current test batch is 0.8572311892319328
Epoch 35030: reducing learning rate of group 0 to 7.1094e-04.
training: bce: 0.018539, dice: 0.126324, loss: 0.018539
training IoU in current batch 2500 is 0.9391491224579138
training IoU uptillnow 2500 is 0.0003070847147976527
testing: bce: 1.783317, dice: 12.151417, loss: 1.783317
IoU in current test batch is 0.854138730062594
Epoch 35131: reducing learning rate of group 0 to 7.1023e-04.
training: bce: 0.018348, dice: 0.125262, loss: 0.018348
training IoU in current batch 2600 is 0.9318189872072008
training IoU uptillnow 2600 is 0.0003194661433278974
testing: bce: 1.835510, dice: 12.530976, loss: 1.835510
IoU in current test batch is 0.8664461447006027
Epoch 35232: reducing learning rate of group 0 to 7.0952e-04.
training: bce: 0.018332, dice: 0.124592, loss: 0.018332
training IoU in current batch 2700 is 0.9086778135615653
training IoU uptillnow 2700 is 0.00033144907336798285
testing: bce: 1.904434, dice: 12.943198, loss: 1.904434
IoU in current test batch is 0.8782604513363708
Epoch 35333: reducing learning rate of group 0 to 7.0881e-04.
training: bce: 0.018197, dice: 0.123896, loss: 0.018197
training IoU in current batch 2800 is 0.9164354496490942
training IoU uptillnow 2800 is 0.0003434739341860354
testing: bce: 1.960425, dice: 13.347370, loss: 1.960425
IoU in current test batch is 0.8504962779864969
Epoch 35434: reducing learning rate of group 0 to 7.0810e-04.
training: bce: 0.018014, dice: 0.123115, loss: 0.018014
training IoU in current batch 2900 is 0.7116364111270733
training IoU uptillnow 2900 is 0.0003525423501621404
testing: bce: 2.009903, dice: 13.736745, loss: 2.009903
IoU in current test batch is 0.8750677673110024
Epoch 35535: reducing learning rate of group 0 to 7.0739e-04.
training: bce: 0.017862, dice: 0.122412, loss: 0.017862
training IoU in current batch 3000 is 0.9210792093493002
training IoU uptillnow 3000 is 0.0003645055386643257
testing: bce: 2.061731, dice: 14.129199, loss: 2.061731
IoU in current test batch is 0.8616701200113845
Epoch 35636: reducing learning rate of group 0 to 7.0668e-04.
training: bce: 0.017719, dice: 0.121873, loss: 0.017719
training IoU in current batch 3100 is 0.7995163465562944
training IoU uptillnow 3100 is 0.0003746966386632518
testing: bce: 2.113275, dice: 14.535722, loss: 2.113275
IoU in current test batch is 0.8547903874488244
Epoch 35737: reducing learning rate of group 0 to 7.0598e-04.
training: bce: 0.019049, dice: 0.125186, loss: 0.019049
training IoU in current batch 3200 is 0.865231477276168
training IoU uptillnow 3200 is 0.00038574983030989745
testing: bce: 2.345184, dice: 15.412334, loss: 2.345184
IoU in current test batch is 0.7281620508436343
Epoch 35838: reducing learning rate of group 0 to 7.0527e-04.
training: bce: 0.018914, dice: 0.124659, loss: 0.018914
training IoU in current batch 3300 is 0.9025655574919953
training IoU uptillnow 3300 is 0.00039726206327590834
testing: bce: 2.401386, dice: 15.826933, loss: 2.401386
IoU in current test batch is 0.8279190480641921
Epoch 35939: reducing learning rate of group 0 to 7.0456e-04.
training: bce: 0.018801, dice: 0.124318, loss: 0.018801
training IoU in current batch 3400 is 0.9288415799118805
training IoU uptillnow 3400 is 0.0004090757069601974
testing: bce: 2.459331, dice: 16.261806, loss: 2.459331
IoU in current test batch is 0.8681438412983914
Epoch 36040: reducing learning rate of group 0 to 7.0386e-04.
training: bce: 0.018676, dice: 0.123803, loss: 0.018676
training IoU in current batch 3500 is 0.8824216293901015
training IoU uptillnow 3500 is 0.0004201799731497155
testing: bce: 2.514792, dice: 16.670607, loss: 2.514792
IoU in current test batch is 0.874675581471104
Epoch 36141: reducing learning rate of group 0 to 7.0316e-04.
training: bce: 0.018549, dice: 0.123299, loss: 0.018549
training IoU in current batch 3600 is 0.8039275851488187
training IoU uptillnow 3600 is 0.00043013711765404995
testing: bce: 2.569047, dice: 17.076869, loss: 2.569047
IoU in current test batch is 0.8591920727259106
Epoch 36242: reducing learning rate of group 0 to 7.0245e-04.
training: bce: 0.018542, dice: 0.123609, loss: 0.018542
training IoU in current batch 3700 is 0.9066146278001382
training IoU uptillnow 3700 is 0.0004414557179694933
testing: bce: 2.639389, dice: 17.595333, loss: 2.639389
IoU in current test batch is 0.8575054387726401
Epoch 36343: reducing learning rate of group 0 to 7.0175e-04.
training: bce: 0.018790, dice: 0.124520, loss: 0.018790
training IoU in current batch 3800 is 0.8565569347004857
training IoU uptillnow 3800 is 0.00045202348081501505
testing: bce: 2.746931, dice: 18.203912, loss: 2.746931
IoU in current test batch is 0.8628033439127653
Epoch 36444: reducing learning rate of group 0 to 7.0105e-04.
training: bce: 0.018671, dice: 0.124125, loss: 0.018671
training IoU in current batch 3900 is 0.9104218002609074
training IoU uptillnow 3900 is 0.0004632721550642901
testing: bce: 2.801436, dice: 18.623488, loss: 2.801436
IoU in current test batch is 0.8346001469933986
Epoch 36545: reducing learning rate of group 0 to 7.0035e-04.
training: bce: 0.018666, dice: 0.124043, loss: 0.018666
training IoU in current batch 4000 is 0.880438424352654
training IoU uptillnow 4000 is 0.0004740491013062332
testing: bce: 2.872390, dice: 19.088343, loss: 2.872390
IoU in current test batch is 0.8643382707757458
Epoch 36646: reducing learning rate of group 0 to 6.9965e-04.
training: bce: 0.018548, dice: 0.123514, loss: 0.018548
training IoU in current batch 4100 is 0.8773476206936283
training IoU uptillnow 4100 is 0.0004847250696063789
testing: bce: 2.925536, dice: 19.481938, loss: 2.925536
IoU in current test batch is 0.8624390717472976
Epoch 36747: reducing learning rate of group 0 to 6.9895e-04.
training: bce: 0.018424, dice: 0.122959, loss: 0.018424
training IoU in current batch 4200 is 0.911168852259348
training IoU uptillnow 4200 is 0.0004958030956793605
testing: bce: 2.976946, dice: 19.867365, loss: 2.976946
IoU in current test batch is 0.8315512884977072
Epoch 36848: reducing learning rate of group 0 to 6.9825e-04.
training: bce: 0.018880, dice: 0.124024, loss: 0.018880
training IoU in current batch 4300 is 0.8900553004030368
training IoU uptillnow 4300 is 0.0005065345124050577
testing: bce: 3.123175, dice: 20.516394, loss: 3.123175
IoU in current test batch is 0.8607076428236557
Epoch 36949: reducing learning rate of group 0 to 6.9755e-04.
training: bce: 0.018795, dice: 0.123673, loss: 0.018795
training IoU in current batch 4400 is 0.9190685501459585
training IoU uptillnow 4400 is 0.0005176004489896522
testing: bce: 3.181399, dice: 20.934057, loss: 3.181399
IoU in current test batch is 0.8594080008096902
training: bce: 0.018815, dice: 0.123602, loss: 0.018815
training IoU in current batch 4500 is 0.8974584801207851
training IoU uptillnow 4500 is 0.0005283150118086221
testing: bce: 3.257237, dice: 21.397356, loss: 3.257237
IoU in current test batch is 0.8715163641187851
Epoch 37050: reducing learning rate of group 0 to 6.9685e-04.
training: bce: 0.018699, dice: 0.123107, loss: 0.018699
training IoU in current batch 4600 is 0.9127710182051048
training IoU uptillnow 4600 is 0.0005391779846056098
testing: bce: 3.309022, dice: 21.785277, loss: 3.309022
IoU in current test batch is 0.8366538424351124
Epoch 37151: reducing learning rate of group 0 to 6.9616e-04.
training: bce: 0.018820, dice: 0.123549, loss: 0.018820
training IoU in current batch 4700 is 0.8731897449682471
training IoU uptillnow 4700 is 0.0005494513325437986
testing: bce: 3.402814, dice: 22.338635, loss: 3.402814
IoU in current test batch is 0.8341806958395958
Epoch 37252: reducing learning rate of group 0 to 6.9546e-04.
training: bce: 0.019386, dice: 0.125139, loss: 0.019386
training IoU in current batch 4800 is 0.8700735858426256
training IoU uptillnow 4800 is 0.0005596279523022139
testing: bce: 3.579674, dice: 23.107390, loss: 3.579674
IoU in current test batch is 0.8463035519154979
Epoch 37353: reducing learning rate of group 0 to 6.9476e-04.
training: bce: 0.019267, dice: 0.124638, loss: 0.019267
training IoU in current batch 4900 is 0.9120667269937444
training IoU uptillnow 4900 is 0.0005703108871415749
testing: bce: 3.631750, dice: 23.494319, loss: 3.631750
IoU in current test batch is 0.8719929228171568
Epoch 37454: reducing learning rate of group 0 to 6.9407e-04.
training: bce: 0.019252, dice: 0.124688, loss: 0.019252
training IoU in current batch 5000 is 0.9026362977375911
training IoU uptillnow 5000 is 0.0005808113481371844
testing: bce: 3.703032, dice: 23.983238, loss: 3.703032
IoU in current test batch is 0.8673535042139162
Epoch 37555: reducing learning rate of group 0 to 6.9338e-04.
training: bce: 0.019133, dice: 0.124242, loss: 0.019133
training IoU in current batch 5100 is 0.8519082560339656
training IoU uptillnow 5100 is 0.0005905823409313321
testing: bce: 3.753775, dice: 24.375387, loss: 3.753775
IoU in current test batch is 0.8468322732024851
Epoch 37656: reducing learning rate of group 0 to 6.9268e-04.
training: bce: 0.019203, dice: 0.124226, loss: 0.019203
training IoU in current batch 5200 is 0.8807922951117573
training IoU uptillnow 5200 is 0.0006006841413118073
testing: bce: 3.841347, dice: 24.850042, loss: 3.841347
IoU in current test batch is 0.8556630878406029
Epoch 37757: reducing learning rate of group 0 to 6.9199e-04.
training: bce: 0.019092, dice: 0.123816, loss: 0.019092
training IoU in current batch 5300 is 0.8952934955050238
training IoU uptillnow 5300 is 0.0006109241268762489
testing: bce: 3.892593, dice: 25.244258, loss: 3.892593
IoU in current test batch is 0.8597334703153976
Epoch 37858: reducing learning rate of group 0 to 6.9130e-04.
training: bce: 0.018966, dice: 0.123225, loss: 0.018966
training IoU in current batch 5400 is 0.8921055093068937
training IoU uptillnow 5400 is 0.0006210681430547445
testing: bce: 3.939773, dice: 25.597710, loss: 3.939773
IoU in current test batch is 0.8648407692526351
Epoch 37959: reducing learning rate of group 0 to 6.9061e-04.
training: bce: 0.018894, dice: 0.122991, loss: 0.018894
training IoU in current batch 5500 is 0.3161696019685129
training IoU uptillnow 5500 is 0.0006235905937223954
testing: bce: 3.997631, dice: 26.022137, loss: 3.997631
IoU in current test batch is 0.8749516722445049
Epoch 38060: reducing learning rate of group 0 to 6.8992e-04.
training: bce: 0.018873, dice: 0.123097, loss: 0.018873
training IoU in current batch 5600 is 0.9326073556605712
training IoU uptillnow 5600 is 0.000634179058013095
testing: bce: 4.065685, dice: 26.517911, loss: 4.065685
IoU in current test batch is 0.8666406509980551
Epoch 38161: reducing learning rate of group 0 to 6.8923e-04.
training: bce: 0.018784, dice: 0.122853, loss: 0.018784
training IoU in current batch 5700 is 0.9080404939842053
training IoU uptillnow 5700 is 0.0006443910174162452
testing: bce: 4.118655, dice: 26.937958, loss: 4.118655
IoU in current test batch is 0.8529512286218262
Epoch 38262: reducing learning rate of group 0 to 6.8854e-04.
training: bce: 0.018711, dice: 0.122435, loss: 0.018711
training IoU in current batch 5800 is 0.9235000167689573
training IoU uptillnow 5800 is 0.0006547512804351334
testing: bce: 4.174733, dice: 27.317239, loss: 4.174733
IoU in current test batch is 0.8611767495021626
Epoch 38363: reducing learning rate of group 0 to 6.8785e-04.
training: bce: 0.018611, dice: 0.122013, loss: 0.018611
training IoU in current batch 5900 is 0.9484178237003552
training IoU uptillnow 5900 is 0.0006653816861310895
testing: bce: 4.224064, dice: 27.692272, loss: 4.224064
IoU in current test batch is 0.8650256363101475
Epoch 38464: reducing learning rate of group 0 to 6.8716e-04.
training: bce: 0.018533, dice: 0.121698, loss: 0.018533
training IoU in current batch 6000 is 0.875331219321489
training IoU uptillnow 6000 is 0.0006750089819727381
testing: bce: 4.277512, dice: 28.088775, loss: 4.277512
IoU in current test batch is 0.8690501433189696
Epoch 38565: reducing learning rate of group 0 to 6.8647e-04.
training: bce: 0.018432, dice: 0.121209, loss: 0.018432
training IoU in current batch 6100 is 0.9365092470277411
training IoU uptillnow 6100 is 0.0006853779058997384
testing: bce: 4.325193, dice: 28.442270, loss: 4.325193
IoU in current test batch is 0.8688577785102773
Epoch 38666: reducing learning rate of group 0 to 6.8579e-04.
training: bce: 0.018345, dice: 0.120909, loss: 0.018345
training IoU in current batch 6200 is 0.9153831454232754
training IoU uptillnow 6200 is 0.0006954207137326566
testing: bce: 4.375348, dice: 28.836720, loss: 4.375348
IoU in current test batch is 0.8669320734926613
Epoch 38767: reducing learning rate of group 0 to 6.8510e-04.
training: bce: 0.018264, dice: 0.120622, loss: 0.018264
training IoU in current batch 6300 is 0.8797828829173197
training IoU uptillnow 6300 is 0.0007049536387403245
testing: bce: 4.426201, dice: 29.232365, loss: 4.426201
IoU in current test batch is 0.8892422874905745
Epoch 38868: reducing learning rate of group 0 to 6.8442e-04.
training: bce: 0.018286, dice: 0.120526, loss: 0.018286
training IoU in current batch 6400 is 0.9468254942895739
training IoU uptillnow 6400 is 0.0007152982486395724
testing: bce: 4.501946, dice: 29.672616, loss: 4.501946
IoU in current test batch is 0.8702316428002301
Epoch 38969: reducing learning rate of group 0 to 6.8373e-04.
training: bce: 0.018182, dice: 0.120102, loss: 0.018182
training IoU in current batch 6500 is 0.9180159545059632
training IoU uptillnow 6500 is 0.0007252209916295985
testing: bce: 4.546317, dice: 30.030095, loss: 4.546317
IoU in current test batch is 0.8906655153703168
Epoch 39070: reducing learning rate of group 0 to 6.8305e-04.
training: bce: 0.018190, dice: 0.120164, loss: 0.018190
training IoU in current batch 6600 is 0.8940239043824701
training IoU uptillnow 6600 is 0.0007347866272833942
testing: bce: 4.618187, dice: 30.507739, loss: 4.618187
IoU in current test batch is 0.8639403974955755
Epoch 39171: reducing learning rate of group 0 to 6.8236e-04.
training: bce: 0.018253, dice: 0.120115, loss: 0.018253
training IoU in current batch 6700 is 0.4492289417304796
training IoU uptillnow 6700 is 0.0007386372701740527
testing: bce: 4.704282, dice: 30.957382, loss: 4.704282
IoU in current test batch is 0.7422914607143644
Epoch 39272: reducing learning rate of group 0 to 6.8168e-04.
training: bce: 0.018281, dice: 0.120504, loss: 0.018281
training IoU in current batch 6800 is 0.8183863227354529
training IoU uptillnow 6800 is 0.0007471590921629045
testing: bce: 4.781964, dice: 31.521068, loss: 4.781964
IoU in current test batch is 0.8707370917312905
Epoch 39373: reducing learning rate of group 0 to 6.8100e-04.
training: bce: 0.018255, dice: 0.120529, loss: 0.018255
training IoU in current batch 6900 is 0.8506616257088847
training IoU uptillnow 6900 is 0.0007560467815794536
testing: bce: 4.845174, dice: 31.991299, loss: 4.845174
IoU in current test batch is 0.8579413102600397
Epoch 39474: reducing learning rate of group 0 to 6.8032e-04.
training: bce: 0.018177, dice: 0.120239, loss: 0.018177
training IoU in current batch 7000 is 0.876136056870332
training IoU uptillnow 7000 is 0.0007652115839227758
testing: bce: 4.894475, dice: 32.376728, loss: 4.894475
IoU in current test batch is 0.8492281493015285
Epoch 39575: reducing learning rate of group 0 to 6.7964e-04.
training: bce: 0.018109, dice: 0.119994, loss: 0.018109
training IoU in current batch 7100 is 0.8718671127545929
training IoU uptillnow 7100 is 0.0007742763236542987
testing: bce: 4.945788, dice: 32.772187, loss: 4.945788
IoU in current test batch is 0.8762487893685053
Epoch 39676: reducing learning rate of group 0 to 6.7896e-04.
training: bce: 0.018043, dice: 0.119682, loss: 0.018043
training IoU in current batch 7200 is 0.9140014938548244
training IoU uptillnow 7200 is 0.0007838254529405032
testing: bce: 4.997182, dice: 33.147194, loss: 4.997182
IoU in current test batch is 0.8322881284904433
Epoch 39777: reducing learning rate of group 0 to 6.7828e-04.
training: bce: 0.017965, dice: 0.119338, loss: 0.017965
training IoU in current batch 7300 is 0.8562014936301069
training IoU uptillnow 7300 is 0.0007926014275831211
testing: bce: 5.044595, dice: 33.510947, loss: 5.044595
IoU in current test batch is 0.8798357928055437
Epoch 39878: reducing learning rate of group 0 to 6.7760e-04.
training: bce: 0.017903, dice: 0.119087, loss: 0.017903
training IoU in current batch 7400 is 0.8871525438951008
training IoU uptillnow 7400 is 0.0008017208440761746
testing: bce: 5.096229, dice: 33.898610, loss: 5.096229
IoU in current test batch is 0.8505578581428037
Epoch 39979: reducing learning rate of group 0 to 6.7692e-04.
training: bce: 0.017830, dice: 0.118807, loss: 0.017830
training IoU in current batch 7500 is 0.9161187300394514
training IoU uptillnow 7500 is 0.0008111563496533257
testing: bce: 5.143986, dice: 34.275722, loss: 5.143986
IoU in current test batch is 0.873825858577573
Epoch 40080: reducing learning rate of group 0 to 6.7625e-04.
training: bce: 0.017762, dice: 0.118403, loss: 0.017762
training IoU in current batch 7600 is 0.9044123450943248
training IoU uptillnow 7600 is 0.0008203990684314382
testing: bce: 5.192685, dice: 34.614619, loss: 5.192685
IoU in current test batch is 0.8920110861874897
Epoch 40181: reducing learning rate of group 0 to 6.7557e-04.
training: bce: 0.017695, dice: 0.118122, loss: 0.017695
training IoU in current batch 7700 is 0.9525233904907655
training IoU uptillnow 7700 is 0.0008301935202482867
testing: bce: 5.241065, dice: 34.986902, loss: 5.241065
IoU in current test batch is 0.8875210067585338
Epoch 40282: reducing learning rate of group 0 to 6.7490e-04.
training: bce: 0.017619, dice: 0.117670, loss: 0.017619
training IoU in current batch 7800 is 0.9521785727484386
training IoU uptillnow 7800 is 0.00083993515110739
testing: bce: 5.286361, dice: 35.305532, loss: 5.286361
IoU in current test batch is 0.8761255247681405
Epoch 40387: reducing learning rate of group 0 to 6.7422e-04.
training: bce: 0.017557, dice: 0.117344, loss: 0.017557
training IoU in current batch 7900 is 0.7627379064234734
training IoU uptillnow 7900 is 0.000847286921539682
testing: bce: 5.335234, dice: 35.659051, loss: 5.335234
IoU in current test batch is 0.8826090482144118
Epoch 40488: reducing learning rate of group 0 to 6.7355e-04.
training: bce: 0.017479, dice: 0.116942, loss: 0.017479
training IoU in current batch 8000 is 0.9395534579778544
training IoU uptillnow 8000 is 0.0008567826745535282
testing: bce: 5.378836, dice: 35.986787, loss: 5.378836
IoU in current test batch is 0.8822498502553547
Epoch 40589: reducing learning rate of group 0 to 6.7287e-04.
training: bce: 0.017433, dice: 0.116688, loss: 0.017433
training IoU in current batch 8100 is 0.9418648193222243
training IoU uptillnow 8100 is 0.0008662601377992203
testing: bce: 5.431610, dice: 36.357409, loss: 5.431610
IoU in current test batch is 0.8759135363615544
Epoch 40690: reducing learning rate of group 0 to 6.7220e-04.
training: bce: 0.017376, dice: 0.116489, loss: 0.017376
training IoU in current batch 8200 is 0.9294001966568338
training IoU uptillnow 8200 is 0.0008755381432851401
testing: bce: 5.480895, dice: 36.743236, loss: 5.480895
IoU in current test batch is 0.8606343803420493
Epoch 40791: reducing learning rate of group 0 to 6.7153e-04.
training: bce: 0.017320, dice: 0.116251, loss: 0.017320
training IoU in current batch 8300 is 0.8358645303363961
training IoU uptillnow 8300 is 0.0008836258420535383
testing: bce: 5.529883, dice: 37.115209, loss: 5.529883
IoU in current test batch is 0.882542259876716
Epoch 40892: reducing learning rate of group 0 to 6.7086e-04.
training: bce: 0.017343, dice: 0.116231, loss: 0.017343
training IoU in current batch 8400 is 0.8942652329749103
training IoU uptillnow 8400 is 0.0008923871219783751
testing: bce: 5.603871, dice: 37.556017, loss: 5.603871
IoU in current test batch is 0.8702574028852406
Epoch 40993: reducing learning rate of group 0 to 6.7019e-04.
training: bce: 0.017379, dice: 0.116311, loss: 0.017379
training IoU in current batch 8500 is 0.8130455407969639
training IoU uptillnow 8500 is 0.0009001164258237483
testing: bce: 5.682318, dice: 38.029080, loss: 5.682318
IoU in current test batch is 0.8906081198345616
Epoch 41094: reducing learning rate of group 0 to 6.6952e-04.
training: bce: 0.017345, dice: 0.116135, loss: 0.017345
training IoU in current batch 8600 is 0.9519212496674395
training IoU uptillnow 8600 is 0.0009094956159050699
testing: bce: 5.737743, dice: 38.418361, loss: 5.737743
IoU in current test batch is 0.869538189155093
Epoch 41195: reducing learning rate of group 0 to 6.6885e-04.
training: bce: 0.017448, dice: 0.116574, loss: 0.017448
training IoU in current batch 8700 is 0.8786554440487753
training IoU uptillnow 8700 is 0.0009179412494384188
testing: bce: 5.839197, dice: 39.011988, loss: 5.839197
IoU in current test batch is 0.837403753142756
Epoch 41296: reducing learning rate of group 0 to 6.6818e-04.
training: bce: 0.017435, dice: 0.116602, loss: 0.017435
training IoU in current batch 8800 is 0.8872284355315593
training IoU uptillnow 8800 is 0.0009264496979764166
testing: bce: 5.901794, dice: 39.469943, loss: 5.901794
IoU in current test batch is 0.8722960825753105
Epoch 41397: reducing learning rate of group 0 to 6.6751e-04.
training: bce: 0.017380, dice: 0.116385, loss: 0.017380
training IoU in current batch 8900 is 0.8417281086902494
training IoU uptillnow 8900 is 0.0009343682273808119
testing: bce: 5.950087, dice: 39.843847, loss: 5.950087
IoU in current test batch is 0.8589902077790315
Epoch 41498: reducing learning rate of group 0 to 6.6684e-04.
training: bce: 0.017350, dice: 0.116184, loss: 0.017350
training IoU in current batch 9000 is 0.8753069793234571
training IoU uptillnow 9000 is 0.0009426527233897565
testing: bce: 6.006279, dice: 40.221884, loss: 6.006279
IoU in current test batch is 0.8591925065730334
Epoch 41599: reducing learning rate of group 0 to 6.6617e-04.
training: bce: 0.017301, dice: 0.115947, loss: 0.017301
training IoU in current batch 9100 is 0.9388552088369844
training IoU uptillnow 9100 is 0.0009516603304937917
testing: bce: 6.055934, dice: 40.585826, loss: 6.055934
IoU in current test batch is 0.8433605376853248
Epoch 41700: reducing learning rate of group 0 to 6.6551e-04.
training: bce: 0.017363, dice: 0.116168, loss: 0.017363
training IoU in current batch 9200 is 0.8309286639567606
training IoU uptillnow 9200 is 0.0009593322379161321
testing: bce: 6.144622, dice: 41.110038, loss: 6.144622
IoU in current test batch is 0.8790541920637012
Epoch 41801: reducing learning rate of group 0 to 6.6484e-04.
training: bce: 0.017313, dice: 0.115971, loss: 0.017313
training IoU in current batch 9300 is 0.9408978361502853
training IoU uptillnow 9300 is 0.0009682813458931337
testing: bce: 6.193401, dice: 41.486455, loss: 6.193401
IoU in current test batch is 0.8179240144955747
Epoch 41902: reducing learning rate of group 0 to 6.6418e-04.
training: bce: 0.017279, dice: 0.115749, loss: 0.017279
training IoU in current batch 9400 is 0.9422459023060967
training IoU uptillnow 9400 is 0.0009772038554954827
testing: bce: 6.247638, dice: 41.852319, loss: 6.247638
IoU in current test batch is 0.8535614613075425
Epoch 42003: reducing learning rate of group 0 to 6.6351e-04.
training: bce: 0.017231, dice: 0.115533, loss: 0.017231
training IoU in current batch 9500 is 0.9166862791244998
training IoU uptillnow 9500 is 0.0009857800039398804
testing: bce: 6.296544, dice: 42.218490, loss: 6.296544
IoU in current test batch is 0.8731876089439141
Epoch 42104: reducing learning rate of group 0 to 6.6285e-04.
training: bce: 0.017183, dice: 0.115342, loss: 0.017183
training IoU in current batch 9600 is 0.7891725666881213
training IoU uptillnow 9600 is 0.0009928028223113931
testing: bce: 6.345024, dice: 42.592368, loss: 6.345024
IoU in current test batch is 0.8639502827791925
training: bce: 0.017140, dice: 0.115146, loss: 0.017140
training IoU in current batch 9700 is 0.8838992332968236
training IoU uptillnow 9700 is 0.0010009134350859176
testing: bce: 6.395070, dice: 42.962630, loss: 6.395070
IoU in current test batch is 0.8695524215181736
Epoch 42282: reducing learning rate of group 0 to 6.6219e-04.
training: bce: 0.017114, dice: 0.115056, loss: 0.017114
training IoU in current batch 9800 is 0.8552527284921652
training IoU uptillnow 9800 is 0.0010086475292484813
testing: bce: 6.451260, dice: 43.371773, loss: 6.451260
IoU in current test batch is 0.8697606077220534
Epoch 42383: reducing learning rate of group 0 to 6.6153e-04.
training: bce: 0.017081, dice: 0.114881, loss: 0.017081
training IoU in current batch 9900 is 0.9108839619925136
training IoU uptillnow 9900 is 0.0010170004480831297
testing: bce: 6.504393, dice: 43.747518, loss: 6.504393
IoU in current test batch is 0.8325871984773177
Epoch 42484: reducing learning rate of group 0 to 6.6086e-04.
training: bce: 0.017146, dice: 0.115014, loss: 0.017146
training IoU in current batch 10000 is 0.901802898778428
training IoU uptillnow 10000 is 0.0010252073930432561
testing: bce: 6.595292, dice: 44.240396, loss: 6.595292
IoU in current test batch is 0.87727739806471
Epoch 42585: reducing learning rate of group 0 to 6.6020e-04.
training: bce: 0.017125, dice: 0.114953, loss: 0.017125
training IoU in current batch 10100 is 0.9289611442896114
training IoU uptillnow 10100 is 0.0010336942412557905
testing: bce: 6.653249, dice: 44.659314, loss: 6.653249
IoU in current test batch is 0.8570650490969288
Epoch 42686: reducing learning rate of group 0 to 6.5954e-04.
training: bce: 0.017111, dice: 0.114880, loss: 0.017111
training IoU in current batch 10200 is 0.9235704102645335
training IoU uptillnow 10200 is 0.0010420783341927064
testing: bce: 6.713263, dice: 45.072766, loss: 6.713263
IoU in current test batch is 0.8792559169355184
Epoch 42787: reducing learning rate of group 0 to 6.5888e-04.
training: bce: 0.017075, dice: 0.114787, loss: 0.017075
training IoU in current batch 10300 is 0.7825299220379928
training IoU uptillnow 10300 is 0.0010487775285263562
testing: bce: 6.764866, dice: 45.477605, loss: 6.764866
IoU in current test batch is 0.8728071370395027
Epoch 42888: reducing learning rate of group 0 to 6.5822e-04.
training: bce: 0.017166, dice: 0.114902, loss: 0.017166
training IoU in current batch 10400 is 0.8576006408972562
training IoU uptillnow 10400 is 0.0010563194688887818
testing: bce: 6.867091, dice: 45.965024, loss: 6.867091
IoU in current test batch is 0.8610342742869699
Epoch 42989: reducing learning rate of group 0 to 6.5757e-04.
training: bce: 0.017156, dice: 0.114848, loss: 0.017156
training IoU in current batch 10500 is 0.7885821522050471
training IoU uptillnow 10500 is 0.0010630247530201572
testing: bce: 6.929131, dice: 46.385496, loss: 6.929131
IoU in current test batch is 0.8579319467390228
Epoch 43090: reducing learning rate of group 0 to 6.5691e-04.
training: bce: 0.017112, dice: 0.114674, loss: 0.017112
training IoU in current batch 10600 is 0.9255106127352823
training IoU uptillnow 10600 is 0.0010712856327769475
testing: bce: 6.977098, dice: 46.756012, loss: 6.977098
IoU in current test batch is 0.8817184812991973
Epoch 43191: reducing learning rate of group 0 to 6.5625e-04.
training: bce: 0.017068, dice: 0.114445, loss: 0.017068
training IoU in current batch 10700 is 0.9133807369101486
training IoU uptillnow 10700 is 0.0010793680800925782
testing: bce: 7.024832, dice: 47.102949, loss: 7.024832
IoU in current test batch is 0.8904599866384549
Epoch 43292: reducing learning rate of group 0 to 6.5560e-04.
training: bce: 0.017163, dice: 0.114516, loss: 0.017163
training IoU in current batch 10800 is 0.8902021519400065
training IoU uptillnow 10800 is 0.0010871458922463689
testing: bce: 7.129726, dice: 47.572779, loss: 7.129726
IoU in current test batch is 0.831997505247699
Epoch 43393: reducing learning rate of group 0 to 6.5494e-04.
training: bce: 0.017126, dice: 0.114339, loss: 0.017126
training IoU in current batch 10900 is 0.9426869854615519
training IoU uptillnow 10900 is 0.0010954918778965175
testing: bce: 7.180448, dice: 47.938678, loss: 7.180448
IoU in current test batch is 0.8356575597809367
Epoch 43494: reducing learning rate of group 0 to 6.5429e-04.
training: bce: 0.017084, dice: 0.114142, loss: 0.017084
training IoU in current batch 11000 is 0.8339524982259883
training IoU uptillnow 11000 is 0.0011025511337162937
testing: bce: 7.228441, dice: 48.295285, loss: 7.228441
IoU in current test batch is 0.8606022416344098
Epoch 43595: reducing learning rate of group 0 to 6.5363e-04.
training: bce: 0.017067, dice: 0.114070, loss: 0.017067
training IoU in current batch 11100 is 0.9561485200631655
training IoU uptillnow 11100 is 0.001110977785720554
testing: bce: 7.287023, dice: 48.703569, loss: 7.287023
IoU in current test batch is 0.8792730116058115
Epoch 43696: reducing learning rate of group 0 to 6.5298e-04.
training: bce: 0.017039, dice: 0.113956, loss: 0.017039
training IoU in current batch 11200 is 0.869282658769332
training IoU uptillnow 11200 is 0.0011183731514004499
testing: bce: 7.340638, dice: 49.093306, loss: 7.340638
IoU in current test batch is 0.8737632604473533
Epoch 43797: reducing learning rate of group 0 to 6.5232e-04.
training: bce: 0.017033, dice: 0.113944, loss: 0.017033
training IoU in current batch 11300 is 0.8134353824404037
training IoU uptillnow 11300 is 0.0011250979801004386
testing: bce: 7.403507, dice: 49.526047, loss: 7.403507
IoU in current test batch is 0.8352110528723029
Epoch 43898: reducing learning rate of group 0 to 6.5167e-04.
training: bce: 0.016994, dice: 0.113808, loss: 0.016994
training IoU in current batch 11400 is 0.9099771167048055
training IoU uptillnow 11400 is 0.0011328905320143936
testing: bce: 7.451965, dice: 49.904903, loss: 7.451965
IoU in current test batch is 0.8158349485234043
Epoch 43999: reducing learning rate of group 0 to 6.5102e-04.
training: bce: 0.017053, dice: 0.113979, loss: 0.017053
training IoU in current batch 11500 is 0.8595415493208798
training IoU uptillnow 11500 is 0.0011400752156421078
testing: bce: 7.543534, dice: 50.418100, loss: 7.543534
IoU in current test batch is 0.8663353176742048
Epoch 44100: reducing learning rate of group 0 to 6.5037e-04.
training: bce: 0.017043, dice: 0.114065, loss: 0.017043
training IoU in current batch 11600 is 0.8279775161735072
training IoU uptillnow 11600 is 0.0011468698845856415
testing: bce: 7.604476, dice: 50.894847, loss: 7.604476
IoU in current test batch is 0.8424755152464064
Epoch 44201: reducing learning rate of group 0 to 6.4972e-04.
training: bce: 0.017019, dice: 0.113957, loss: 0.017019
training IoU in current batch 11700 is 0.8950264639166596
training IoU uptillnow 11700 is 0.0011543914666035122
testing: bce: 7.659353, dice: 51.285225, loss: 7.659353
IoU in current test batch is 0.8751769322109545
Epoch 44302: reducing learning rate of group 0 to 6.4907e-04.
training: bce: 0.016991, dice: 0.113867, loss: 0.016991
training IoU in current batch 11800 is 0.8778685674547984
training IoU uptillnow 11800 is 0.0011616856894711217
testing: bce: 7.712135, dice: 51.682351, loss: 7.712135
IoU in current test batch is 0.8766124124664483
Epoch 44403: reducing learning rate of group 0 to 6.4842e-04.
training: bce: 0.017027, dice: 0.114003, loss: 0.017027
training IoU in current batch 11900 is 0.8172360398722539
training IoU uptillnow 11900 is 0.0011682650537156918
testing: bce: 7.793903, dice: 52.182549, loss: 7.793903
IoU in current test batch is 0.8923946610858002
Epoch 44504: reducing learning rate of group 0 to 6.4777e-04.
training: bce: 0.017002, dice: 0.113870, loss: 0.017002
training IoU in current batch 12000 is 0.907375585430776
training IoU uptillnow 12000 is 0.0011758265591724044
testing: bce: 7.847858, dice: 52.559934, loss: 7.847858
IoU in current test batch is 0.8671417664419978
Epoch 44605: reducing learning rate of group 0 to 6.4712e-04.
training: bce: 0.016977, dice: 0.113777, loss: 0.016977
training IoU in current batch 12100 is 0.8795507493306444
training IoU uptillnow 12100 is 0.0011830426023251403
testing: bce: 7.901446, dice: 52.954362, loss: 7.901446
IoU in current test batch is 0.8893825197630895
Epoch 44706: reducing learning rate of group 0 to 6.4648e-04.
training: bce: 0.016951, dice: 0.113598, loss: 0.016951
training IoU in current batch 12200 is 0.9395923029610754
training IoU uptillnow 12200 is 0.001190897257488841
testing: bce: 7.954772, dice: 53.308080, loss: 7.954772
IoU in current test batch is 0.8808378802548145
Epoch 44807: reducing learning rate of group 0 to 6.4583e-04.
training: bce: 0.017052, dice: 0.113735, loss: 0.017052
training IoU in current batch 12300 is 0.9369873488113444
training IoU uptillnow 12300 is 0.0011986878448678928
testing: bce: 8.067382, dice: 53.809659, loss: 8.067382
IoU in current test batch is 0.8473992214759387
Epoch 44908: reducing learning rate of group 0 to 6.4518e-04.
training: bce: 0.017018, dice: 0.113562, loss: 0.017018
training IoU in current batch 12400 is 0.9014785465790491
training IoU uptillnow 12400 is 0.0012060487830051964
testing: bce: 8.117047, dice: 54.164827, loss: 8.117047
IoU in current test batch is 0.8824739080396663
Epoch 45009: reducing learning rate of group 0 to 6.4454e-04.
training: bce: 0.017103, dice: 0.113746, loss: 0.017103
training IoU in current batch 12500 is 0.7655403941953649
training IoU uptillnow 12500 is 0.0012118682775344846
testing: bce: 8.223473, dice: 54.689961, loss: 8.223473
IoU in current test batch is 0.8650923907944978
Epoch 45110: reducing learning rate of group 0 to 6.4389e-04.
training: bce: 0.017070, dice: 0.113622, loss: 0.017070
training IoU in current batch 12600 is 0.9279038718291055
training IoU uptillnow 12600 is 0.0012194600583551162
testing: bce: 8.273272, dice: 55.067538, loss: 8.273272
IoU in current test batch is 0.871815742991555
Epoch 45211: reducing learning rate of group 0 to 6.4325e-04.
training: bce: 0.017135, dice: 0.113740, loss: 0.017135
training IoU in current batch 12700 is 0.7737028899780294
training IoU uptillnow 12700 is 0.0012253143868925253
testing: bce: 8.370301, dice: 55.562092, loss: 8.370301
IoU in current test batch is 0.7846986827326478
Epoch 45312: reducing learning rate of group 0 to 6.4261e-04.
training: bce: 0.017240, dice: 0.114007, loss: 0.017240
training IoU in current batch 12800 is 0.8016413417431193
training IoU uptillnow 12800 is 0.0012314509315552518
testing: bce: 8.488253, dice: 56.130761, loss: 8.488253
IoU in current test batch is 0.8048526447864432
Epoch 45413: reducing learning rate of group 0 to 6.4196e-04.
training: bce: 0.017254, dice: 0.114085, loss: 0.017254
training IoU in current batch 12900 is 0.898321283138037
training IoU uptillnow 12900 is 0.001238624069838698
testing: bce: 8.561163, dice: 56.608212, loss: 8.561163
IoU in current test batch is 0.8693252363987434
Epoch 45514: reducing learning rate of group 0 to 6.4132e-04.
training: bce: 0.017226, dice: 0.114048, loss: 0.017226
training IoU in current batch 13000 is 0.8870913380519043
training IoU uptillnow 13000 is 0.0012456424402278808
testing: bce: 8.613668, dice: 57.028631, loss: 8.613668
IoU in current test batch is 0.8522675673598464
Epoch 45615: reducing learning rate of group 0 to 6.4068e-04.
training: bce: 0.017291, dice: 0.114263, loss: 0.017291
training IoU in current batch 13100 is 0.779094610392228
training IoU uptillnow 13100 is 0.001251447171083056
testing: bce: 8.712476, dice: 57.575524, loss: 8.712476
IoU in current test batch is 0.864861003766099
Epoch 45716: reducing learning rate of group 0 to 6.4004e-04.
training: bce: 0.017295, dice: 0.114174, loss: 0.017295
training IoU in current batch 13200 is 0.890328110862422
training IoU uptillnow 13200 is 0.0012584422057462306
testing: bce: 8.781333, dice: 57.969498, loss: 8.781333
IoU in current test batch is 0.8711586999714286
Epoch 45817: reducing learning rate of group 0 to 6.3940e-04.
training: bce: 0.017308, dice: 0.114218, loss: 0.017308
training IoU in current batch 13300 is 0.9116851084238292
training IoU uptillnow 13300 is 0.0012656396306611653
testing: bce: 8.854298, dice: 58.431444, loss: 8.854298
IoU in current test batch is 0.8706872757052041
Epoch 45918: reducing learning rate of group 0 to 6.3876e-04.
training: bce: 0.017281, dice: 0.114097, loss: 0.017281
training IoU in current batch 13400 is 0.8854617685131455
training IoU uptillnow 13400 is 0.0012725203784645243
testing: bce: 8.907154, dice: 58.808303, loss: 8.907154
IoU in current test batch is 0.8534370020748553
Epoch 46019: reducing learning rate of group 0 to 6.3812e-04.
training: bce: 0.017249, dice: 0.113992, loss: 0.017249
training IoU in current batch 13500 is 0.8680818111419851
training IoU uptillnow 13500 is 0.0012791825326187397
testing: bce: 8.956704, dice: 59.192428, loss: 8.956704
IoU in current test batch is 0.8058407760303443
Epoch 46120: reducing learning rate of group 0 to 6.3748e-04.
training: bce: 0.017210, dice: 0.113856, loss: 0.017210
training IoU in current batch 13600 is 0.9173946360153257
training IoU uptillnow 13600 is 0.001286350087299631
testing: bce: 9.003064, dice: 59.559663, loss: 9.003064
IoU in current test batch is 0.8851605389491619
Epoch 46221: reducing learning rate of group 0 to 6.3685e-04.
training: bce: 0.017179, dice: 0.113748, loss: 0.017179
training IoU in current batch 13700 is 0.9206295234856948
training IoU uptillnow 13700 is 0.0012935216189489005
testing: bce: 9.052747, dice: 59.940708, loss: 9.052747
IoU in current test batch is 0.8764524500020985
Epoch 46322: reducing learning rate of group 0 to 6.3621e-04.
training: bce: 0.017154, dice: 0.113623, loss: 0.017154
training IoU in current batch 13800 is 0.9062849909936225
training IoU uptillnow 13800 is 0.0013005074620238402
testing: bce: 9.105495, dice: 60.312049, loss: 9.105495
IoU in current test batch is 0.8793965764737568
training: bce: 0.017119, dice: 0.113458, loss: 0.017119
training IoU in current batch 13900 is 0.861873663526556
training IoU uptillnow 13900 is 0.0013069851654557586
testing: bce: 9.152602, dice: 60.660874, loss: 9.152602
IoU in current test batch is 0.8805576852976652
Epoch 46507: reducing learning rate of group 0 to 6.3557e-04.
training: bce: 0.017089, dice: 0.113317, loss: 0.017089
training IoU in current batch 14000 is 0.887780030147097
training IoU uptillnow 14000 is 0.0013137133042870667
testing: bce: 9.202334, dice: 61.021428, loss: 9.202334
IoU in current test batch is 0.8736510328363415
Epoch 46608: reducing learning rate of group 0 to 6.3494e-04.
training: bce: 0.017058, dice: 0.113182, loss: 0.017058
training IoU in current batch 14100 is 0.8669552126458412
training IoU uptillnow 14100 is 0.001320189392474383
testing: bce: 9.251208, dice: 61.383846, loss: 9.251208
IoU in current test batch is 0.851507381725014
Epoch 46709: reducing learning rate of group 0 to 6.3430e-04.
training: bce: 0.017024, dice: 0.112996, loss: 0.017024
training IoU in current batch 14200 is 0.906008476689105
training IoU uptillnow 14200 is 0.0013270554616108896
testing: bce: 9.298547, dice: 61.717379, loss: 9.298547
IoU in current test batch is 0.8576170705634453
Epoch 46810: reducing learning rate of group 0 to 6.3367e-04.
training: bce: 0.016986, dice: 0.112805, loss: 0.016986
training IoU in current batch 14300 is 0.8953066892623452
training IoU uptillnow 14300 is 0.0013337780050418778
testing: bce: 9.343149, dice: 62.046867, loss: 9.343149
IoU in current test batch is 0.8782808004742407
Epoch 46911: reducing learning rate of group 0 to 6.3304e-04.
training: bce: 0.016967, dice: 0.112742, loss: 0.016967
training IoU in current batch 14400 is 0.8748102546803845
training IoU uptillnow 14400 is 0.001340253629422031
testing: bce: 9.397725, dice: 62.446112, loss: 9.397725
IoU in current test batch is 0.8592754456004089
Epoch 47012: reducing learning rate of group 0 to 6.3240e-04.
training: bce: 0.016942, dice: 0.112634, loss: 0.016942
training IoU in current batch 14500 is 0.8423467715503113
training IoU uptillnow 14500 is 0.0013463567340848426
testing: bce: 9.449254, dice: 62.819622, loss: 9.449254
IoU in current test batch is 0.8655931871562739
Epoch 47113: reducing learning rate of group 0 to 6.3177e-04.
training: bce: 0.016910, dice: 0.112502, loss: 0.016910
training IoU in current batch 14600 is 0.55687812795888
training IoU uptillnow 14600 is 0.001349406679271344
testing: bce: 9.496232, dice: 63.178746, loss: 9.496232
IoU in current test batch is 0.894778461481869
Epoch 47214: reducing learning rate of group 0 to 6.3114e-04.
training: bce: 0.016920, dice: 0.112516, loss: 0.016920
training IoU in current batch 14700 is 0.8875739644970414
training IoU uptillnow 14700 is 0.0013559431783733745
testing: bce: 9.566895, dice: 63.618916, loss: 9.566895
IoU in current test batch is 0.8809481353954121
Epoch 47315: reducing learning rate of group 0 to 6.3051e-04.
training: bce: 0.016895, dice: 0.112423, loss: 0.016895
training IoU in current batch 14800 is 0.9372705767191452
training IoU uptillnow 14800 is 0.001362976852868823
testing: bce: 9.617792, dice: 63.998845, loss: 9.617792
IoU in current test batch is 0.8649119606531568
Epoch 47416: reducing learning rate of group 0 to 6.2988e-04.
training: bce: 0.016861, dice: 0.112269, loss: 0.016861
training IoU in current batch 14900 is 0.9267061747275855
training IoU uptillnow 14900 is 0.0013698695577883042
testing: bce: 9.663421, dice: 64.342823, loss: 9.663421
IoU in current test batch is 0.846629008880304
Epoch 47517: reducing learning rate of group 0 to 6.2925e-04.
training: bce: 0.016824, dice: 0.112102, loss: 0.016824
training IoU in current batch 15000 is 0.7798604099907818
training IoU uptillnow 15000 is 0.0013751891352647564
testing: bce: 9.707048, dice: 64.678569, loss: 9.707048
IoU in current test batch is 0.8888459708595666
Epoch 47618: reducing learning rate of group 0 to 6.2862e-04.
training: bce: 0.016786, dice: 0.111913, loss: 0.016786
training IoU in current batch 15100 is 0.8818589269396458
training IoU uptillnow 15100 is 0.001381556684765661
testing: bce: 9.749324, dice: 65.000155, loss: 9.749324
IoU in current test batch is 0.846691103583592
Epoch 47719: reducing learning rate of group 0 to 6.2799e-04.
training: bce: 0.016751, dice: 0.111731, loss: 0.016751
training IoU in current batch 15200 is 0.8980287474332649
training IoU uptillnow 15200 is 0.0013880668828879464
testing: bce: 9.793652, dice: 65.323670, loss: 9.793652
IoU in current test batch is 0.8937976004906895
Epoch 47820: reducing learning rate of group 0 to 6.2736e-04.
training: bce: 0.016717, dice: 0.111598, loss: 0.016717
training IoU in current batch 15300 is 0.9348040382066012
training IoU uptillnow 15300 is 0.001394934150692511
testing: bce: 9.838157, dice: 65.675365, loss: 9.838157
IoU in current test batch is 0.8768852118223491
Epoch 47921: reducing learning rate of group 0 to 6.2673e-04.
training: bce: 0.016761, dice: 0.111656, loss: 0.016761
training IoU in current batch 15400 is 0.8787131665805066
training IoU uptillnow 15400 is 0.0014011878794742709
testing: bce: 9.928356, dice: 66.139273, loss: 9.928356
IoU in current test batch is 0.8597769119471905
Epoch 48022: reducing learning rate of group 0 to 6.2611e-04.
training: bce: 0.016743, dice: 0.111583, loss: 0.016743
training IoU in current batch 15500 is 0.8782648862013941
training IoU uptillnow 15500 is 0.0014074109131198503
testing: bce: 9.982150, dice: 66.524821, loss: 9.982150
IoU in current test batch is 0.8847210316570416
Epoch 48123: reducing learning rate of group 0 to 6.2548e-04.
training: bce: 0.016744, dice: 0.111585, loss: 0.016744
training IoU in current batch 15600 is 0.8945972964749859
training IoU uptillnow 15600 is 0.0014137776990039304
testing: bce: 10.046848, dice: 66.955097, loss: 10.046848
IoU in current test batch is 0.8605909843006508
Epoch 48224: reducing learning rate of group 0 to 6.2486e-04.
training: bce: 0.016714, dice: 0.111442, loss: 0.016714
training IoU in current batch 15700 is 0.8673904614191547
training IoU uptillnow 15700 is 0.0014198361547559937
testing: bce: 10.093348, dice: 67.298197, loss: 10.093348
IoU in current test batch is 0.8772058911068646
Epoch 48325: reducing learning rate of group 0 to 6.2423e-04.
training: bce: 0.016743, dice: 0.111516, loss: 0.016743
training IoU in current batch 15800 is 0.6731433506044905
training IoU uptillnow 15800 is 0.001423860768450585
testing: bce: 10.175253, dice: 67.771969, loss: 10.175253
IoU in current test batch is 0.8068841658218276
Epoch 48426: reducing learning rate of group 0 to 6.2361e-04.
training: bce: 0.016725, dice: 0.111425, loss: 0.016725
training IoU in current batch 15900 is 0.8907942797003653
training IoU uptillnow 15900 is 0.0014301149313006687
testing: bce: 10.228882, dice: 68.145014, loss: 10.228882
IoU in current test batch is 0.8824436179733597
Epoch 48527: reducing learning rate of group 0 to 6.2298e-04.
training: bce: 0.016708, dice: 0.111289, loss: 0.016708
training IoU in current batch 16000 is 0.9056666543451742
training IoU uptillnow 16000 is 0.0014364964972085053
testing: bce: 10.282661, dice: 68.489705, loss: 10.282661
IoU in current test batch is 0.8853180489978022
Epoch 48628: reducing learning rate of group 0 to 6.2236e-04.
training: bce: 0.016685, dice: 0.111235, loss: 0.016685
training IoU in current batch 16100 is 0.8504761904761905
training IoU uptillnow 16100 is 0.001442284602852289
testing: bce: 10.332734, dice: 68.884206, loss: 10.332734
IoU in current test batch is 0.8648030781067095
Epoch 48729: reducing learning rate of group 0 to 6.2174e-04.
training: bce: 0.016814, dice: 0.111497, loss: 0.016814
training IoU in current batch 16200 is 0.9188749336635414
training IoU uptillnow 16200 is 0.0014487504949444445
testing: bce: 10.476916, dice: 69.475334, loss: 10.476916
IoU in current test batch is 0.8748896775588244
Epoch 48830: reducing learning rate of group 0 to 6.2112e-04.
training: bce: 0.016794, dice: 0.111439, loss: 0.016794
training IoU in current batch 16300 is 0.8144002744543076
training IoU uptillnow 16300 is 0.001454120561940682
testing: bce: 10.529022, dice: 69.867764, loss: 10.529022
IoU in current test batch is 0.885641605441008
Epoch 48931: reducing learning rate of group 0 to 6.2049e-04.
training: bce: 0.016769, dice: 0.111360, loss: 0.016769
training IoU in current batch 16400 is 0.8908784326320861
training IoU uptillnow 16400 is 0.0014602498821609496
testing: bce: 10.578086, dice: 70.246456, loss: 10.578086
IoU in current test batch is 0.8757261181087111
Epoch 49032: reducing learning rate of group 0 to 6.1987e-04.
training: bce: 0.016810, dice: 0.111475, loss: 0.016810
training IoU in current batch 16500 is 0.8984477992770572
training IoU uptillnow 16500 is 0.0014664313704823887
testing: bce: 10.668808, dice: 70.747907, loss: 10.668808
IoU in current test batch is 0.8589418530042582
Epoch 49133: reducing learning rate of group 0 to 6.1925e-04.
training: bce: 0.016797, dice: 0.111460, loss: 0.016797
training IoU in current batch 16600 is 0.8479106537834107
training IoU uptillnow 16600 is 0.0014720735884061408
testing: bce: 10.725076, dice: 71.166905, loss: 10.725076
IoU in current test batch is 0.8808057980212554
Epoch 49234: reducing learning rate of group 0 to 6.1863e-04.
training: bce: 0.016795, dice: 0.111470, loss: 0.016795
training IoU in current batch 16700 is 0.9422541247053782
training IoU uptillnow 16700 is 0.0014786507049964024
testing: bce: 10.787942, dice: 71.602583, loss: 10.787942
IoU in current test batch is 0.8662228494499614
Epoch 49335: reducing learning rate of group 0 to 6.1802e-04.
training: bce: 0.016766, dice: 0.111345, loss: 0.016766
training IoU in current batch 16800 is 0.8839051326151897
training IoU uptillnow 16800 is 0.0014846099851473248
testing: bce: 10.833976, dice: 71.950105, loss: 10.833976
IoU in current test batch is 0.8774543154085636
Epoch 49436: reducing learning rate of group 0 to 6.1740e-04.
training: bce: 0.016735, dice: 0.111202, loss: 0.016735
training IoU in current batch 16900 is 0.9330378229117465
training IoU uptillnow 16900 is 0.0014910419594431447
testing: bce: 10.878492, dice: 72.285254, loss: 10.878492
IoU in current test batch is 0.8760347863303332
Epoch 49537: reducing learning rate of group 0 to 6.1678e-04.
training: bce: 0.016715, dice: 0.111126, loss: 0.016715
training IoU in current batch 17000 is 0.9314319904268049
training IoU uptillnow 17000 is 0.00149743176760002
testing: bce: 10.929649, dice: 72.663692, loss: 10.929649
IoU in current test batch is 0.8609644640093506
Epoch 49638: reducing learning rate of group 0 to 6.1616e-04.
training: bce: 0.016699, dice: 0.111079, loss: 0.016699
training IoU in current batch 17100 is 0.8634672845905238
training IoU uptillnow 17100 is 0.0015031113910712584
testing: bce: 10.983379, dice: 73.060208, loss: 10.983379
IoU in current test batch is 0.8738766871696844
Epoch 49739: reducing learning rate of group 0 to 6.1555e-04.
training: bce: 0.016700, dice: 0.111046, loss: 0.016700
training IoU in current batch 17200 is 0.8471356740728008
training IoU uptillnow 17200 is 0.0015086040432170946
testing: bce: 11.048216, dice: 73.465277, loss: 11.048216
IoU in current test batch is 0.8868997570132835
Epoch 49840: reducing learning rate of group 0 to 6.1493e-04.
training: bce: 0.016708, dice: 0.111020, loss: 0.016708
training IoU in current batch 17300 is 0.7636992028541167
training IoU uptillnow 17300 is 0.0015132377746909379
testing: bce: 11.117611, dice: 73.875575, loss: 11.117611
IoU in current test batch is 0.7795037401813154
Epoch 49941: reducing learning rate of group 0 to 6.1432e-04.
training: bce: 0.016688, dice: 0.110941, loss: 0.016688
training IoU in current batch 17400 is 0.8917939028749425
training IoU uptillnow 17400 is 0.0015191351945643776
testing: bce: 11.168806, dice: 74.249418, loss: 11.168806
IoU in current test batch is 0.8645879362809926
Maximum training samples requirement meet, I have been training for more than  100001  samples.
Making network now
ResNetUNet(
  (base_model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer0_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer1_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2_1x1): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3_1x1): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4_1x1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (upsample): Upsample(scale_factor=2.0, mode=bilinear)
  (conv_up3): Sequential(
    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up2): Sequential(
    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up1): Sequential(
    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up0): Sequential(
    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size2): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 512, 512]           1,792
              ReLU-2         [-1, 64, 512, 512]               0
            Conv2d-3         [-1, 64, 512, 512]          36,928
              ReLU-4         [-1, 64, 512, 512]               0
            Conv2d-5         [-1, 64, 256, 256]           9,408
            Conv2d-6         [-1, 64, 256, 256]           9,408
       BatchNorm2d-7         [-1, 64, 256, 256]             128
       BatchNorm2d-8         [-1, 64, 256, 256]             128
              ReLU-9         [-1, 64, 256, 256]               0
             ReLU-10         [-1, 64, 256, 256]               0
        MaxPool2d-11         [-1, 64, 128, 128]               0
        MaxPool2d-12         [-1, 64, 128, 128]               0
           Conv2d-13         [-1, 64, 128, 128]          36,864
           Conv2d-14         [-1, 64, 128, 128]          36,864
      BatchNorm2d-15         [-1, 64, 128, 128]             128
      BatchNorm2d-16         [-1, 64, 128, 128]             128
             ReLU-17         [-1, 64, 128, 128]               0
             ReLU-18         [-1, 64, 128, 128]               0
           Conv2d-19         [-1, 64, 128, 128]          36,864
           Conv2d-20         [-1, 64, 128, 128]          36,864
      BatchNorm2d-21         [-1, 64, 128, 128]             128
      BatchNorm2d-22         [-1, 64, 128, 128]             128
             ReLU-23         [-1, 64, 128, 128]               0
             ReLU-24         [-1, 64, 128, 128]               0
       BasicBlock-25         [-1, 64, 128, 128]               0
       BasicBlock-26         [-1, 64, 128, 128]               0
           Conv2d-27         [-1, 64, 128, 128]          36,864
           Conv2d-28         [-1, 64, 128, 128]          36,864
      BatchNorm2d-29         [-1, 64, 128, 128]             128
      BatchNorm2d-30         [-1, 64, 128, 128]             128
             ReLU-31         [-1, 64, 128, 128]               0
             ReLU-32         [-1, 64, 128, 128]               0
           Conv2d-33         [-1, 64, 128, 128]          36,864
           Conv2d-34         [-1, 64, 128, 128]          36,864
      BatchNorm2d-35         [-1, 64, 128, 128]             128
      BatchNorm2d-36         [-1, 64, 128, 128]             128
             ReLU-37         [-1, 64, 128, 128]               0
             ReLU-38         [-1, 64, 128, 128]               0
       BasicBlock-39         [-1, 64, 128, 128]               0
       BasicBlock-40         [-1, 64, 128, 128]               0
           Conv2d-41          [-1, 128, 64, 64]          73,728
           Conv2d-42          [-1, 128, 64, 64]          73,728
      BatchNorm2d-43          [-1, 128, 64, 64]             256
      BatchNorm2d-44          [-1, 128, 64, 64]             256
             ReLU-45          [-1, 128, 64, 64]               0
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]         147,456
           Conv2d-48          [-1, 128, 64, 64]         147,456
      BatchNorm2d-49          [-1, 128, 64, 64]             256
      BatchNorm2d-50          [-1, 128, 64, 64]             256
           Conv2d-51          [-1, 128, 64, 64]           8,192
           Conv2d-52          [-1, 128, 64, 64]           8,192
      BatchNorm2d-53          [-1, 128, 64, 64]             256
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
             ReLU-56          [-1, 128, 64, 64]               0
       BasicBlock-57          [-1, 128, 64, 64]               0
       BasicBlock-58          [-1, 128, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]         147,456
           Conv2d-60          [-1, 128, 64, 64]         147,456
      BatchNorm2d-61          [-1, 128, 64, 64]             256
      BatchNorm2d-62          [-1, 128, 64, 64]             256
             ReLU-63          [-1, 128, 64, 64]               0
             ReLU-64          [-1, 128, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]         147,456
           Conv2d-66          [-1, 128, 64, 64]         147,456
      BatchNorm2d-67          [-1, 128, 64, 64]             256
      BatchNorm2d-68          [-1, 128, 64, 64]             256
             ReLU-69          [-1, 128, 64, 64]               0
             ReLU-70          [-1, 128, 64, 64]               0
       BasicBlock-71          [-1, 128, 64, 64]               0
       BasicBlock-72          [-1, 128, 64, 64]               0
           Conv2d-73          [-1, 256, 32, 32]         294,912
           Conv2d-74          [-1, 256, 32, 32]         294,912
      BatchNorm2d-75          [-1, 256, 32, 32]             512
      BatchNorm2d-76          [-1, 256, 32, 32]             512
             ReLU-77          [-1, 256, 32, 32]               0
             ReLU-78          [-1, 256, 32, 32]               0
           Conv2d-79          [-1, 256, 32, 32]         589,824
           Conv2d-80          [-1, 256, 32, 32]         589,824
      BatchNorm2d-81          [-1, 256, 32, 32]             512
      BatchNorm2d-82          [-1, 256, 32, 32]             512
           Conv2d-83          [-1, 256, 32, 32]          32,768
           Conv2d-84          [-1, 256, 32, 32]          32,768
      BatchNorm2d-85          [-1, 256, 32, 32]             512
      BatchNorm2d-86          [-1, 256, 32, 32]             512
             ReLU-87          [-1, 256, 32, 32]               0
             ReLU-88          [-1, 256, 32, 32]               0
       BasicBlock-89          [-1, 256, 32, 32]               0
       BasicBlock-90          [-1, 256, 32, 32]               0
           Conv2d-91          [-1, 256, 32, 32]         589,824
           Conv2d-92          [-1, 256, 32, 32]         589,824
      BatchNorm2d-93          [-1, 256, 32, 32]             512
      BatchNorm2d-94          [-1, 256, 32, 32]             512
             ReLU-95          [-1, 256, 32, 32]               0
             ReLU-96          [-1, 256, 32, 32]               0
           Conv2d-97          [-1, 256, 32, 32]         589,824
           Conv2d-98          [-1, 256, 32, 32]         589,824
      BatchNorm2d-99          [-1, 256, 32, 32]             512
     BatchNorm2d-100          [-1, 256, 32, 32]             512
            ReLU-101          [-1, 256, 32, 32]               0
            ReLU-102          [-1, 256, 32, 32]               0
      BasicBlock-103          [-1, 256, 32, 32]               0
      BasicBlock-104          [-1, 256, 32, 32]               0
          Conv2d-105          [-1, 512, 16, 16]       1,179,648
          Conv2d-106          [-1, 512, 16, 16]       1,179,648
     BatchNorm2d-107          [-1, 512, 16, 16]           1,024
     BatchNorm2d-108          [-1, 512, 16, 16]           1,024
            ReLU-109          [-1, 512, 16, 16]               0
            ReLU-110          [-1, 512, 16, 16]               0
          Conv2d-111          [-1, 512, 16, 16]       2,359,296
          Conv2d-112          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-113          [-1, 512, 16, 16]           1,024
     BatchNorm2d-114          [-1, 512, 16, 16]           1,024
          Conv2d-115          [-1, 512, 16, 16]         131,072
          Conv2d-116          [-1, 512, 16, 16]         131,072
     BatchNorm2d-117          [-1, 512, 16, 16]           1,024
     BatchNorm2d-118          [-1, 512, 16, 16]           1,024
            ReLU-119          [-1, 512, 16, 16]               0
            ReLU-120          [-1, 512, 16, 16]               0
      BasicBlock-121          [-1, 512, 16, 16]               0
      BasicBlock-122          [-1, 512, 16, 16]               0
          Conv2d-123          [-1, 512, 16, 16]       2,359,296
          Conv2d-124          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-125          [-1, 512, 16, 16]           1,024
     BatchNorm2d-126          [-1, 512, 16, 16]           1,024
            ReLU-127          [-1, 512, 16, 16]               0
            ReLU-128          [-1, 512, 16, 16]               0
          Conv2d-129          [-1, 512, 16, 16]       2,359,296
          Conv2d-130          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-131          [-1, 512, 16, 16]           1,024
     BatchNorm2d-132          [-1, 512, 16, 16]           1,024
            ReLU-133          [-1, 512, 16, 16]               0
            ReLU-134          [-1, 512, 16, 16]               0
      BasicBlock-135          [-1, 512, 16, 16]               0
      BasicBlock-136          [-1, 512, 16, 16]               0
          Conv2d-137          [-1, 512, 16, 16]         262,656
            ReLU-138          [-1, 512, 16, 16]               0
        Upsample-139          [-1, 512, 32, 32]               0
          Conv2d-140          [-1, 256, 32, 32]          65,792
            ReLU-141          [-1, 256, 32, 32]               0
          Conv2d-142          [-1, 512, 32, 32]       3,539,456
            ReLU-143          [-1, 512, 32, 32]               0
        Upsample-144          [-1, 512, 64, 64]               0
          Conv2d-145          [-1, 128, 64, 64]          16,512
            ReLU-146          [-1, 128, 64, 64]               0
          Conv2d-147          [-1, 256, 64, 64]       1,474,816
            ReLU-148          [-1, 256, 64, 64]               0
        Upsample-149        [-1, 256, 128, 128]               0
          Conv2d-150         [-1, 64, 128, 128]           4,160
            ReLU-151         [-1, 64, 128, 128]               0
          Conv2d-152        [-1, 256, 128, 128]         737,536
            ReLU-153        [-1, 256, 128, 128]               0
        Upsample-154        [-1, 256, 256, 256]               0
          Conv2d-155         [-1, 64, 256, 256]           4,160
            ReLU-156         [-1, 64, 256, 256]               0
          Conv2d-157        [-1, 128, 256, 256]         368,768
            ReLU-158        [-1, 128, 256, 256]               0
        Upsample-159        [-1, 128, 512, 512]               0
          Conv2d-160         [-1, 64, 512, 512]         110,656
            ReLU-161         [-1, 64, 512, 512]               0
          Conv2d-162          [-1, 1, 512, 512]              65
================================================================
Total params: 28,976,321
Trainable params: 28,976,321
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 2172.00
Params size (MB): 110.54
Estimated Total Size (MB): 2285.54
----------------------------------------------------------------
Start training now...
training: bce: 0.623077, dice: 0.979254, loss: 0.801166
training IoU in current batch 0 is 0.0002298916635535504
training IoU uptillnow 0 is 0.0001149458317767752
testing: bce: 0.023964, dice: 0.037664, loss: 0.030814
IoU in current test batch is 0.0
training: bce: 0.628555, dice: 0.607349, loss: 0.617952
training IoU in current batch 100 is 0.6933343004279393
training IoU uptillnow 100 is 0.003433486099462836
testing: bce: 2.441695, dice: 2.359316, loss: 2.400506
IoU in current test batch is 0.6815829936336159
training: bce: 0.362979, dice: 0.419503, loss: 0.391241
training IoU in current batch 200 is 0.46761097716635935
training IoU uptillnow 200 is 0.0028884954459150554
testing: bce: 2.806107, dice: 3.243079, loss: 3.024593
IoU in current test batch is 0.5910337203636736
training: bce: 0.324265, dice: 0.467936, loss: 0.396101
training IoU in current batch 300 is 0.0
training IoU uptillnow 300 is 0.0019288624074050701
testing: bce: 3.753996, dice: 5.417255, loss: 4.585626
IoU in current test batch is 0.0
Epoch   326: reducing learning rate of group 0 to 9.9900e-04.
training: bce: 0.289830, dice: 0.488594, loss: 0.389212
training IoU in current batch 400 is 0.7420523801680119
training IoU uptillnow 400 is 0.0023731016825758906
testing: bce: 4.470065, dice: 7.535616, loss: 6.002840
IoU in current test batch is 0.632038096381284
Epoch   427: reducing learning rate of group 0 to 9.9800e-04.
training: bce: 0.249485, dice: 0.451030, loss: 0.350257
training IoU in current batch 500 is 0.5965743875721967
training IoU uptillnow 500 is 0.0024948123123733146
testing: bce: 4.807389, dice: 8.690993, loss: 6.749191
IoU in current test batch is 0.610213912594374
Epoch   528: reducing learning rate of group 0 to 9.9700e-04.
training: bce: 0.219831, dice: 0.419135, loss: 0.319483
training IoU in current batch 600 is 0.5947352766210589
training IoU uptillnow 600 is 0.002574490194358669
testing: bce: 5.081480, dice: 9.688459, loss: 7.384969
IoU in current test batch is 0.6855808043489778
Epoch   629: reducing learning rate of group 0 to 9.9601e-04.
training: bce: 0.198186, dice: 0.394976, loss: 0.296581
training IoU in current batch 700 is 0.5834348905438953
training IoU uptillnow 700 is 0.002623375252612707
testing: bce: 5.343412, dice: 10.649169, loss: 7.996291
IoU in current test batch is 0.7057781431032946
Epoch   730: reducing learning rate of group 0 to 9.9501e-04.
training: bce: 0.184698, dice: 0.377130, loss: 0.280914
training IoU in current batch 800 is 0.22417916113114442
training IoU uptillnow 800 is 0.0024357997910700122
testing: bce: 5.690109, dice: 11.618512, loss: 8.654310
IoU in current test batch is 0.5431669809583749
Epoch   831: reducing learning rate of group 0 to 9.9401e-04.
training: bce: 0.172632, dice: 0.364138, loss: 0.268385
training IoU in current batch 900 is 0.6703277545327755
training IoU uptillnow 900 is 0.002537446736862894
testing: bce: 5.982362, dice: 12.618788, loss: 9.300575
IoU in current test batch is 0.6765154129493335
Epoch   932: reducing learning rate of group 0 to 9.9302e-04.
training: bce: 0.160783, dice: 0.349297, loss: 0.255040
training IoU in current batch 1000 is 0.7605024424284718
training IoU uptillnow 1000 is 0.0026638269042234797
testing: bce: 6.190162, dice: 13.447944, loss: 9.819053
IoU in current test batch is 0.6932890845477667
Epoch  1033: reducing learning rate of group 0 to 9.9203e-04.
training: bce: 0.152418, dice: 0.335412, loss: 0.243915
training IoU in current batch 1100 is 0.7912517289073305
training IoU uptillnow 1100 is 0.002781213983270998
testing: bce: 6.454330, dice: 14.203414, loss: 10.328872
IoU in current test batch is 0.7369271891086752
Epoch  1134: reducing learning rate of group 0 to 9.9104e-04.
training: bce: 0.144403, dice: 0.323455, loss: 0.233929
training IoU in current batch 1200 is 0.6354052934753962
training IoU uptillnow 1200 is 0.0028141708928551765
testing: bce: 6.670301, dice: 14.941131, loss: 10.805716
IoU in current test batch is 0.6986990522958849
Epoch  1235: reducing learning rate of group 0 to 9.9004e-04.
training: bce: 0.136692, dice: 0.311339, loss: 0.224015
training IoU in current batch 1300 is 0.8752793180501531
training IoU uptillnow 1300 is 0.002934249732009334
testing: bce: 6.839840, dice: 15.578924, loss: 11.209382
IoU in current test batch is 0.7433126676404951
Epoch  1336: reducing learning rate of group 0 to 9.8905e-04.
training: bce: 0.130374, dice: 0.301260, loss: 0.215817
training IoU in current batch 1400 is 0.7107045094366806
training IoU uptillnow 1400 is 0.0029784519315221155
testing: bce: 7.025173, dice: 16.233275, loss: 11.629224
IoU in current test batch is 0.7924316243045294
Epoch  1437: reducing learning rate of group 0 to 9.8807e-04.
training: bce: 0.125188, dice: 0.292771, loss: 0.208979
training IoU in current batch 1500 is 0.7090666072621965
training IoU uptillnow 1500 is 0.003016218827244225
testing: bce: 7.227197, dice: 16.901879, loss: 12.064538
IoU in current test batch is 0.7165044844600966
Epoch  1538: reducing learning rate of group 0 to 9.8708e-04.
training: bce: 0.120266, dice: 0.284980, loss: 0.202623
training IoU in current batch 1600 is 0.8290121430915655
training IoU uptillnow 1600 is 0.0030867273774137197
testing: bce: 7.405627, dice: 17.548209, loss: 12.476918
IoU in current test batch is 0.7853533703954365
Epoch  1639: reducing learning rate of group 0 to 9.8609e-04.
training: bce: 0.115561, dice: 0.277110, loss: 0.196335
training IoU in current batch 1700 is 0.8310955658354959
training IoU uptillnow 1700 is 0.003149558091803123
testing: bce: 7.560335, dice: 18.129404, loss: 12.844870
IoU in current test batch is 0.8016231527829216
training: bce: 0.111358, dice: 0.270278, loss: 0.190818
training IoU in current batch 1800 is 0.7524569266828406
training IoU uptillnow 1800 is 0.0031835795544134
testing: bce: 7.713716, dice: 18.721948, loss: 13.217832
IoU in current test batch is 0.7544500585957383
Epoch  1830: reducing learning rate of group 0 to 9.8510e-04.
training: bce: 0.107867, dice: 0.263335, loss: 0.185601
training IoU in current batch 1900 is 0.9182573380099687
training IoU uptillnow 1900 is 0.0032576304295126343
testing: bce: 7.886758, dice: 19.253829, loss: 13.570293
IoU in current test batch is 0.7813119078775308
training: bce: 0.103879, dice: 0.255752, loss: 0.179815
training IoU in current batch 2000 is 0.8740683380118092
training IoU uptillnow 2000 is 0.003313238188660381
testing: bce: 7.994676, dice: 19.683070, loss: 13.838873
IoU in current test batch is 0.7425957862632598
Epoch  2019: reducing learning rate of group 0 to 9.8412e-04.
training: bce: 0.100234, dice: 0.248531, loss: 0.174382
training IoU in current batch 2100 is 0.8010260749986777
training IoU uptillnow 2100 is 0.0033461697539308717
testing: bce: 8.099711, dice: 20.083179, loss: 14.091445
IoU in current test batch is 0.7642631524235658
training: bce: 0.097383, dice: 0.243016, loss: 0.170199
training IoU in current batch 2200 is 0.846628797233885
training IoU uptillnow 2200 is 0.003386468446899457
testing: bce: 8.243826, dice: 20.572205, loss: 14.408016
IoU in current test batch is 0.8145330223050714
Epoch  2221: reducing learning rate of group 0 to 9.8314e-04.
training: bce: 0.094454, dice: 0.237155, loss: 0.165805
training IoU in current batch 2300 is 0.8842118106393363
training IoU uptillnow 2300 is 0.0034314310981944252
testing: bce: 8.359147, dice: 20.988253, loss: 14.673700
IoU in current test batch is 0.7616892514283997
training: bce: 0.091599, dice: 0.231155, loss: 0.161377
training IoU in current batch 2400 is 0.8671545268890402
training IoU uptillnow 2400 is 0.0034690963017034123
testing: bce: 8.458810, dice: 21.346266, loss: 14.902538
IoU in current test batch is 0.8245742029594206
Epoch  2432: reducing learning rate of group 0 to 9.8215e-04.
training: bce: 0.089150, dice: 0.226099, loss: 0.157624
training IoU in current batch 2500 is 0.8910430399379604
training IoU uptillnow 2500 is 0.003508525286029138
testing: bce: 8.575585, dice: 21.748939, loss: 15.162262
IoU in current test batch is 0.8350989014883076
Epoch  2533: reducing learning rate of group 0 to 9.8117e-04.
training: bce: 0.086758, dice: 0.221263, loss: 0.154011
training IoU in current batch 2600 is 0.8562344184702099
training IoU uptillnow 2600 is 0.003538231045595532
testing: bce: 8.679112, dice: 22.134836, loss: 15.406974
IoU in current test batch is 0.8592844239051848
Epoch  2634: reducing learning rate of group 0 to 9.8019e-04.
training: bce: 0.084849, dice: 0.217417, loss: 0.151133
training IoU in current batch 2700 is 0.8926472219141621
training IoU uptillnow 2700 is 0.0035724778084231987
testing: bce: 8.814542, dice: 22.586273, loss: 15.700408
IoU in current test batch is 0.8379522665428258
Epoch  2735: reducing learning rate of group 0 to 9.7921e-04.
training: bce: 0.082852, dice: 0.213353, loss: 0.148102
training IoU in current batch 2800 is 0.7966307562315167
training IoU uptillnow 2800 is 0.0035871395711056116
testing: bce: 8.925717, dice: 22.984629, loss: 15.955173
IoU in current test batch is 0.8549787448757702
Epoch  2836: reducing learning rate of group 0 to 9.7823e-04.
training: bce: 0.081014, dice: 0.209588, loss: 0.145301
training IoU in current batch 2900 is 0.8278350844697091
training IoU uptillnow 2900 is 0.003606168728335633
testing: bce: 9.039250, dice: 23.385170, loss: 16.212210
IoU in current test batch is 0.7275969429821931
Epoch  2937: reducing learning rate of group 0 to 9.7725e-04.
training: bce: 0.079324, dice: 0.205909, loss: 0.142617
training IoU in current batch 3000 is 0.8963227783452502
training IoU uptillnow 3000 is 0.003635340509854814
testing: bce: 9.155817, dice: 23.766697, loss: 16.461257
IoU in current test batch is 0.8254633412188089
Epoch  3038: reducing learning rate of group 0 to 9.7627e-04.
training: bce: 0.077887, dice: 0.202677, loss: 0.140282
training IoU in current batch 3100 is 0.7498485766202302
training IoU uptillnow 3100 is 0.0036390135950933288
testing: bce: 9.289467, dice: 24.173130, loss: 16.731298
IoU in current test batch is 0.7702397522937314
Epoch  3164: reducing learning rate of group 0 to 9.7530e-04.
training: bce: 0.076421, dice: 0.199579, loss: 0.138000
training IoU in current batch 3200 is 0.8804627249357326
training IoU uptillnow 3200 is 0.0036628592692446982
testing: bce: 9.408547, dice: 24.571231, loss: 16.989889
IoU in current test batch is 0.8802302110686434
Epoch  3265: reducing learning rate of group 0 to 9.7432e-04.
training: bce: 0.074849, dice: 0.196331, loss: 0.135590
training IoU in current batch 3300 is 0.8931647503076074
training IoU uptillnow 3300 is 0.0036871841551063566
testing: bce: 9.503002, dice: 24.926537, loss: 17.214769
IoU in current test batch is 0.8550517472794896
Epoch  3366: reducing learning rate of group 0 to 9.7335e-04.
training: bce: 0.073400, dice: 0.193165, loss: 0.133283
training IoU in current batch 3400 is 0.8873553676127225
training IoU uptillnow 3400 is 0.0037092245162635828
testing: bce: 9.601301, dice: 25.267530, loss: 17.434416
IoU in current test batch is 0.8200919863095808
Epoch  3467: reducing learning rate of group 0 to 9.7237e-04.
training: bce: 0.072204, dice: 0.190326, loss: 0.131265
training IoU in current batch 3500 is 0.8877675152061275
training IoU uptillnow 3500 is 0.0037300646493617566
testing: bce: 9.722506, dice: 25.628169, loss: 17.675338
IoU in current test batch is 0.8693775128363934
Epoch  3568: reducing learning rate of group 0 to 9.7140e-04.
training: bce: 0.071035, dice: 0.187957, loss: 0.129496
training IoU in current batch 3600 is 0.9458315396757067
training IoU uptillnow 3600 is 0.003757809527146171
testing: bce: 9.838291, dice: 26.032029, loss: 17.935160
IoU in current test batch is 0.8391893886353782
Epoch  3669: reducing learning rate of group 0 to 9.7043e-04.
training: bce: 0.069908, dice: 0.185621, loss: 0.127764
training IoU in current batch 3700 is 0.845498743759585
training IoU uptillnow 3700 is 0.00377050026455908
testing: bce: 9.951088, dice: 26.422365, loss: 18.186726
IoU in current test batch is 0.8610161810097505
Epoch  3770: reducing learning rate of group 0 to 9.6946e-04.
training: bce: 0.069131, dice: 0.183336, loss: 0.126234
training IoU in current batch 3800 is 0.7992489507400045
training IoU uptillnow 3800 is 0.0037764393460939637
testing: bce: 10.106467, dice: 26.802339, loss: 18.454403
IoU in current test batch is 0.796271858656678
Epoch  3871: reducing learning rate of group 0 to 9.6849e-04.
training: bce: 0.068055, dice: 0.180954, loss: 0.124504
training IoU in current batch 3900 is 0.909119920573009
training IoU uptillnow 3900 is 0.003796156348318293
testing: bce: 10.210854, dice: 27.150007, loss: 18.680430
IoU in current test batch is 0.8877459733679794
Epoch  3972: reducing learning rate of group 0 to 9.6752e-04.
training: bce: 0.066990, dice: 0.178721, loss: 0.122855
training IoU in current batch 4000 is 0.737116616588242
training IoU uptillnow 4000 is 0.0037933927075940468
testing: bce: 10.308714, dice: 27.502337, loss: 18.905525
IoU in current test batch is 0.8699123322033882
Epoch  4073: reducing learning rate of group 0 to 9.6656e-04.
training: bce: 0.066383, dice: 0.177337, loss: 0.121860
training IoU in current batch 4100 is 0.9106731416823225
training IoU uptillnow 4100 is 0.0038119241145878916
testing: bce: 10.470582, dice: 27.971453, loss: 19.221017
IoU in current test batch is 0.8834506349408403
Epoch  4174: reducing learning rate of group 0 to 9.6559e-04.
training: bce: 0.065354, dice: 0.175326, loss: 0.120340
training IoU in current batch 4200 is 0.8468732041168173
training IoU uptillnow 4200 is 0.003821979860981517
testing: bce: 10.559638, dice: 28.328643, loss: 19.444140
IoU in current test batch is 0.8734016832092844
Epoch  4275: reducing learning rate of group 0 to 9.6462e-04.
training: bce: 0.064264, dice: 0.172948, loss: 0.118606
training IoU in current batch 4300 is 0.9224515322076298
training IoU uptillnow 4300 is 0.003840354141382741
testing: bce: 10.630721, dice: 28.609652, loss: 19.620186
IoU in current test batch is 0.8693690371916963
training: bce: 0.063237, dice: 0.170671, loss: 0.116954
training IoU in current batch 4400 is 0.8963665875889624
training IoU uptillnow 4400 is 0.0038549298922703136
testing: bce: 10.704087, dice: 28.889416, loss: 19.796752
IoU in current test batch is 0.8330059278222829
Epoch  4420: reducing learning rate of group 0 to 9.6366e-04.
training: bce: 0.062535, dice: 0.168977, loss: 0.115756
training IoU in current batch 4500 is 0.8433954818979974
training IoU uptillnow 4500 is 0.0038629736051612198
testing: bce: 10.825857, dice: 29.252574, loss: 20.039216
IoU in current test batch is 0.8144397028754776
Epoch  4521: reducing learning rate of group 0 to 9.6269e-04.
training: bce: 0.061684, dice: 0.167065, loss: 0.114375
training IoU in current batch 4600 is 0.9187132941804591
training IoU uptillnow 4600 is 0.0038788526068074067
testing: bce: 10.915769, dice: 29.564091, loss: 20.239930
IoU in current test batch is 0.848117862446842
Epoch  4622: reducing learning rate of group 0 to 9.6173e-04.
training: bce: 0.060849, dice: 0.165189, loss: 0.113019
training IoU in current batch 4700 is 0.9294153401967861
training IoU uptillnow 4700 is 0.00389519432333956
testing: bce: 11.002056, dice: 29.867513, loss: 20.434785
IoU in current test batch is 0.8136654367958898
Epoch  4723: reducing learning rate of group 0 to 9.6077e-04.
training: bce: 0.060220, dice: 0.163715, loss: 0.111967
training IoU in current batch 4800 is 0.8349581405595946
training IoU uptillnow 4800 is 0.0039010180346384224
testing: bce: 11.119818, dice: 30.230636, loss: 20.675227
IoU in current test batch is 0.8322543084318486
Epoch  4824: reducing learning rate of group 0 to 9.5981e-04.
training: bce: 0.059432, dice: 0.161973, loss: 0.110702
training IoU in current batch 4900 is 0.9017998227155355
training IoU uptillnow 4900 is 0.00391342328007689
testing: bce: 11.202920, dice: 30.531866, loss: 20.867393
IoU in current test batch is 0.8751424301923711
Epoch  4925: reducing learning rate of group 0 to 9.5885e-04.
training: bce: 0.058578, dice: 0.160008, loss: 0.109293
training IoU in current batch 5000 is 0.8978385368557177
training IoU uptillnow 5000 is 0.00392493636554383
testing: bce: 11.267297, dice: 30.776958, loss: 21.022127
IoU in current test batch is 0.8541558163672326
Epoch  5026: reducing learning rate of group 0 to 9.5789e-04.
training: bce: 0.057803, dice: 0.158352, loss: 0.108078
training IoU in current batch 5100 is 0.82323626538577
training IoU uptillnow 5100 is 0.003928685531616856
testing: bce: 11.340504, dice: 31.067527, loss: 21.204015
IoU in current test batch is 0.8887711456575368
Epoch  5127: reducing learning rate of group 0 to 9.5693e-04.
training: bce: 0.057170, dice: 0.156846, loss: 0.107008
training IoU in current batch 5200 is 0.928831417624521
training IoU uptillnow 5200 is 0.003942441954545249
testing: bce: 11.436242, dice: 31.375168, loss: 21.405705
IoU in current test batch is 0.8446894497528586
Epoch  5228: reducing learning rate of group 0 to 9.5598e-04.
training: bce: 0.056451, dice: 0.155156, loss: 0.105804
training IoU in current batch 5300 is 0.928096539162113
training IoU uptillnow 5300 is 0.003955610050022806
testing: bce: 11.509538, dice: 31.633953, loss: 21.571746
IoU in current test batch is 0.8450880034531144
Epoch  5329: reducing learning rate of group 0 to 9.5502e-04.
training: bce: 0.055878, dice: 0.153708, loss: 0.104793
training IoU in current batch 5400 is 0.9080893012467381
training IoU uptillnow 5400 is 0.003966438349526804
testing: bce: 11.607532, dice: 31.929887, loss: 21.768709
IoU in current test batch is 0.8592962173231457
Epoch  5430: reducing learning rate of group 0 to 9.5406e-04.
training: bce: 0.055301, dice: 0.152444, loss: 0.103872
training IoU in current batch 5500 is 0.8565895976143449
training IoU uptillnow 5500 is 0.00397219202410497
testing: bce: 11.700400, dice: 32.253593, loss: 21.976997
IoU in current test batch is 0.8128838766981106
Epoch  5531: reducing learning rate of group 0 to 9.5311e-04.
training: bce: 0.054704, dice: 0.151160, loss: 0.102932
training IoU in current batch 5600 is 0.9256209945268279
training IoU uptillnow 5600 is 0.003983902664142985
testing: bce: 11.784460, dice: 32.563444, loss: 22.173952
IoU in current test batch is 0.8818577002063541
Epoch  5632: reducing learning rate of group 0 to 9.5216e-04.
training: bce: 0.054145, dice: 0.149826, loss: 0.101986
training IoU in current batch 5700 is 0.9313947590870668
training IoU uptillnow 5700 is 0.003995708858342114
testing: bce: 11.872376, dice: 32.852215, loss: 22.362295
IoU in current test batch is 0.8789720017116882
Epoch  5733: reducing learning rate of group 0 to 9.5121e-04.
training: bce: 0.053620, dice: 0.148577, loss: 0.101098
training IoU in current batch 5800 is 0.8690787620235844
training IoU uptillnow 5800 is 0.004001736869922458
testing: bce: 11.963367, dice: 33.149749, loss: 22.556558
IoU in current test batch is 0.8617507767731692
Epoch  5834: reducing learning rate of group 0 to 9.5025e-04.
training: bce: 0.053147, dice: 0.147545, loss: 0.100346
training IoU in current batch 5900 is 0.8786626596543952
training IoU uptillnow 5900 is 0.0040083726338328044
testing: bce: 12.062266, dice: 33.487048, loss: 22.774657
IoU in current test batch is 0.849773345093046
Epoch  5935: reducing learning rate of group 0 to 9.4930e-04.
training: bce: 0.052834, dice: 0.146461, loss: 0.099647
training IoU in current batch 6000 is 0.8923822714681441
training IoU uptillnow 6000 is 0.004015930352938086
testing: bce: 12.194427, dice: 33.804366, loss: 22.999397
IoU in current test batch is 0.8714586309095621
Epoch  6036: reducing learning rate of group 0 to 9.4835e-04.
training: bce: 0.052391, dice: 0.145428, loss: 0.098910
training IoU in current batch 6100 is 0.8726608514128085
training IoU uptillnow 6100 is 0.004021624073707238
testing: bce: 12.293843, dice: 34.125282, loss: 23.209563
IoU in current test batch is 0.8036266786349598
Epoch  6137: reducing learning rate of group 0 to 9.4741e-04.
training: bce: 0.051911, dice: 0.144359, loss: 0.098135
training IoU in current batch 6200 is 0.8894173207396567
training IoU uptillnow 6200 is 0.004028485265934153
testing: bce: 12.380686, dice: 34.429585, loss: 23.405136
IoU in current test batch is 0.8468110070840241
Epoch  6238: reducing learning rate of group 0 to 9.4646e-04.
training: bce: 0.051360, dice: 0.143158, loss: 0.097259
training IoU in current batch 6300 is 0.7651300826537971
training IoU uptillnow 6300 is 0.004025266176064844
testing: bce: 12.446952, dice: 34.693739, loss: 23.570346
IoU in current test batch is 0.8859348922100172
Epoch  6339: reducing learning rate of group 0 to 9.4551e-04.
training: bce: 0.050880, dice: 0.142159, loss: 0.096519
training IoU in current batch 6400 is 0.9365681635568253
training IoU uptillnow 6400 is 0.004035539174685673
testing: bce: 12.526148, dice: 34.998469, loss: 23.762308
IoU in current test batch is 0.8787132656201225
training: bce: 0.050394, dice: 0.141040, loss: 0.095717
training IoU in current batch 6500 is 0.7752122365339579
training IoU uptillnow 6500 is 0.004033086044520839
testing: bce: 12.600324, dice: 35.265457, loss: 23.932891
IoU in current test batch is 0.8807953709425622
Epoch  6534: reducing learning rate of group 0 to 9.4457e-04.
training: bce: 0.049911, dice: 0.140007, loss: 0.094959
training IoU in current batch 6600 is 0.8111479699107602
training IoU uptillnow 6600 is 0.004033429231992934
testing: bce: 12.671707, dice: 35.545605, loss: 24.108656
IoU in current test batch is 0.8764725956918059
Epoch  6635: reducing learning rate of group 0 to 9.4362e-04.
training: bce: 0.049691, dice: 0.139195, loss: 0.094443
training IoU in current batch 6700 is 0.9382230857546586
training IoU uptillnow 6700 is 0.0040432439789975655
testing: bce: 12.806868, dice: 35.874873, loss: 24.340871
IoU in current test batch is 0.8701061363462308
Epoch  6736: reducing learning rate of group 0 to 9.4268e-04.
training: bce: 0.049283, dice: 0.138081, loss: 0.093682
training IoU in current batch 6800 is 0.5211409395973154
training IoU uptillnow 6800 is 0.004022106803861395
testing: bce: 12.891208, dice: 36.118707, loss: 24.504957
IoU in current test batch is 0.8619900276924517
Epoch  6884: reducing learning rate of group 0 to 9.4174e-04.
training: bce: 0.048969, dice: 0.137337, loss: 0.093153
training IoU in current batch 6900 is 0.8966468024548608
training IoU uptillnow 6900 is 0.004028788838471058
testing: bce: 12.997520, dice: 36.452316, loss: 24.724918
IoU in current test batch is 0.8430389619054169
Epoch  6985: reducing learning rate of group 0 to 9.4079e-04.
training: bce: 0.048528, dice: 0.136323, loss: 0.092426
training IoU in current batch 7000 is 0.9150253488267756
training IoU uptillnow 7000 is 0.004036592550878754
testing: bce: 13.067100, dice: 36.707693, loss: 24.887397
IoU in current test batch is 0.8917581060137407
Epoch  7086: reducing learning rate of group 0 to 9.3985e-04.
training: bce: 0.048065, dice: 0.135206, loss: 0.091636
training IoU in current batch 7100 is 0.9033674373640538
training IoU uptillnow 7100 is 0.00404335560729252
testing: bce: 13.127347, dice: 36.926906, loss: 25.027126
IoU in current test batch is 0.8885674132898933
Epoch  7192: reducing learning rate of group 0 to 9.3891e-04.
training: bce: 0.047618, dice: 0.134181, loss: 0.090899
training IoU in current batch 7200 is 0.7327259606711167
training IoU uptillnow 7200 is 0.004038082370187439
testing: bce: 13.188370, dice: 37.162883, loss: 25.175627
IoU in current test batch is 0.8969756553887536
Epoch  7293: reducing learning rate of group 0 to 9.3797e-04.
training: bce: 0.047609, dice: 0.133688, loss: 0.090648
training IoU in current batch 7300 is 0.890084660639977
training IoU uptillnow 7300 is 0.004043730102457161
testing: bce: 13.369015, dice: 37.540543, loss: 25.454779
IoU in current test batch is 0.8740069413125109
Epoch  7394: reducing learning rate of group 0 to 9.3704e-04.
training: bce: 0.047182, dice: 0.132729, loss: 0.089955
training IoU in current batch 7400 is 0.7289771861185915
training IoU uptillnow 7400 is 0.00403834104460195
testing: bce: 13.430407, dice: 37.781870, loss: 25.606138
IoU in current test batch is 0.8593047159414637
Epoch  7495: reducing learning rate of group 0 to 9.3610e-04.
training: bce: 0.046768, dice: 0.131807, loss: 0.089288
training IoU in current batch 7500 is 0.8013025948516489
training IoU uptillnow 7500 is 0.0040379167269063935
testing: bce: 13.492579, dice: 38.026463, loss: 25.759521
IoU in current test batch is 0.8695335667407105
Epoch  7596: reducing learning rate of group 0 to 9.3516e-04.
training: bce: 0.046405, dice: 0.131047, loss: 0.088726
training IoU in current batch 7600 is 0.9165571877861379
training IoU uptillnow 7600 is 0.004045085115434538
testing: bce: 13.566258, dice: 38.311184, loss: 25.938721
IoU in current test batch is 0.8937087154499943
Epoch  7697: reducing learning rate of group 0 to 9.3423e-04.
training: bce: 0.046045, dice: 0.130285, loss: 0.088165
training IoU in current batch 7700 is 0.9260504955379165
training IoU uptillnow 7700 is 0.004052683704737941
testing: bce: 13.638269, dice: 38.589533, loss: 26.113901
IoU in current test batch is 0.8563521475512155
Epoch  7798: reducing learning rate of group 0 to 9.3329e-04.
training: bce: 0.045667, dice: 0.129484, loss: 0.087575
training IoU in current batch 7800 is 0.9392520215633423
training IoU uptillnow 7800 is 0.004060933626582304
testing: bce: 13.701808, dice: 38.850159, loss: 26.275983
IoU in current test batch is 0.9062983317834726
training: bce: 0.045313, dice: 0.128690, loss: 0.087001
training IoU in current batch 7900 is 0.913458816869516
training IoU uptillnow 7900 is 0.004067342441387586
testing: bce: 13.769929, dice: 39.106882, loss: 26.438406
IoU in current test batch is 0.8753662663149581
Epoch  7908: reducing learning rate of group 0 to 9.3236e-04.
training: bce: 0.045175, dice: 0.128060, loss: 0.086618
training IoU in current batch 8000 is 0.941311656807252
training IoU uptillnow 8000 is 0.004075331640770772
testing: bce: 13.901813, dice: 39.407969, loss: 26.654891
IoU in current test batch is 0.8457449168222221
Epoch  8009: reducing learning rate of group 0 to 9.3143e-04.
training: bce: 0.044827, dice: 0.127295, loss: 0.086061
training IoU in current batch 8100 is 0.9003911580096425
training IoU uptillnow 8100 is 0.004080597955414364
testing: bce: 13.967105, dice: 39.662280, loss: 26.814693
IoU in current test batch is 0.8779187115172551
Epoch  8110: reducing learning rate of group 0 to 9.3050e-04.
training: bce: 0.044464, dice: 0.126475, loss: 0.085470
training IoU in current batch 8200 is 0.914125847598274
training IoU uptillnow 8200 is 0.004086573217974747
testing: bce: 14.025091, dice: 39.893284, loss: 26.959187
IoU in current test batch is 0.860304794635809
Epoch  8211: reducing learning rate of group 0 to 9.2957e-04.
training: bce: 0.044170, dice: 0.125780, loss: 0.084975
training IoU in current batch 8300 is 0.7621219867080892
training IoU uptillnow 8300 is 0.004083248759663287
testing: bce: 14.101968, dice: 40.157726, loss: 27.129847
IoU in current test batch is 0.8547336165783617
Epoch  8312: reducing learning rate of group 0 to 9.2864e-04.
training: bce: 0.043845, dice: 0.125013, loss: 0.084429
training IoU in current batch 8400 is 0.929063835654212
training IoU uptillnow 8400 is 0.004089939277680283
testing: bce: 14.166877, dice: 40.393605, loss: 27.280241
IoU in current test batch is 0.8884267803084429
Epoch  8413: reducing learning rate of group 0 to 9.2771e-04.
training: bce: 0.043573, dice: 0.124470, loss: 0.084021
training IoU in current batch 8500 is 0.8580425190533494
training IoU uptillnow 8500 is 0.0040922951571954755
testing: bce: 14.246658, dice: 40.696889, loss: 27.471773
IoU in current test batch is 0.8778750490004097
Epoch  8514: reducing learning rate of group 0 to 9.2678e-04.
training: bce: 0.043277, dice: 0.123837, loss: 0.083557
training IoU in current batch 8600 is 0.9004662670092302
training IoU uptillnow 8600 is 0.004097062465390461
testing: bce: 14.316224, dice: 40.966328, loss: 27.641276
IoU in current test batch is 0.8730207466362111
Epoch  8615: reducing learning rate of group 0 to 9.2585e-04.
training: bce: 0.043027, dice: 0.123273, loss: 0.083150
training IoU in current batch 8700 is 0.7038445762016631
training IoU uptillnow 8700 is 0.004090421394428707
testing: bce: 14.399181, dice: 41.253924, loss: 27.826553
IoU in current test batch is 0.8762110805347395
Epoch  8791: reducing learning rate of group 0 to 9.2493e-04.
training: bce: 0.042765, dice: 0.122734, loss: 0.082750
training IoU in current batch 8800 is 0.8739339502812556
training IoU uptillnow 8800 is 0.004093594310653882
testing: bce: 14.476098, dice: 41.545357, loss: 28.010728
IoU in current test batch is 0.878041581940722
Epoch  8892: reducing learning rate of group 0 to 9.2400e-04.
training: bce: 0.042524, dice: 0.122123, loss: 0.082323
training IoU in current batch 8900 is 0.9281629886907535
training IoU uptillnow 8900 is 0.004099742166319536
testing: bce: 14.557797, dice: 41.808205, loss: 28.183001
IoU in current test batch is 0.8929994129280454
Epoch  8993: reducing learning rate of group 0 to 9.2308e-04.
training: bce: 0.042283, dice: 0.121626, loss: 0.081954
training IoU in current batch 9000 is 0.9017087062652563
training IoU uptillnow 9000 is 0.0041042838990715275
testing: bce: 14.638062, dice: 42.105931, loss: 28.371997
IoU in current test batch is 0.823856698402409
Epoch  9094: reducing learning rate of group 0 to 9.2216e-04.
training: bce: 0.041982, dice: 0.120987, loss: 0.081485
training IoU in current batch 9100 is 0.891353919239905
training IoU uptillnow 9100 is 0.00410815694266155
testing: bce: 14.695418, dice: 42.350078, loss: 28.522748
IoU in current test batch is 0.8730646470315573
Epoch  9195: reducing learning rate of group 0 to 9.2123e-04.
training: bce: 0.041762, dice: 0.120399, loss: 0.081080
training IoU in current batch 9200 is 0.9170021678538247
training IoU uptillnow 9200 is 0.0041133395738604155
testing: bce: 14.778830, dice: 42.607257, loss: 28.693044
IoU in current test batch is 0.8902887702949726
Epoch  9296: reducing learning rate of group 0 to 9.2031e-04.
training: bce: 0.041598, dice: 0.119962, loss: 0.080780
training IoU in current batch 9300 is 0.8479323084256942
training IoU uptillnow 9300 is 0.004114697728556342
testing: bce: 14.881054, dice: 42.914276, loss: 28.897665
IoU in current test batch is 0.8828693491990995
Epoch  9397: reducing learning rate of group 0 to 9.1939e-04.
training: bce: 0.041442, dice: 0.119578, loss: 0.080510
training IoU in current batch 9400 is 0.9406221885505942
training IoU uptillnow 9400 is 0.004120956777744689
testing: bce: 14.984443, dice: 43.236567, loss: 29.110505
IoU in current test batch is 0.8896355171289797
Epoch  9498: reducing learning rate of group 0 to 9.1847e-04.
training: bce: 0.041209, dice: 0.119012, loss: 0.080110
training IoU in current batch 9500 is 0.9475174870188388
training IoU uptillnow 9500 is 0.004127446943594069
testing: bce: 15.058818, dice: 43.489627, loss: 29.274222
IoU in current test batch is 0.8638732131105505
Epoch  9599: reducing learning rate of group 0 to 9.1755e-04.
training: bce: 0.041012, dice: 0.118508, loss: 0.079760
training IoU in current batch 9600 is 0.9089637749808274
training IoU uptillnow 9600 is 0.004131794115048189
testing: bce: 15.144415, dice: 43.761242, loss: 29.452829
IoU in current test batch is 0.8804632775789146
Epoch  9700: reducing learning rate of group 0 to 9.1664e-04.
training: bce: 0.040784, dice: 0.117959, loss: 0.079371
training IoU in current batch 9700 is 0.8709014095038696
training IoU uptillnow 9700 is 0.0041340898879836715
testing: bce: 15.217067, dice: 44.012248, loss: 29.614658
IoU in current test batch is 0.8630617368384292
training: bce: 0.040574, dice: 0.117452, loss: 0.079013
training IoU in current batch 9800 is 0.9068226981673658
training IoU uptillnow 9800 is 0.004138171345006967
testing: bce: 15.294940, dice: 44.274901, loss: 29.784921
IoU in current test batch is 0.870064937329488
Epoch  9801: reducing learning rate of group 0 to 9.1572e-04.
training: bce: 0.040337, dice: 0.116927, loss: 0.078632
training IoU in current batch 9900 is 0.9473586597014003
training IoU uptillnow 9900 is 0.004144217420691241
testing: bce: 15.360721, dice: 44.526559, loss: 29.943640
IoU in current test batch is 0.8737210376089641
Epoch  9902: reducing learning rate of group 0 to 9.1480e-04.
training: bce: 0.040178, dice: 0.116544, loss: 0.078361
training IoU in current batch 10000 is 0.9141083012453602
training IoU uptillnow 10000 is 0.004148480235265139
testing: bce: 15.454730, dice: 44.829091, loss: 30.141911
IoU in current test batch is 0.8485479565522878
Epoch 10003: reducing learning rate of group 0 to 9.1389e-04.
training: bce: 0.039943, dice: 0.116033, loss: 0.077988
training IoU in current batch 10100 is 0.8850153272825629
training IoU uptillnow 10100 is 0.004151218542374808
testing: bce: 15.518029, dice: 45.078723, loss: 30.298376
IoU in current test batch is 0.896693952397376
Epoch 10104: reducing learning rate of group 0 to 9.1298e-04.
training: bce: 0.039737, dice: 0.115573, loss: 0.077655
training IoU in current batch 10200 is 0.9269010297773043
training IoU uptillnow 10200 is 0.004155956181885755
testing: bce: 15.590850, dice: 45.344692, loss: 30.467771
IoU in current test batch is 0.8884564773463416
Epoch 10205: reducing learning rate of group 0 to 9.1206e-04.
training: bce: 0.039585, dice: 0.115256, loss: 0.077420
training IoU in current batch 10300 is 0.8595095578296968
training IoU uptillnow 10300 is 0.0041573307242337085
testing: bce: 15.683085, dice: 45.663717, loss: 30.673401
IoU in current test batch is 0.8968152496894652
Epoch 10306: reducing learning rate of group 0 to 9.1115e-04.
training: bce: 0.039414, dice: 0.114843, loss: 0.077129
training IoU in current batch 10400 is 0.8618098660568442
training IoU uptillnow 10400 is 0.004158789416725301
testing: bce: 15.767269, dice: 45.941742, loss: 30.854506
IoU in current test batch is 0.8732890320560025
Epoch 10407: reducing learning rate of group 0 to 9.1024e-04.
training: bce: 0.039359, dice: 0.114486, loss: 0.076922
training IoU in current batch 10500 is 0.8455421926249875
training IoU uptillnow 10500 is 0.004159445749897376
testing: bce: 15.896523, dice: 46.238950, loss: 31.067737
IoU in current test batch is 0.8348832083849916
Epoch 10508: reducing learning rate of group 0 to 9.0933e-04.
training: bce: 0.039168, dice: 0.114061, loss: 0.076614
training IoU in current batch 10600 is 0.9214549993313422
training IoU uptillnow 10600 is 0.004163670155583249
testing: bce: 15.969878, dice: 46.506214, loss: 31.238046
IoU in current test batch is 0.8688522220745435
Epoch 10609: reducing learning rate of group 0 to 9.0842e-04.
training: bce: 0.039408, dice: 0.113785, loss: 0.076597
training IoU in current batch 10700 is 0.9391058857163278
training IoU uptillnow 10700 is 0.004168640338491373
testing: bce: 16.219302, dice: 46.831444, loss: 31.525373
IoU in current test batch is 0.8868985883214187
Epoch 10710: reducing learning rate of group 0 to 9.0751e-04.
training: bce: 0.039252, dice: 0.113388, loss: 0.076320
training IoU in current batch 10800 is 0.9231682264323214
training IoU uptillnow 10800 is 0.004172780703213809
testing: bce: 16.306133, dice: 47.104186, loss: 31.705159
IoU in current test batch is 0.8902382580760483
Epoch 10811: reducing learning rate of group 0 to 9.0660e-04.
training: bce: 0.039069, dice: 0.113026, loss: 0.076047
training IoU in current batch 10900 is 0.8793572863200179
training IoU uptillnow 10900 is 0.004174835613115527
testing: bce: 16.380458, dice: 47.388293, loss: 31.884375
IoU in current test batch is 0.8795123480861406
Epoch 10912: reducing learning rate of group 0 to 9.0570e-04.
training: bce: 0.038902, dice: 0.112658, loss: 0.075780
training IoU in current batch 11000 is 0.9287417461482025
training IoU uptillnow 11000 is 0.0041790977085398116
testing: bce: 16.460066, dice: 47.667174, loss: 32.063620
IoU in current test batch is 0.8767718243110837
Epoch 11013: reducing learning rate of group 0 to 9.0479e-04.
training: bce: 0.038704, dice: 0.112210, loss: 0.075457
training IoU in current batch 11100 is 0.9365403833923867
training IoU uptillnow 11100 is 0.004183634274690808
testing: bce: 16.525256, dice: 47.909190, loss: 32.217223
IoU in current test batch is 0.889091200213197
Epoch 11114: reducing learning rate of group 0 to 9.0389e-04.
training: bce: 0.038490, dice: 0.111725, loss: 0.075108
training IoU in current batch 11200 is 0.9356744186046512
training IoU uptillnow 11200 is 0.004188051182273456
testing: bce: 16.581952, dice: 48.132104, loss: 32.357028
IoU in current test batch is 0.8693983076122195
Epoch 11215: reducing learning rate of group 0 to 9.0298e-04.
training: bce: 0.038294, dice: 0.111283, loss: 0.074788
training IoU in current batch 11300 is 0.8676212865997008
training IoU uptillnow 11300 is 0.004189378987341371
testing: bce: 16.644520, dice: 48.369466, loss: 32.506993
IoU in current test batch is 0.8587249007681618
Epoch 11316: reducing learning rate of group 0 to 9.0208e-04.
training: bce: 0.038105, dice: 0.110892, loss: 0.074498
training IoU in current batch 11400 is 0.8665521191294387
training IoU uptillnow 11400 is 0.004190636610429747
testing: bce: 16.709059, dice: 48.625962, loss: 32.667511
IoU in current test batch is 0.9098815844586763
Epoch 11417: reducing learning rate of group 0 to 9.0118e-04.
training: bce: 0.038130, dice: 0.110673, loss: 0.074402
training IoU in current batch 11500 is 0.9165121358223102
training IoU uptillnow 11500 is 0.004194044349484454
testing: bce: 16.866789, dice: 48.955662, loss: 32.911226
IoU in current test batch is 0.9020089087733768
Epoch 11518: reducing learning rate of group 0 to 9.0028e-04.
training: bce: 0.037995, dice: 0.110339, loss: 0.074167
training IoU in current batch 11600 is 0.8192719714331397
training IoU uptillnow 11600 is 0.004193202314381284
testing: bce: 16.953013, dice: 49.232397, loss: 33.092705
IoU in current test batch is 0.9055096639600512
Epoch 11619: reducing learning rate of group 0 to 8.9938e-04.
training: bce: 0.037844, dice: 0.109965, loss: 0.073904
training IoU in current batch 11700 is 0.889300461501093
training IoU uptillnow 11700 is 0.00419536708656421
testing: bce: 17.031278, dice: 49.488309, loss: 33.259794
IoU in current test batch is 0.8994014805983087
Epoch 11791: reducing learning rate of group 0 to 8.9848e-04.
training: bce: 0.037680, dice: 0.109622, loss: 0.073651
training IoU in current batch 11800 is 0.5036757301107754
training IoU uptillnow 11800 is 0.004181156524442268
testing: bce: 17.102483, dice: 49.755687, loss: 33.429085
IoU in current test batch is 0.8583087325006098
Epoch 11892: reducing learning rate of group 0 to 8.9758e-04.
training: bce: 0.037499, dice: 0.109232, loss: 0.073366
training IoU in current batch 11900 is 0.892282489989079
training IoU uptillnow 11900 is 0.004183511418362974
testing: bce: 17.164672, dice: 49.998936, loss: 33.581804
IoU in current test batch is 0.8774787145343137
Epoch 11993: reducing learning rate of group 0 to 8.9668e-04.
training: bce: 0.037323, dice: 0.108830, loss: 0.073077
training IoU in current batch 12000 is 0.8964613368283093
training IoU uptillnow 12000 is 0.0041860011714317065
testing: bce: 17.227253, dice: 50.233602, loss: 33.730428
IoU in current test batch is 0.8665360437209562
Epoch 12094: reducing learning rate of group 0 to 8.9578e-04.
training: bce: 0.037183, dice: 0.108519, loss: 0.072851
training IoU in current batch 12100 is 0.9050448909790508
training IoU uptillnow 12100 is 0.004188804437967228
testing: bce: 17.305739, dice: 50.507265, loss: 33.906502
IoU in current test batch is 0.822161847204164
Epoch 12195: reducing learning rate of group 0 to 8.9489e-04.
training: bce: 0.037010, dice: 0.108130, loss: 0.072570
training IoU in current batch 12200 is 0.9376540916613192
training IoU uptillnow 12200 is 0.004192898086195565
testing: bce: 17.367699, dice: 50.742137, loss: 34.054918
IoU in current test batch is 0.866218201394032
Epoch 12296: reducing learning rate of group 0 to 8.9399e-04.
training: bce: 0.036839, dice: 0.107740, loss: 0.072290
training IoU in current batch 12300 is 0.9336542554933359
training IoU uptillnow 12300 is 0.004196762594701143
testing: bce: 17.428955, dice: 50.973621, loss: 34.201288
IoU in current test batch is 0.8550379049381107
Epoch 12397: reducing learning rate of group 0 to 8.9310e-04.
training: bce: 0.036679, dice: 0.107369, loss: 0.072024
training IoU in current batch 12400 is 0.9281084019907155
training IoU uptillnow 12400 is 0.0042003411723582065
testing: bce: 17.494546, dice: 51.210731, loss: 34.352639
IoU in current test batch is 0.8684099291555494
Epoch 12498: reducing learning rate of group 0 to 8.9221e-04.
training: bce: 0.036684, dice: 0.107077, loss: 0.071880
training IoU in current batch 12500 is 0.9179924841077512
training IoU uptillnow 12500 is 0.004203457893005999
testing: bce: 17.637738, dice: 51.483585, loss: 34.560661
IoU in current test batch is 0.8888924038303716
Epoch 12599: reducing learning rate of group 0 to 8.9131e-04.
training: bce: 0.036653, dice: 0.106885, loss: 0.071769
training IoU in current batch 12600 is 0.9017204593944346
training IoU uptillnow 12600 is 0.004205879481800271
testing: bce: 17.763890, dice: 51.802152, loss: 34.783021
IoU in current test batch is 0.889618169141903
Epoch 12700: reducing learning rate of group 0 to 8.9042e-04.
training: bce: 0.036537, dice: 0.106569, loss: 0.071553
training IoU in current batch 12700 is 0.9435748314050609
training IoU uptillnow 12700 is 0.00420991061852356
testing: bce: 17.848175, dice: 52.058904, loss: 34.953540
IoU in current test batch is 0.8763487183986016
training: bce: 0.036372, dice: 0.106225, loss: 0.071298
training IoU in current batch 12800 is 0.8758554283159669
training IoU uptillnow 12800 is 0.004211233691119891
testing: bce: 17.907676, dice: 52.299225, loss: 35.103450
IoU in current test batch is 0.8593224820588534
Epoch 12801: reducing learning rate of group 0 to 8.8953e-04.
training: bce: 0.036334, dice: 0.106026, loss: 0.071180
training IoU in current batch 12900 is 0.7983362179666127
training IoU uptillnow 12900 is 0.004209531864894894
testing: bce: 18.028667, dice: 52.609161, loss: 35.318914
IoU in current test batch is 0.8771146615973235
Epoch 12902: reducing learning rate of group 0 to 8.8864e-04.
training: bce: 0.036276, dice: 0.105744, loss: 0.071010
training IoU in current batch 13000 is 0.8962319458508767
training IoU uptillnow 13000 is 0.0042116211492911676
testing: bce: 18.139555, dice: 52.875835, loss: 35.507695
IoU in current test batch is 0.8945628511501246
Epoch 13003: reducing learning rate of group 0 to 8.8775e-04.
training: bce: 0.036163, dice: 0.105453, loss: 0.070808
training IoU in current batch 13100 is 0.8601206221563855
training IoU uptillnow 13100 is 0.0042123003490582905
testing: bce: 18.221870, dice: 53.136000, loss: 35.678935
IoU in current test batch is 0.8580720786022076
Epoch 13104: reducing learning rate of group 0 to 8.8687e-04.
training: bce: 0.036015, dice: 0.105132, loss: 0.070574
training IoU in current batch 13200 is 0.9148006134969325
training IoU uptillnow 13200 is 0.00421504031359451
testing: bce: 18.286128, dice: 53.378796, loss: 35.832462
IoU in current test batch is 0.9045729704721939
Epoch 13205: reducing learning rate of group 0 to 8.8598e-04.
training: bce: 0.035886, dice: 0.104849, loss: 0.070367
training IoU in current batch 13300 is 0.89701936326057
training IoU uptillnow 13300 is 0.004217070660957177
testing: bce: 18.358436, dice: 53.638132, loss: 35.998284
IoU in current test batch is 0.8995630501617389
Epoch 13306: reducing learning rate of group 0 to 8.8509e-04.
training: bce: 0.035738, dice: 0.104530, loss: 0.070134
training IoU in current batch 13400 is 0.955991198239648
training IoU uptillnow 13400 is 0.004221270984293055
testing: bce: 18.420028, dice: 53.877230, loss: 36.148629
IoU in current test batch is 0.8984447931892563
Epoch 13407: reducing learning rate of group 0 to 8.8421e-04.
training: bce: 0.035617, dice: 0.104277, loss: 0.069947
training IoU in current batch 13500 is 0.9154745910060855
training IoU uptillnow 13500 is 0.004223908581291333
testing: bce: 18.495045, dice: 54.147617, loss: 36.321331
IoU in current test batch is 0.8620849692943915
Epoch 13508: reducing learning rate of group 0 to 8.8333e-04.
training: bce: 0.035473, dice: 0.103940, loss: 0.069706
training IoU in current batch 13600 is 0.9053616113549657
training IoU uptillnow 13600 is 0.004226135619564132
testing: bce: 18.556285, dice: 54.372775, loss: 36.464530
IoU in current test batch is 0.8643046240476322
Epoch 13609: reducing learning rate of group 0 to 8.8244e-04.
training: bce: 0.035346, dice: 0.103692, loss: 0.069519
training IoU in current batch 13700 is 0.9244738993846021
training IoU uptillnow 13700 is 0.004229027626551644
testing: bce: 18.625832, dice: 54.641673, loss: 36.633753
IoU in current test batch is 0.8632060658863815
Epoch 13710: reducing learning rate of group 0 to 8.8156e-04.
training: bce: 0.035258, dice: 0.103499, loss: 0.069378
training IoU in current batch 13800 is 0.9154954761649322
training IoU uptillnow 13800 is 0.004231552441813386
testing: bce: 18.714981, dice: 54.938002, loss: 36.826491
IoU in current test batch is 0.852027685862632
Epoch 13811: reducing learning rate of group 0 to 8.8068e-04.
training: bce: 0.035170, dice: 0.103332, loss: 0.069251
training IoU in current batch 13900 is 0.8984927714549369
training IoU uptillnow 13900 is 0.004233429367325661
testing: bce: 18.803803, dice: 55.246887, loss: 37.025345
IoU in current test batch is 0.8788681582392516
Epoch 13912: reducing learning rate of group 0 to 8.7980e-04.
training: bce: 0.035055, dice: 0.103062, loss: 0.069059
training IoU in current batch 14000 is 0.9307111907963544
training IoU uptillnow 14000 is 0.004236430057181072
testing: bce: 18.877313, dice: 55.499087, loss: 37.188200
IoU in current test batch is 0.8714277453507043
Epoch 14013: reducing learning rate of group 0 to 8.7892e-04.
training: bce: 0.034910, dice: 0.102727, loss: 0.068818
training IoU in current batch 14100 is 0.9320482130107933
training IoU uptillnow 14100 is 0.004239435595851186
testing: bce: 18.933084, dice: 55.713508, loss: 37.323296
IoU in current test batch is 0.8102113756242025
Epoch 14114: reducing learning rate of group 0 to 8.7804e-04.
training: bce: 0.034803, dice: 0.102487, loss: 0.068645
training IoU in current batch 14200 is 0.894985848857348
training IoU uptillnow 14200 is 0.004241093885045155
testing: bce: 19.008916, dice: 55.977838, loss: 37.493377
IoU in current test batch is 0.886523014298321
Epoch 14215: reducing learning rate of group 0 to 8.7716e-04.
training: bce: 0.034708, dice: 0.102243, loss: 0.068475
training IoU in current batch 14300 is 0.9308376093083761
training IoU uptillnow 14300 is 0.004243982453407485
testing: bce: 19.090764, dice: 56.237467, loss: 37.664115
IoU in current test batch is 0.900315739694848
Epoch 14316: reducing learning rate of group 0 to 8.7628e-04.
training: bce: 0.034637, dice: 0.102060, loss: 0.068348
training IoU in current batch 14400 is 0.7871807271288521
training IoU uptillnow 14400 is 0.004241843165734662
testing: bce: 19.184844, dice: 56.529202, loss: 37.857023
IoU in current test batch is 0.8648615211847692
Epoch 14417: reducing learning rate of group 0 to 8.7541e-04.
training: bce: 0.034520, dice: 0.101791, loss: 0.068155
training IoU in current batch 14500 is 0.9176206224031205
training IoU uptillnow 14500 is 0.0042442310006859125
testing: bce: 19.252860, dice: 56.771882, loss: 38.012371
IoU in current test batch is 0.8613664766157363
training: bce: 0.034384, dice: 0.101487, loss: 0.067935
training IoU in current batch 14600 is 0.9425719895287958
training IoU uptillnow 14600 is 0.004247440568160456
testing: bce: 19.309056, dice: 56.992693, loss: 38.150875
IoU in current test batch is 0.8787329310840152
Epoch 14605: reducing learning rate of group 0 to 8.7453e-04.
training: bce: 0.034276, dice: 0.101253, loss: 0.067764
training IoU in current batch 14700 is 0.881937058124257
training IoU uptillnow 14700 is 0.0042485441986785215
testing: bce: 19.380178, dice: 57.250845, loss: 38.315512
IoU in current test batch is 0.8911959915878604
Epoch 14706: reducing learning rate of group 0 to 8.7366e-04.
training: bce: 0.034180, dice: 0.101023, loss: 0.067602
training IoU in current batch 14800 is 0.9361113307503756
training IoU uptillnow 14800 is 0.00425146300453673
testing: bce: 19.457880, dice: 57.509036, loss: 38.483458
IoU in current test batch is 0.8788805524199481
Epoch 14807: reducing learning rate of group 0 to 8.7278e-04.
training: bce: 0.034066, dice: 0.100789, loss: 0.067427
training IoU in current batch 14900 is 0.9162566065245125
training IoU uptillnow 14900 is 0.004253676413221287
testing: bce: 19.523929, dice: 57.763531, loss: 38.643730
IoU in current test batch is 0.8655133466026133
Epoch 14908: reducing learning rate of group 0 to 8.7191e-04.
training: bce: 0.033946, dice: 0.100508, loss: 0.067227
training IoU in current batch 15000 is 0.6558641045720137
training IoU uptillnow 15000 is 0.00424718114030374
testing: bce: 19.585593, dice: 57.989273, loss: 38.787433
IoU in current test batch is 0.890647067356363
Epoch 15009: reducing learning rate of group 0 to 8.7104e-04.
training: bce: 0.033825, dice: 0.100236, loss: 0.067031
training IoU in current batch 15100 is 0.9649887411927072
training IoU uptillnow 15100 is 0.004251007129083687
testing: bce: 19.646097, dice: 58.217844, loss: 38.931970
IoU in current test batch is 0.8959066474594672
Epoch 15110: reducing learning rate of group 0 to 8.7017e-04.
training: bce: 0.033712, dice: 0.100005, loss: 0.066858
training IoU in current batch 15200 is 0.9241166615900092
training IoU uptillnow 15200 is 0.004253438391361605
testing: bce: 19.709781, dice: 58.468287, loss: 39.089034
IoU in current test batch is 0.8862679438526891
Epoch 15211: reducing learning rate of group 0 to 8.6930e-04.
training: bce: 0.033645, dice: 0.099792, loss: 0.066718
training IoU in current batch 15300 is 0.9274645565446752
training IoU uptillnow 15300 is 0.004255947275691791
testing: bce: 19.799879, dice: 58.727785, loss: 39.263832
IoU in current test batch is 0.8655485680513254
Epoch 15312: reducing learning rate of group 0 to 8.6843e-04.
training: bce: 0.033539, dice: 0.099551, loss: 0.066545
training IoU in current batch 15400 is 0.9513326622609305
training IoU uptillnow 15400 is 0.004259198467404101
testing: bce: 19.866738, dice: 58.968930, loss: 39.417834
IoU in current test batch is 0.8874277646743861
Epoch 15413: reducing learning rate of group 0 to 8.6756e-04.
training: bce: 0.033547, dice: 0.099414, loss: 0.066481
training IoU in current batch 15500 is 0.8978591845067958
training IoU uptillnow 15500 is 0.00426068287134662
testing: bce: 20.000307, dice: 59.270050, loss: 39.635178
IoU in current test batch is 0.8736397084699704
Epoch 15514: reducing learning rate of group 0 to 8.6669e-04.
training: bce: 0.033432, dice: 0.099159, loss: 0.066295
training IoU in current batch 15600 is 0.8692128457096545
training IoU uptillnow 15600 is 0.0042612301526568025
testing: bce: 20.060329, dice: 59.499321, loss: 39.779825
IoU in current test batch is 0.8562037836081225
Epoch 15615: reducing learning rate of group 0 to 8.6583e-04.
training: bce: 0.033347, dice: 0.098934, loss: 0.066141
training IoU in current batch 15700 is 0.9203072864126639
training IoU uptillnow 15700 is 0.004263397570524495
testing: bce: 20.137800, dice: 59.744728, loss: 39.941264
IoU in current test batch is 0.8435552405461273
Epoch 15716: reducing learning rate of group 0 to 8.6496e-04.
training: bce: 0.033242, dice: 0.098714, loss: 0.065978
training IoU in current batch 15800 is 0.8743003945316083
training IoU uptillnow 15800 is 0.004264081732299912
testing: bce: 20.202224, dice: 59.991468, loss: 40.096846
IoU in current test batch is 0.8903628199604895
Epoch 15817: reducing learning rate of group 0 to 8.6409e-04.
training: bce: 0.033289, dice: 0.098640, loss: 0.065965
training IoU in current batch 15900 is 0.9333785004516711
training IoU uptillnow 15900 is 0.004266614974045453
testing: bce: 20.358920, dice: 60.326253, loss: 40.342587
IoU in current test batch is 0.8736652228598945
Epoch 15918: reducing learning rate of group 0 to 8.6323e-04.
training: bce: 0.033210, dice: 0.098412, loss: 0.065811
training IoU in current batch 16000 is 0.9007724719101123
training IoU uptillnow 16000 is 0.004268097677535891
testing: bce: 20.437918, dice: 60.565070, loss: 40.501494
IoU in current test batch is 0.899579005895916
Epoch 16019: reducing learning rate of group 0 to 8.6237e-04.
training: bce: 0.033117, dice: 0.098189, loss: 0.065653
training IoU in current batch 16100 is 0.8518972365418744
training IoU uptillnow 16100 is 0.004268044193312386
testing: bce: 20.508396, dice: 60.805363, loss: 40.656880
IoU in current test batch is 0.8479053136318069
Epoch 16120: reducing learning rate of group 0 to 8.6150e-04.
training: bce: 0.033020, dice: 0.097981, loss: 0.065500
training IoU in current batch 16200 is 0.9553479476239517
training IoU uptillnow 16200 is 0.004271184095446868
testing: bce: 20.574972, dice: 61.053432, loss: 40.814202
IoU in current test batch is 0.8836905740548313
Epoch 16221: reducing learning rate of group 0 to 8.6064e-04.
training: bce: 0.033003, dice: 0.097903, loss: 0.065453
training IoU in current batch 16300 is 0.8602249488752556
training IoU uptillnow 16300 is 0.0042713677691413
testing: bce: 20.691742, dice: 61.381275, loss: 41.036508
IoU in current test batch is 0.8426501497484163
Epoch 16322: reducing learning rate of group 0 to 8.5978e-04.
training: bce: 0.032988, dice: 0.097819, loss: 0.065403
training IoU in current batch 16400 is 0.9259528547374866
training IoU uptillnow 16400 is 0.004273552980436624
testing: bce: 20.809154, dice: 61.704684, loss: 41.256919
IoU in current test batch is 0.8682406064240726
Epoch 16423: reducing learning rate of group 0 to 8.5892e-04.
training: bce: 0.032971, dice: 0.097695, loss: 0.065333
training IoU in current batch 16500 is 0.36789594053744995
training IoU uptillnow 16500 is 0.004258801915181492
testing: bce: 20.925404, dice: 62.002483, loss: 41.463943
IoU in current test batch is 0.9073082584596559
Epoch 16524: reducing learning rate of group 0 to 8.5806e-04.
training: bce: 0.032939, dice: 0.097608, loss: 0.065273
training IoU in current batch 16600 is 0.8756200280767431
training IoU uptillnow 16600 is 0.00425952053589833
testing: bce: 21.031306, dice: 62.322821, loss: 41.677064
IoU in current test batch is 0.8712128728650884
Epoch 16625: reducing learning rate of group 0 to 8.5721e-04.
training: bce: 0.032837, dice: 0.097395, loss: 0.065116
training IoU in current batch 16700 is 0.8655300681959083
training IoU uptillnow 16700 is 0.004259928474375554
testing: bce: 21.092822, dice: 62.561384, loss: 41.827103
IoU in current test batch is 0.8934907726025696
Epoch 16726: reducing learning rate of group 0 to 8.5635e-04.
training: bce: 0.032743, dice: 0.097190, loss: 0.064966
training IoU in current batch 16800 is 0.8808123460513748
training IoU uptillnow 16800 is 0.0042607863593578845
testing: bce: 21.158012, dice: 62.803589, loss: 41.980800
IoU in current test batch is 0.8829394755458642
Epoch 16827: reducing learning rate of group 0 to 8.5549e-04.
training: bce: 0.032644, dice: 0.096962, loss: 0.064803
training IoU in current batch 16900 is 0.8821161290322581
training IoU uptillnow 16900 is 0.004261672663634575
testing: bce: 21.219965, dice: 63.029241, loss: 42.124603
IoU in current test batch is 0.8916903281789428
Epoch 16928: reducing learning rate of group 0 to 8.5464e-04.
training: bce: 0.032580, dice: 0.096803, loss: 0.064691
training IoU in current batch 17000 is 0.8651235472214084
training IoU uptillnow 17000 is 0.004262048788994686
testing: bce: 21.303510, dice: 63.297959, loss: 42.300734
IoU in current test batch is 0.8859408167305388
Epoch 17029: reducing learning rate of group 0 to 8.5378e-04.
training: bce: 0.032484, dice: 0.096604, loss: 0.064544
training IoU in current batch 17100 is 0.9155969687596659
training IoU uptillnow 17100 is 0.0042638962602232905
testing: bce: 21.365888, dice: 63.539404, loss: 42.452646
IoU in current test batch is 0.8944978486250974
Epoch 17130: reducing learning rate of group 0 to 8.5293e-04.
training: bce: 0.032421, dice: 0.096420, loss: 0.064420
training IoU in current batch 17200 is 0.8843413729128015
training IoU uptillnow 17200 is 0.004264813710396773
testing: bce: 21.448819, dice: 63.789132, loss: 42.618976
IoU in current test batch is 0.9053006007195342
Epoch 17231: reducing learning rate of group 0 to 8.5208e-04.
training: bce: 0.032324, dice: 0.096199, loss: 0.064262
training IoU in current batch 17300 is 0.8855131213996159
training IoU uptillnow 17300 is 0.004265754418428686
testing: bce: 21.509115, dice: 64.013348, loss: 42.761232
IoU in current test batch is 0.8990653142957231
Epoch 17332: reducing learning rate of group 0 to 8.5122e-04.
training: bce: 0.032288, dice: 0.096080, loss: 0.064184
training IoU in current batch 17400 is 0.8507922136713445
training IoU uptillnow 17400 is 0.0042656866444497656
testing: bce: 21.609475, dice: 64.303310, loss: 42.956393
IoU in current test batch is 0.8855196264911483
Epoch 17433: reducing learning rate of group 0 to 8.5037e-04.
training: bce: 0.032212, dice: 0.095952, loss: 0.064082
training IoU in current batch 17500 is 0.9225379525107046
training IoU uptillnow 17500 is 0.004267669406109693
testing: bce: 21.682674, dice: 64.586781, loss: 43.134728
IoU in current test batch is 0.885785413956056
Epoch 17534: reducing learning rate of group 0 to 8.4952e-04.
training: bce: 0.032165, dice: 0.095799, loss: 0.063982
training IoU in current batch 17600 is 0.9366513802715584
training IoU uptillnow 17600 is 0.004270030564539601
testing: bce: 21.774474, dice: 64.852026, loss: 43.313250
IoU in current test batch is 0.8885265062971528
Epoch 17635: reducing learning rate of group 0 to 8.4867e-04.
training: bce: 0.032079, dice: 0.095627, loss: 0.063853
training IoU in current batch 17700 is 0.943851548547284
training IoU uptillnow 17700 is 0.004272568427813974
testing: bce: 21.839867, dice: 65.103894, loss: 43.471881
IoU in current test batch is 0.9070998999351866
Epoch 17736: reducing learning rate of group 0 to 8.4782e-04.
training: bce: 0.032026, dice: 0.095476, loss: 0.063751
training IoU in current batch 17800 is 0.9283871760226017
training IoU uptillnow 17800 is 0.004274643409288605
testing: bce: 21.926494, dice: 65.367689, loss: 43.647091
IoU in current test batch is 0.8810925883118748
Epoch 17837: reducing learning rate of group 0 to 8.4698e-04.
training: bce: 0.031993, dice: 0.095325, loss: 0.063659
training IoU in current batch 17900 is 0.9105406755562387
training IoU uptillnow 17900 is 0.004276196730156113
testing: bce: 22.026877, dice: 65.631580, loss: 43.829229
IoU in current test batch is 0.8794459918900096
Epoch 17938: reducing learning rate of group 0 to 8.4613e-04.
training: bce: 0.031905, dice: 0.095155, loss: 0.063530
training IoU in current batch 18000 is 0.9181821737259963
training IoU uptillnow 18000 is 0.0042779450449079255
testing: bce: 22.089319, dice: 65.880447, loss: 43.984883
IoU in current test batch is 0.8946902617763219
Epoch 18039: reducing learning rate of group 0 to 8.4528e-04.
training: bce: 0.031817, dice: 0.094951, loss: 0.063384
training IoU in current batch 18100 is 0.9020267318948196
training IoU uptillnow 18100 is 0.004279227784063586
testing: bce: 22.150570, dice: 66.104214, loss: 44.127392
IoU in current test batch is 0.8951592138468162
Epoch 18140: reducing learning rate of group 0 to 8.4444e-04.
training: bce: 0.031735, dice: 0.094802, loss: 0.063268
training IoU in current batch 18200 is 0.9591000368837342
training IoU uptillnow 18200 is 0.004282064289752038
testing: bce: 22.215997, dice: 66.364704, loss: 44.290351
IoU in current test batch is 0.8565223867485418
Epoch 18241: reducing learning rate of group 0 to 8.4359e-04.
training: bce: 0.031648, dice: 0.094615, loss: 0.063132
training IoU in current batch 18300 is 0.9311866384654657
training IoU uptillnow 18300 is 0.004284107177586448
testing: bce: 22.276386, dice: 66.598371, loss: 44.437378
IoU in current test batch is 0.889546413628415
Epoch 18342: reducing learning rate of group 0 to 8.4275e-04.
training: bce: 0.031726, dice: 0.094627, loss: 0.063177
training IoU in current batch 18400 is 0.7263129503332948
training IoU uptillnow 18400 is 0.004280560944088703
testing: bce: 22.453602, dice: 66.970469, loss: 44.712036
IoU in current test batch is 0.8704237021811659
Epoch 18443: reducing learning rate of group 0 to 8.4191e-04.
training: bce: 0.031646, dice: 0.094472, loss: 0.063059
training IoU in current batch 18500 is 0.8697437677135751
training IoU uptillnow 18500 is 0.004280929345226367
testing: bce: 22.518279, dice: 67.224247, loss: 44.871263
IoU in current test batch is 0.8968371540825026
Epoch 18544: reducing learning rate of group 0 to 8.4106e-04.
training: bce: 0.031573, dice: 0.094316, loss: 0.062945
training IoU in current batch 18600 is 0.8742372314636797
training IoU uptillnow 18600 is 0.004281414570816883
testing: bce: 22.588406, dice: 67.475953, loss: 45.032180
IoU in current test batch is 0.8768724560205597
Epoch 18645: reducing learning rate of group 0 to 8.4022e-04.
training: bce: 0.031537, dice: 0.094166, loss: 0.062851
training IoU in current batch 18700 is 0.8894158890550085
training IoU uptillnow 18700 is 0.004282300431864197
testing: bce: 22.683569, dice: 67.730492, loss: 45.207031
IoU in current test batch is 0.9090614062330556
Epoch 18746: reducing learning rate of group 0 to 8.3938e-04.
training: bce: 0.031479, dice: 0.094001, loss: 0.062740
training IoU in current batch 18800 is 0.9013437407370813
training IoU uptillnow 18800 is 0.004283494082583952
testing: bce: 22.762707, dice: 67.973707, loss: 45.368207
IoU in current test batch is 0.8997608925527657
Epoch 18847: reducing learning rate of group 0 to 8.3854e-04.
training: bce: 0.031401, dice: 0.093850, loss: 0.062626
training IoU in current batch 18900 is 0.9210958390017357
training IoU uptillnow 18900 is 0.004285197617383301
testing: bce: 22.827589, dice: 68.225449, loss: 45.526519
IoU in current test batch is 0.890562137708125
Epoch 18948: reducing learning rate of group 0 to 8.3771e-04.
training: bce: 0.031401, dice: 0.093754, loss: 0.062577
training IoU in current batch 19000 is 0.8600774798654297
training IoU uptillnow 19000 is 0.004285277559396583
testing: bce: 22.948002, dice: 68.516182, loss: 45.732092
IoU in current test batch is 0.8867210034278438
Epoch 19049: reducing learning rate of group 0 to 8.3687e-04.
training: bce: 0.031317, dice: 0.093568, loss: 0.062443
training IoU in current batch 19100 is 0.8428359788359788
training IoU uptillnow 19100 is 0.004284905339799616
testing: bce: 23.006915, dice: 68.740455, loss: 45.873685
IoU in current test batch is 0.8831493083025149
Epoch 19150: reducing learning rate of group 0 to 8.3603e-04.
training: bce: 0.031243, dice: 0.093413, loss: 0.062328
training IoU in current batch 19200 is 0.5997189914916868
training IoU uptillnow 19200 is 0.004278206155474106
testing: bce: 23.073077, dice: 68.985485, loss: 46.029281
IoU in current test batch is 0.8931664314452732
Epoch 19251: reducing learning rate of group 0 to 8.3519e-04.
training: bce: 0.031190, dice: 0.093298, loss: 0.062244
training IoU in current batch 19300 is 0.9052783803326103
training IoU uptillnow 19300 is 0.004279492025357475
testing: bce: 23.154073, dice: 69.259629, loss: 46.206851
IoU in current test batch is 0.8850033849341824
Epoch 19352: reducing learning rate of group 0 to 8.3436e-04.
training: bce: 0.031119, dice: 0.093156, loss: 0.062137
training IoU in current batch 19400 is 0.8239176020722833
training IoU uptillnow 19400 is 0.00427866782034229
testing: bce: 23.220582, dice: 69.512084, loss: 46.366333
IoU in current test batch is 0.8805866864937948
Epoch 19453: reducing learning rate of group 0 to 8.3353e-04.
training: bce: 0.031037, dice: 0.092973, loss: 0.062005
training IoU in current batch 19500 is 0.939550544201707
training IoU uptillnow 19500 is 0.004280816863471699
testing: bce: 23.278917, dice: 69.733461, loss: 46.506189
IoU in current test batch is 0.9038211565265526
Epoch 19554: reducing learning rate of group 0 to 8.3269e-04.
training: bce: 0.030977, dice: 0.092834, loss: 0.061905
training IoU in current batch 19600 is 0.8489966052167934
training IoU uptillnow 19600 is 0.004280634047098108
testing: bce: 23.352815, dice: 69.985928, loss: 46.669372
IoU in current test batch is 0.8785876324788013
Epoch 19655: reducing learning rate of group 0 to 8.3186e-04.
training: bce: 0.030896, dice: 0.092636, loss: 0.061766
training IoU in current batch 19700 is 0.8663457798521976
training IoU uptillnow 19700 is 0.004280893398664844
testing: bce: 23.410940, dice: 70.193342, loss: 46.802141
IoU in current test batch is 0.9008702445693471
Epoch 19756: reducing learning rate of group 0 to 8.3103e-04.
training: bce: 0.030833, dice: 0.092496, loss: 0.061664
training IoU in current batch 19800 is 0.9380841121495327
training IoU uptillnow 19800 is 0.004282961613209983
testing: bce: 23.481648, dice: 70.442831, loss: 46.962239
IoU in current test batch is 0.8966046282159035
Epoch 19857: reducing learning rate of group 0 to 8.3020e-04.
training: bce: 0.030774, dice: 0.092364, loss: 0.061569
training IoU in current batch 19900 is 0.8499748561211377
training IoU uptillnow 19900 is 0.004282795353561703
testing: bce: 23.555380, dice: 70.697673, loss: 47.126526
IoU in current test batch is 0.8959383727097273
Epoch 19958: reducing learning rate of group 0 to 8.2937e-04.
training: bce: 0.030689, dice: 0.092169, loss: 0.061429
training IoU in current batch 20000 is 0.9603607788487594
training IoU uptillnow 20000 is 0.004285390266519465
testing: bce: 23.607751, dice: 70.902733, loss: 47.255242
IoU in current test batch is 0.8870621076482756
Epoch 20063: reducing learning rate of group 0 to 8.2854e-04.
training: bce: 0.030627, dice: 0.092029, loss: 0.061328
training IoU in current batch 20100 is 0.9143963151705213
training IoU uptillnow 20100 is 0.004286816022995925
testing: bce: 23.678125, dice: 71.148729, loss: 47.413427
IoU in current test batch is 0.8961556068983605
Epoch 20164: reducing learning rate of group 0 to 8.2771e-04.
training: bce: 0.030550, dice: 0.091849, loss: 0.061199
training IoU in current batch 20200 is 0.9280768632332348
training IoU uptillnow 20200 is 0.004288566274434816
testing: bce: 23.735842, dice: 71.362955, loss: 47.549399
IoU in current test batch is 0.8945545454264815
Epoch 20265: reducing learning rate of group 0 to 8.2688e-04.
training: bce: 0.030464, dice: 0.091655, loss: 0.061060
training IoU in current batch 20300 is 0.9121465325968937
training IoU uptillnow 20300 is 0.004289906929518554
testing: bce: 23.786856, dice: 71.564969, loss: 47.675913
IoU in current test batch is 0.8882170396654744
Epoch 20366: reducing learning rate of group 0 to 8.2605e-04.
training: bce: 0.030432, dice: 0.091592, loss: 0.061012
training IoU in current batch 20400 is 0.956833463353022
training IoU uptillnow 20400 is 0.004292329655792984
testing: bce: 23.878762, dice: 71.868222, loss: 47.873492
IoU in current test batch is 0.900267878148453
Epoch 20467: reducing learning rate of group 0 to 8.2523e-04.
training: bce: 0.030360, dice: 0.091414, loss: 0.060887
training IoU in current batch 20500 is 0.9381538057513021
training IoU uptillnow 20500 is 0.00429427316768491
testing: bce: 23.939135, dice: 72.080142, loss: 48.009639
IoU in current test batch is 0.8935089634398902
Epoch 20568: reducing learning rate of group 0 to 8.2440e-04.
training: bce: 0.030292, dice: 0.091286, loss: 0.060789
training IoU in current batch 20600 is 0.793044731154627
training IoU uptillnow 20600 is 0.0042926759174935995
testing: bce: 24.001972, dice: 72.330385, loss: 48.166178
IoU in current test batch is 0.8820997249353222
Epoch 20669: reducing learning rate of group 0 to 8.2358e-04.
training: bce: 0.030235, dice: 0.091177, loss: 0.060706
training IoU in current batch 20700 is 0.9292208586455742
training IoU uptillnow 20700 is 0.004294383218472944
testing: bce: 24.073039, dice: 72.594092, loss: 48.333565
IoU in current test batch is 0.8383036289594485
Epoch 20770: reducing learning rate of group 0 to 8.2275e-04.
training: bce: 0.030181, dice: 0.091041, loss: 0.060611
training IoU in current batch 20800 is 0.8935119887165022
training IoU uptillnow 20800 is 0.004295215758856145
testing: bce: 24.145739, dice: 72.836385, loss: 48.491062
IoU in current test batch is 0.8784871367509257
Epoch 20871: reducing learning rate of group 0 to 8.2193e-04.
training: bce: 0.030114, dice: 0.090911, loss: 0.060513
training IoU in current batch 20900 is 0.806723293982148
training IoU uptillnow 20900 is 0.004293964147502883
testing: bce: 24.208554, dice: 73.082028, loss: 48.645291
IoU in current test batch is 0.8983373066942667
Epoch 20972: reducing learning rate of group 0 to 8.2111e-04.
training: bce: 0.030046, dice: 0.090758, loss: 0.060402
training IoU in current batch 21000 is 0.9355166572557877
training IoU uptillnow 21000 is 0.0042957908183222536
testing: bce: 24.268948, dice: 73.308384, loss: 48.788666
IoU in current test batch is 0.894100355800034
Epoch 21073: reducing learning rate of group 0 to 8.2029e-04.
training: bce: 0.030007, dice: 0.090663, loss: 0.060335
training IoU in current batch 21100 is 0.9356574057688904
training IoU uptillnow 21100 is 0.004297603510661585
testing: bce: 24.352930, dice: 73.580018, loss: 48.966474
IoU in current test batch is 0.8709595554395043
Epoch 21174: reducing learning rate of group 0 to 8.1947e-04.
training: bce: 0.029930, dice: 0.090488, loss: 0.060209
training IoU in current batch 21200 is 0.9080810885865037
training IoU uptillnow 21200 is 0.004298748748774272
testing: bce: 24.405793, dice: 73.785941, loss: 49.095867
IoU in current test batch is 0.8872108495778662
Epoch 21275: reducing learning rate of group 0 to 8.1865e-04.
training: bce: 0.029866, dice: 0.090353, loss: 0.060110
training IoU in current batch 21300 is 0.8883848017734701
training IoU uptillnow 21300 is 0.0042994209015374905
testing: bce: 24.468261, dice: 74.023810, loss: 49.246036
IoU in current test batch is 0.9012961198438973
Epoch 21376: reducing learning rate of group 0 to 8.1783e-04.
training: bce: 0.029884, dice: 0.090276, loss: 0.060080
training IoU in current batch 21400 is 0.9041281639212535
training IoU uptillnow 21400 is 0.0043004545911691375
testing: bce: 24.597983, dice: 74.307616, loss: 49.452799
IoU in current test batch is 0.9006723000950513
Epoch 21477: reducing learning rate of group 0 to 8.1701e-04.
training: bce: 0.029823, dice: 0.090138, loss: 0.059980
training IoU in current batch 21500 is 0.8660192298400734
training IoU uptillnow 21500 is 0.0043005924524687576
testing: bce: 24.662291, dice: 74.540576, loss: 49.601434
IoU in current test batch is 0.900313917204509
Epoch 21578: reducing learning rate of group 0 to 8.1620e-04.
training: bce: 0.029754, dice: 0.089982, loss: 0.059868
training IoU in current batch 21600 is 0.9052991312224647
training IoU uptillnow 21600 is 0.004301638252217119
testing: bce: 24.719535, dice: 74.757742, loss: 49.738639
IoU in current test batch is 0.8947087718330873
Epoch 21679: reducing learning rate of group 0 to 8.1538e-04.
training: bce: 0.029712, dice: 0.089887, loss: 0.059799
training IoU in current batch 21700 is 0.9248045997664182
training IoU uptillnow 21700 is 0.004303123827751034
testing: bce: 24.799471, dice: 75.024199, loss: 49.911835
IoU in current test batch is 0.8367959245884767
Epoch 21780: reducing learning rate of group 0 to 8.1456e-04.
training: bce: 0.029667, dice: 0.089820, loss: 0.059744
training IoU in current batch 21800 is 0.8578424206987283
training IoU uptillnow 21800 is 0.004303060015429318
testing: bce: 24.876093, dice: 75.314154, loss: 50.095124
IoU in current test batch is 0.8317158899070567
Epoch 21881: reducing learning rate of group 0 to 8.1375e-04.
training: bce: 0.029628, dice: 0.089747, loss: 0.059688
training IoU in current batch 21900 is 0.9138305896969428
training IoU uptillnow 21900 is 0.004304274996174742
testing: bce: 24.957157, dice: 75.598153, loss: 50.277655
IoU in current test batch is 0.8894785400850023
Epoch 21982: reducing learning rate of group 0 to 8.1294e-04.
training: bce: 0.029566, dice: 0.089605, loss: 0.059585
training IoU in current batch 22000 is 0.954985516084769
training IoU uptillnow 22000 is 0.004306414228865298
testing: bce: 25.018177, dice: 75.823358, loss: 50.420767
IoU in current test batch is 0.8766529623229478
Epoch 22083: reducing learning rate of group 0 to 8.1212e-04.
training: bce: 0.029573, dice: 0.089504, loss: 0.059538
training IoU in current batch 22100 is 0.2548014375330079
training IoU uptillnow 22100 is 0.004292693550881495
testing: bce: 25.138167, dice: 76.081612, loss: 50.609889
IoU in current test batch is 0.8670463223630323
Epoch 22184: reducing learning rate of group 0 to 8.1131e-04.
training: bce: 0.029516, dice: 0.089421, loss: 0.059468
training IoU in current batch 22200 is 0.9437628124713153
training IoU uptillnow 22200 is 0.004294612926186549
testing: bce: 25.202839, dice: 76.354954, loss: 50.778896
IoU in current test batch is 0.9045622657336789
Epoch 22285: reducing learning rate of group 0 to 8.1050e-04.
training: bce: 0.029496, dice: 0.089347, loss: 0.059422
training IoU in current batch 22300 is 0.8945732727707387
training IoU uptillnow 22300 is 0.004295412233112997
testing: bce: 25.299886, dice: 76.635976, loss: 50.967931
IoU in current test batch is 0.862759954808211
Epoch 22386: reducing learning rate of group 0 to 8.0969e-04.
training: bce: 0.029452, dice: 0.089268, loss: 0.059360
training IoU in current batch 22400 is 0.9281961471103327
training IoU uptillnow 22400 is 0.004296954880773542
testing: bce: 25.375504, dice: 76.911133, loss: 51.143318
IoU in current test batch is 0.901248530865515
Epoch 22487: reducing learning rate of group 0 to 8.0888e-04.
training: bce: 0.029393, dice: 0.089157, loss: 0.059275
training IoU in current batch 22500 is 0.6438914027149322
training IoU uptillnow 22500 is 0.004292166214193395
testing: bce: 25.437288, dice: 77.158374, loss: 51.297831
IoU in current test batch is 0.884626296964246
Epoch 22588: reducing learning rate of group 0 to 8.0807e-04.
training: bce: 0.029371, dice: 0.089067, loss: 0.059219
training IoU in current batch 22600 is 0.6698647778493239
training IoU uptillnow 22600 is 0.004287994530086732
testing: bce: 25.531388, dice: 77.422808, loss: 51.477098
IoU in current test batch is 0.8647713830374187
Epoch 22689: reducing learning rate of group 0 to 8.0726e-04.
training: bce: 0.029354, dice: 0.088961, loss: 0.059158
training IoU in current batch 22700 is 0.9271195751693829
training IoU uptillnow 22700 is 0.0042895257549039655
testing: bce: 25.629781, dice: 77.672954, loss: 51.651368
IoU in current test batch is 0.8741570246027904
Epoch 22790: reducing learning rate of group 0 to 8.0645e-04.
training: bce: 0.029314, dice: 0.088867, loss: 0.059091
training IoU in current batch 22800 is 0.7980911878152507
training IoU uptillnow 22800 is 0.004288214102714028
testing: bce: 25.707483, dice: 77.933190, loss: 51.820336
IoU in current test batch is 0.8848441050561251
Epoch 22891: reducing learning rate of group 0 to 8.0565e-04.
training: bce: 0.029265, dice: 0.088749, loss: 0.059007
training IoU in current batch 22900 is 0.9125808770668584
training IoU uptillnow 22900 is 0.004289413571220295
testing: bce: 25.777108, dice: 78.170367, loss: 51.973737
IoU in current test batch is 0.9122698764722988
Epoch 22992: reducing learning rate of group 0 to 8.0484e-04.
training: bce: 0.029210, dice: 0.088626, loss: 0.058918
training IoU in current batch 23000 is 0.9099892788887335
training IoU uptillnow 23000 is 0.004290546273377694
testing: bce: 25.840844, dice: 78.403717, loss: 52.122281
IoU in current test batch is 0.8688781815371206
Epoch 23093: reducing learning rate of group 0 to 8.0404e-04.
training: bce: 0.029145, dice: 0.088480, loss: 0.058812
training IoU in current batch 23100 is 0.9143639771252823
training IoU uptillnow 23100 is 0.004291763855353577
testing: bce: 25.895543, dice: 78.614262, loss: 52.254903
IoU in current test batch is 0.8920650859024017
Epoch 23194: reducing learning rate of group 0 to 8.0323e-04.
training: bce: 0.029084, dice: 0.088333, loss: 0.058708
training IoU in current batch 23200 is 0.9534001517390089
training IoU uptillnow 23200 is 0.004293812201990969
testing: bce: 25.952658, dice: 78.823550, loss: 52.388104
IoU in current test batch is 0.9024230653471714
Epoch 23295: reducing learning rate of group 0 to 8.0243e-04.
training: bce: 0.029022, dice: 0.088196, loss: 0.058609
training IoU in current batch 23300 is 0.8637905129856193
training IoU uptillnow 23300 is 0.004293920095913707
testing: bce: 26.009657, dice: 79.040480, loss: 52.525069
IoU in current test batch is 0.8827104392885295
Epoch 23396: reducing learning rate of group 0 to 8.0163e-04.
training: bce: 0.028972, dice: 0.088094, loss: 0.058533
training IoU in current batch 23400 is 0.8620805369127517
training IoU uptillnow 23400 is 0.004293990531316681
testing: bce: 26.075533, dice: 79.287904, loss: 52.681718
IoU in current test batch is 0.8987765894490592
Epoch 23497: reducing learning rate of group 0 to 8.0083e-04.
training: bce: 0.028957, dice: 0.088029, loss: 0.058493
training IoU in current batch 23500 is 0.9042123287671233
training IoU uptillnow 23500 is 0.004294956750254254
testing: bce: 26.173614, dice: 79.567898, loss: 52.870756
IoU in current test batch is 0.8724074595351653
Epoch 23598: reducing learning rate of group 0 to 8.0003e-04.
training: bce: 0.028906, dice: 0.087921, loss: 0.058413
training IoU in current batch 23600 is 0.9120318311564743
training IoU uptillnow 23600 is 0.004296080441646687
testing: bce: 26.238937, dice: 79.808214, loss: 53.023575
IoU in current test batch is 0.8699943812614642
Epoch 23699: reducing learning rate of group 0 to 7.9923e-04.
training: bce: 0.028850, dice: 0.087809, loss: 0.058330
training IoU in current batch 23700 is 0.8432003963339113
training IoU uptillnow 23700 is 0.004295742572105414
testing: bce: 26.298725, dice: 80.045031, loss: 53.171878
IoU in current test batch is 0.894940410583784
Epoch 23800: reducing learning rate of group 0 to 7.9843e-04.
training: bce: 0.028795, dice: 0.087698, loss: 0.058246
training IoU in current batch 23800 is 0.7707032449585424
training IoU uptillnow 23800 is 0.004293884556277034
testing: bce: 26.359669, dice: 80.280474, loss: 53.320071
IoU in current test batch is 0.8877409588913523
training: bce: 0.028783, dice: 0.087632, loss: 0.058208
training IoU in current batch 23900 is 0.9455996639361479
training IoU uptillnow 23900 is 0.004295700855860331
testing: bce: 26.459779, dice: 80.557625, loss: 53.508702
IoU in current test batch is 0.8823692703457634
Epoch 23901: reducing learning rate of group 0 to 7.9763e-04.
training: bce: 0.028725, dice: 0.087511, loss: 0.058118
training IoU in current batch 24000 is 0.9191037344398341
training IoU uptillnow 24000 is 0.004296950044712207
testing: bce: 26.516571, dice: 80.782608, loss: 53.649590
IoU in current test batch is 0.893631003097592
Epoch 24002: reducing learning rate of group 0 to 7.9683e-04.
training: bce: 0.028683, dice: 0.087413, loss: 0.058048
training IoU in current batch 24100 is 0.6369919274890242
training IoU uptillnow 24100 is 0.0042923361680794235
testing: bce: 26.588331, dice: 81.028217, loss: 53.808274
IoU in current test batch is 0.8860263840285362
Epoch 24103: reducing learning rate of group 0 to 7.9603e-04.
training: bce: 0.028634, dice: 0.087300, loss: 0.057967
training IoU in current batch 24200 is 0.9207099024325637
training IoU uptillnow 24200 is 0.004293622120494958
testing: bce: 26.652631, dice: 81.259116, loss: 53.955873
IoU in current test batch is 0.8989386714603685
Epoch 24204: reducing learning rate of group 0 to 7.9524e-04.
training: bce: 0.028585, dice: 0.087224, loss: 0.057905
training IoU in current batch 24300 is 0.9142772686964233
training IoU uptillnow 24300 is 0.004294765136103317
testing: bce: 26.717082, dice: 81.524669, loss: 54.120876
IoU in current test batch is 0.9020808433613587
Epoch 24305: reducing learning rate of group 0 to 7.9444e-04.
training: bce: 0.028578, dice: 0.087170, loss: 0.057874
training IoU in current batch 24400 is 0.9370341724089439
training IoU uptillnow 24400 is 0.004296365093998245
testing: bce: 26.820059, dice: 81.808641, loss: 54.314350
IoU in current test batch is 0.820774152300806
Epoch 24406: reducing learning rate of group 0 to 7.9365e-04.
training: bce: 0.028530, dice: 0.087075, loss: 0.057803
training IoU in current batch 24500 is 0.9087173868383209
training IoU uptillnow 24500 is 0.00429737412154893
testing: bce: 26.885479, dice: 82.054809, loss: 54.470144
IoU in current test batch is 0.8806537260553845
Epoch 24507: reducing learning rate of group 0 to 7.9285e-04.
training: bce: 0.028489, dice: 0.086986, loss: 0.057738
training IoU in current batch 24600 is 0.9166006861968857
training IoU uptillnow 24600 is 0.004298535169105677
testing: bce: 26.956498, dice: 82.305498, loss: 54.630998
IoU in current test batch is 0.8657304643803485
Epoch 24608: reducing learning rate of group 0 to 7.9206e-04.
training: bce: 0.028439, dice: 0.086876, loss: 0.057658
training IoU in current batch 24700 is 0.9331569062660064
training IoU uptillnow 24700 is 0.004300021948435357
testing: bce: 27.017991, dice: 82.535722, loss: 54.776857
IoU in current test batch is 0.8979038644880196
Epoch 24709: reducing learning rate of group 0 to 7.9127e-04.
training: bce: 0.028382, dice: 0.086750, loss: 0.057566
training IoU in current batch 24800 is 0.939223317048936
training IoU uptillnow 24800 is 0.004301619039830097
testing: bce: 27.073534, dice: 82.749382, loss: 54.911458
IoU in current test batch is 0.8777972771142214
Epoch 24810: reducing learning rate of group 0 to 7.9048e-04.
training: bce: 0.028347, dice: 0.086638, loss: 0.057493
training IoU in current batch 24900 is 0.9473663552869142
training IoU uptillnow 24900 is 0.004303366811954126
testing: bce: 27.149143, dice: 82.975761, loss: 55.062452
IoU in current test batch is 0.8976142590096898
Epoch 24911: reducing learning rate of group 0 to 7.8969e-04.
training: bce: 0.028293, dice: 0.086511, loss: 0.057402
training IoU in current batch 25000 is 0.9457976495013533
training IoU uptillnow 25000 is 0.004305069229599631
testing: bce: 27.205463, dice: 83.187423, loss: 55.196443
IoU in current test batch is 0.8715467518491615
Epoch 25012: reducing learning rate of group 0 to 7.8890e-04.
training: bce: 0.028246, dice: 0.086412, loss: 0.057329
training IoU in current batch 25100 is 0.9290293416618722
training IoU uptillnow 25100 is 0.004306424065975511
testing: bce: 27.269616, dice: 83.424068, loss: 55.346842
IoU in current test batch is 0.8914420397840193
Epoch 25113: reducing learning rate of group 0 to 7.8811e-04.
training: bce: 0.028192, dice: 0.086305, loss: 0.057248
training IoU in current batch 25200 is 0.9208329820419864
training IoU uptillnow 25200 is 0.004307605530378647
testing: bce: 27.325519, dice: 83.652666, loss: 55.489093
IoU in current test batch is 0.8917525426853767
Epoch 25214: reducing learning rate of group 0 to 7.8732e-04.
training: bce: 0.028163, dice: 0.086218, loss: 0.057191
training IoU in current batch 25300 is 0.9107370069492023
training IoU uptillnow 25300 is 0.004308578138197973
testing: bce: 27.405523, dice: 83.900508, loss: 55.653015
IoU in current test batch is 0.8821507269518564
Epoch 25315: reducing learning rate of group 0 to 7.8653e-04.
training: bce: 0.028148, dice: 0.086146, loss: 0.057147
training IoU in current batch 25400 is 0.8810879190385832
training IoU uptillnow 25400 is 0.004308959467503885
testing: bce: 27.499478, dice: 84.161083, loss: 55.830281
IoU in current test batch is 0.877021020730941
Epoch 25416: reducing learning rate of group 0 to 7.8575e-04.
training: bce: 0.028111, dice: 0.086064, loss: 0.057087
training IoU in current batch 25500 is 0.7832312375512045
training IoU uptillnow 25500 is 0.004307419122890937
testing: bce: 27.571240, dice: 84.411829, loss: 55.991535
IoU in current test batch is 0.8769349341351131
Epoch 25517: reducing learning rate of group 0 to 7.8496e-04.
training: bce: 0.028086, dice: 0.085996, loss: 0.057041
training IoU in current batch 25600 is 0.9350817710410849
training IoU uptillnow 25600 is 0.004308856526634207
testing: bce: 27.654596, dice: 84.676581, loss: 56.165588
IoU in current test batch is 0.8508385919068812
Epoch 25618: reducing learning rate of group 0 to 7.8418e-04.
training: bce: 0.028031, dice: 0.085867, loss: 0.056949
training IoU in current batch 25700 is 0.9318467843428313
training IoU uptillnow 25700 is 0.004310219809755798
testing: bce: 27.708292, dice: 84.880012, loss: 56.294152
IoU in current test batch is 0.9089202316696994
Epoch 25719: reducing learning rate of group 0 to 7.8339e-04.
training: bce: 0.027987, dice: 0.085779, loss: 0.056883
training IoU in current batch 25800 is 0.9292241168311985
training IoU uptillnow 25800 is 0.004311521700280972
testing: bce: 27.772741, dice: 85.122933, loss: 56.447837
IoU in current test batch is 0.8446661456796349
Epoch 25820: reducing learning rate of group 0 to 7.8261e-04.
training: bce: 0.027937, dice: 0.085664, loss: 0.056800
training IoU in current batch 25900 is 0.9435303870926599
training IoU uptillnow 25900 is 0.00431308971014616
testing: bce: 27.830165, dice: 85.338142, loss: 56.584153
IoU in current test batch is 0.8734836460058842
Epoch 25921: reducing learning rate of group 0 to 7.8183e-04.
training: bce: 0.027908, dice: 0.085573, loss: 0.056740
training IoU in current batch 26000 is 0.8821614339707368
training IoU uptillnow 26000 is 0.00431346553207496
testing: bce: 27.909378, dice: 85.575824, loss: 56.742601
IoU in current test batch is 0.8914448438533201
Epoch 26022: reducing learning rate of group 0 to 7.8104e-04.
training: bce: 0.027857, dice: 0.085463, loss: 0.056660
training IoU in current batch 26100 is 0.939350746534583
training IoU uptillnow 26100 is 0.004314934012978367
testing: bce: 27.965008, dice: 85.795271, loss: 56.880139
IoU in current test batch is 0.8598431547369286
Epoch 26123: reducing learning rate of group 0 to 7.8026e-04.
training: bce: 0.027828, dice: 0.085423, loss: 0.056626
training IoU in current batch 26200 is 0.8485151151950993
training IoU uptillnow 26200 is 0.004314657846278612
testing: bce: 28.042795, dice: 86.083851, loss: 57.063323
IoU in current test batch is 0.8821839060572357
Epoch 26224: reducing learning rate of group 0 to 7.7948e-04.
training: bce: 0.027811, dice: 0.085349, loss: 0.056580
training IoU in current batch 26300 is 0.9394343649946638
training IoU uptillnow 26300 is 0.004316112216753858
testing: bce: 28.132960, dice: 86.337134, loss: 57.235047
IoU in current test batch is 0.88812305763548
Epoch 26325: reducing learning rate of group 0 to 7.7870e-04.
training: bce: 0.027779, dice: 0.085288, loss: 0.056534
training IoU in current batch 26400 is 0.9205409106470387
training IoU uptillnow 26400 is 0.0043171977526672
testing: bce: 28.207863, dice: 86.603593, loss: 57.405728
IoU in current test batch is 0.8749177854063984
Epoch 26426: reducing learning rate of group 0 to 7.7792e-04.
training: bce: 0.027731, dice: 0.085189, loss: 0.056460
training IoU in current batch 26500 is 0.9375410605349601
training IoU uptillnow 26500 is 0.004318595841607269
testing: bce: 28.265693, dice: 86.830389, loss: 57.548041
IoU in current test batch is 0.8681332739546523
Epoch 26527: reducing learning rate of group 0 to 7.7715e-04.
training: bce: 0.027690, dice: 0.085113, loss: 0.056402
training IoU in current batch 26600 is 0.9366777930492833
training IoU uptillnow 26600 is 0.004319967192773162
testing: bce: 28.330037, dice: 87.080874, loss: 57.705455
IoU in current test batch is 0.8895518196637043
Epoch 26628: reducing learning rate of group 0 to 7.7637e-04.
training: bce: 0.027643, dice: 0.085014, loss: 0.056328
training IoU in current batch 26700 is 0.9271757848721685
training IoU uptillnow 26700 is 0.004321150338466535
testing: bce: 28.387878, dice: 87.306179, loss: 57.847029
IoU in current test batch is 0.9056587571595617
Epoch 26790: reducing learning rate of group 0 to 7.7559e-04.
training: bce: 0.027593, dice: 0.084902, loss: 0.056247
training IoU in current batch 26800 is 0.9321807576077026
training IoU uptillnow 26800 is 0.004322418027916825
testing: bce: 28.442677, dice: 87.517704, loss: 57.980190
IoU in current test batch is 0.9053557404440581
Epoch 26891: reducing learning rate of group 0 to 7.7482e-04.
training: bce: 0.027575, dice: 0.084840, loss: 0.056208
training IoU in current batch 26900 is 0.918195339613287
training IoU uptillnow 26900 is 0.004323416350173059
testing: bce: 28.530694, dice: 87.780228, loss: 58.155461
IoU in current test batch is 0.8539502383584154
Epoch 26992: reducing learning rate of group 0 to 7.7404e-04.
training: bce: 0.027552, dice: 0.084795, loss: 0.056174
training IoU in current batch 27000 is 0.9105741174536638
training IoU uptillnow 27000 is 0.004324266149206781
testing: bce: 28.612880, dice: 88.060089, loss: 58.336485
IoU in current test batch is 0.8953016456414615
Epoch 27093: reducing learning rate of group 0 to 7.7327e-04.
training: bce: 0.027530, dice: 0.084740, loss: 0.056135
training IoU in current batch 27100 is 0.9277268166833883
training IoU uptillnow 27100 is 0.00432542613568038
testing: bce: 28.696264, dice: 88.328846, loss: 58.512555
IoU in current test batch is 0.8558852587017356
Epoch 27194: reducing learning rate of group 0 to 7.7250e-04.
training: bce: 0.027489, dice: 0.084654, loss: 0.056071
training IoU in current batch 27200 is 0.9432224310084374
training IoU uptillnow 27200 is 0.004326862428534915
testing: bce: 28.758262, dice: 88.564786, loss: 58.661524
IoU in current test batch is 0.8977633178780989
Epoch 27295: reducing learning rate of group 0 to 7.7172e-04.
training: bce: 0.027438, dice: 0.084537, loss: 0.055987
training IoU in current batch 27300 is 0.8973412538208663
training IoU uptillnow 27300 is 0.004327447915662014
testing: bce: 28.811048, dice: 88.766581, loss: 58.788814
IoU in current test batch is 0.8383724984278579
Epoch 27396: reducing learning rate of group 0 to 7.7095e-04.
training: bce: 0.027421, dice: 0.084501, loss: 0.055961
training IoU in current batch 27400 is 0.9157306390068254
training IoU uptillnow 27400 is 0.004328364689792054
testing: bce: 28.898126, dice: 89.054362, loss: 58.976244
IoU in current test batch is 0.8905713018943681
Epoch 27497: reducing learning rate of group 0 to 7.7018e-04.
training: bce: 0.027376, dice: 0.084401, loss: 0.055888
training IoU in current batch 27500 is 0.9597046677512201
training IoU uptillnow 27500 is 0.00433007429543899
testing: bce: 28.956052, dice: 89.273556, loss: 59.114804
IoU in current test batch is 0.8912305146132392
Epoch 27598: reducing learning rate of group 0 to 7.6941e-04.
training: bce: 0.027330, dice: 0.084294, loss: 0.055812
training IoU in current batch 27600 is 0.8465641451925742
training IoU uptillnow 27600 is 0.004329721940200136
testing: bce: 29.012676, dice: 89.484427, loss: 59.248551
IoU in current test batch is 0.8561519254744046
Epoch 27699: reducing learning rate of group 0 to 7.6864e-04.
training: bce: 0.027284, dice: 0.084194, loss: 0.055739
training IoU in current batch 27700 is 0.9189944134078212
training IoU uptillnow 27700 is 0.004330679487316988
testing: bce: 29.068480, dice: 89.702085, loss: 59.385283
IoU in current test batch is 0.882376152341912
Epoch 27800: reducing learning rate of group 0 to 7.6787e-04.
training: bce: 0.027239, dice: 0.084107, loss: 0.055673
training IoU in current batch 27800 is 0.8953683586806241
training IoU uptillnow 27800 is 0.004331205232096263
testing: bce: 29.125996, dice: 89.932592, loss: 59.529294
IoU in current test batch is 0.8824530352282918
training: bce: 0.027271, dice: 0.084095, loss: 0.055683
training IoU in current batch 27900 is 0.9039563690146053
training IoU uptillnow 27900 is 0.0043318811097098845
testing: bce: 29.265450, dice: 90.243686, loss: 59.754568
IoU in current test batch is 0.8563073112544717
Epoch 27901: reducing learning rate of group 0 to 7.6710e-04.
training: bce: 0.027245, dice: 0.084058, loss: 0.055651
training IoU in current batch 28000 is 0.8763445449889021
training IoU uptillnow 28000 is 0.004332059109121458
testing: bce: 29.341753, dice: 90.527081, loss: 59.934417
IoU in current test batch is 0.887771544389872
Epoch 28002: reducing learning rate of group 0 to 7.6634e-04.
training: bce: 0.027203, dice: 0.083973, loss: 0.055588
training IoU in current batch 28100 is 0.9408537709754748
training IoU uptillnow 28100 is 0.004333383651827254
testing: bce: 29.400925, dice: 90.758476, loss: 60.079700
IoU in current test batch is 0.9026652629574781
Epoch 28103: reducing learning rate of group 0 to 7.6557e-04.
training: bce: 0.027168, dice: 0.083905, loss: 0.055536
training IoU in current batch 28200 is 0.828882060978492
training IoU uptillnow 28200 is 0.004332713557337929
testing: bce: 29.468025, dice: 91.007481, loss: 60.237753
IoU in current test batch is 0.8834002741392197
Epoch 28204: reducing learning rate of group 0 to 7.6481e-04.
training: bce: 0.027128, dice: 0.083821, loss: 0.055475
training IoU in current batch 28300 is 0.784964617953407
training IoU uptillnow 28300 is 0.00433127229919309
testing: bce: 29.529191, dice: 91.238924, loss: 60.384058
IoU in current test batch is 0.8813425240628464
Epoch 28305: reducing learning rate of group 0 to 7.6404e-04.
training: bce: 0.027101, dice: 0.083757, loss: 0.055429
training IoU in current batch 28400 is 0.7736849138123023
training IoU uptillnow 28400 is 0.004329642611047843
testing: bce: 29.603559, dice: 91.491842, loss: 60.547701
IoU in current test batch is 0.8728040696579414
Epoch 28406: reducing learning rate of group 0 to 7.6328e-04.
training: bce: 0.027075, dice: 0.083698, loss: 0.055387
training IoU in current batch 28500 is 0.9126418225032513
training IoU uptillnow 28500 is 0.004330462113877457
testing: bce: 29.679793, dice: 91.749021, loss: 60.714407
IoU in current test batch is 0.8880432900505782
Epoch 28507: reducing learning rate of group 0 to 7.6251e-04.
training: bce: 0.027066, dice: 0.083627, loss: 0.055347
training IoU in current batch 28600 is 0.9122235686155711
training IoU uptillnow 28600 is 0.00433126857424318
testing: bce: 29.773766, dice: 91.992903, loss: 60.883334
IoU in current test batch is 0.8681713864496194
Epoch 28608: reducing learning rate of group 0 to 7.6175e-04.
training: bce: 0.027030, dice: 0.083566, loss: 0.055298
training IoU in current batch 28700 is 0.8913396897930489
training IoU uptillnow 28700 is 0.0043317055969069275
testing: bce: 29.837871, dice: 92.247490, loss: 61.042680
IoU in current test batch is 0.9064328771742853
Epoch 28709: reducing learning rate of group 0 to 7.6099e-04.
training: bce: 0.027005, dice: 0.083543, loss: 0.055274
training IoU in current batch 28800 is 0.9242254572601717
training IoU uptillnow 28800 is 0.00433271049843602
testing: bce: 29.914321, dice: 92.543354, loss: 61.228838
IoU in current test batch is 0.8958510960258926
Epoch 28810: reducing learning rate of group 0 to 7.6023e-04.
training: bce: 0.026967, dice: 0.083456, loss: 0.055211
training IoU in current batch 28900 is 0.9174856295197478
training IoU uptillnow 28900 is 0.004333591843888297
testing: bce: 29.976012, dice: 92.767421, loss: 61.371716
IoU in current test batch is 0.8887369304390179
Epoch 28911: reducing learning rate of group 0 to 7.5947e-04.
training: bce: 0.026930, dice: 0.083370, loss: 0.055150
training IoU in current batch 29000 is 0.915502504047897
training IoU uptillnow 29000 is 0.004334432920666171
testing: bce: 30.037786, dice: 92.992498, loss: 61.515142
IoU in current test batch is 0.875902883805429
Epoch 29012: reducing learning rate of group 0 to 7.5871e-04.
training: bce: 0.026907, dice: 0.083298, loss: 0.055103
training IoU in current batch 29100 is 0.8724959742351047
training IoU uptillnow 29100 is 0.0043345292986274415
testing: bce: 30.116669, dice: 93.232711, loss: 61.674690
IoU in current test batch is 0.869664680748871
Epoch 29113: reducing learning rate of group 0 to 7.5795e-04.
training: bce: 0.027012, dice: 0.083277, loss: 0.055145
training IoU in current batch 29200 is 0.9136985179726521
training IoU uptillnow 29200 is 0.004335330515336582
testing: bce: 30.337938, dice: 93.529976, loss: 61.933957
IoU in current test batch is 0.9004560042946822
Epoch 29214: reducing learning rate of group 0 to 7.5719e-04.
training: bce: 0.026974, dice: 0.083184, loss: 0.055079
training IoU in current batch 29300 is 0.9190974377161096
training IoU uptillnow 29300 is 0.004336218391768253
testing: bce: 30.398591, dice: 93.745628, loss: 62.072110
IoU in current test batch is 0.853292315985004
Epoch 29315: reducing learning rate of group 0 to 7.5643e-04.
training: bce: 0.026950, dice: 0.083107, loss: 0.055029
training IoU in current batch 29400 is 0.9428103385511467
training IoU uptillnow 29400 is 0.004337503495339517
testing: bce: 30.474956, dice: 93.978351, loss: 62.226654
IoU in current test batch is 0.900956406593141
Epoch 29416: reducing learning rate of group 0 to 7.5568e-04.
training: bce: 0.026918, dice: 0.083038, loss: 0.054978
training IoU in current batch 29500 is 0.9416610054347826
training IoU uptillnow 29500 is 0.004338760407077541
testing: bce: 30.543035, dice: 94.219189, loss: 62.381112
IoU in current test batch is 0.8898324705322881
Epoch 29517: reducing learning rate of group 0 to 7.5492e-04.
training: bce: 0.026886, dice: 0.082978, loss: 0.054932
training IoU in current batch 29600 is 0.8970258370025754
training IoU uptillnow 29600 is 0.004339254879487039
testing: bce: 30.609978, dice: 94.470582, loss: 62.540280
IoU in current test batch is 0.9098269542061796
Epoch 29618: reducing learning rate of group 0 to 7.5417e-04.
training: bce: 0.026853, dice: 0.082913, loss: 0.054883
training IoU in current batch 29700 is 0.9526513831788725
training IoU uptillnow 29700 is 0.004340682447704968
testing: bce: 30.675882, dice: 94.715447, loss: 62.695665
IoU in current test batch is 0.9144363951152343
Epoch 29719: reducing learning rate of group 0 to 7.5341e-04.
training: bce: 0.026816, dice: 0.082847, loss: 0.054832
training IoU in current batch 29800 is 0.8855552410604774
training IoU uptillnow 29800 is 0.004340974698829419
testing: bce: 30.736060, dice: 94.959135, loss: 62.847598
IoU in current test batch is 0.8564694916243492
Epoch 29820: reducing learning rate of group 0 to 7.5266e-04.
training: bce: 0.026775, dice: 0.082750, loss: 0.054763
training IoU in current batch 29900 is 0.9105885260079898
training IoU uptillnow 29900 is 0.004341683597967275
testing: bce: 30.792487, dice: 95.166248, loss: 62.979368
IoU in current test batch is 0.9065967295533521
Epoch 29921: reducing learning rate of group 0 to 7.5191e-04.
training: bce: 0.026737, dice: 0.082672, loss: 0.054704
training IoU in current batch 30000 is 0.8403648360178537
training IoU uptillnow 30000 is 0.004341217415447099
testing: bce: 30.851073, dice: 95.393393, loss: 63.122233
IoU in current test batch is 0.8970801815857015
Epoch 30022: reducing learning rate of group 0 to 7.5116e-04.
training: bce: 0.026710, dice: 0.082602, loss: 0.054656
training IoU in current batch 30100 is 0.8601911535896866
training IoU uptillnow 30100 is 0.004341083660264552
testing: bce: 30.923262, dice: 95.631380, loss: 63.277321
IoU in current test batch is 0.8902498569300208
Epoch 30123: reducing learning rate of group 0 to 7.5040e-04.
training: bce: 0.026680, dice: 0.082528, loss: 0.054604
training IoU in current batch 30200 is 0.9430091802745958
training IoU uptillnow 30200 is 0.0043423219048296605
testing: bce: 30.990905, dice: 95.862868, loss: 63.426887
IoU in current test batch is 0.8625445952944217
Epoch 30224: reducing learning rate of group 0 to 7.4965e-04.
training: bce: 0.026641, dice: 0.082437, loss: 0.054539
training IoU in current batch 30300 is 0.8611903897047377
training IoU uptillnow 30300 is 0.004342201875931915
testing: bce: 31.047841, dice: 96.074130, loss: 63.560986
IoU in current test batch is 0.8781946889846655
Epoch 30325: reducing learning rate of group 0 to 7.4890e-04.
training: bce: 0.026604, dice: 0.082354, loss: 0.054479
training IoU in current batch 30400 is 0.9338018424789717
training IoU uptillnow 30400 is 0.004343276864703544
testing: bce: 31.106934, dice: 96.293874, loss: 63.700404
IoU in current test batch is 0.9047543790908286
Epoch 30426: reducing learning rate of group 0 to 7.4816e-04.
training: bce: 0.026567, dice: 0.082277, loss: 0.054422
training IoU in current batch 30500 is 0.9212930117062789
training IoU uptillnow 30500 is 0.004344139748523182
testing: bce: 31.166683, dice: 96.520443, loss: 63.843563
IoU in current test batch is 0.8963297102551157
Epoch 30527: reducing learning rate of group 0 to 7.4741e-04.
training: bce: 0.026534, dice: 0.082223, loss: 0.054379
training IoU in current batch 30600 is 0.9401051641884598
training IoU uptillnow 30600 is 0.004345304370831013
testing: bce: 31.229461, dice: 96.773665, loss: 64.001563
IoU in current test batch is 0.90337320860722
Epoch 30628: reducing learning rate of group 0 to 7.4666e-04.
training: bce: 0.026516, dice: 0.082175, loss: 0.054346
training IoU in current batch 30700 is 0.8414459994287813
training IoU uptillnow 30700 is 0.004344854631820274
testing: bce: 31.310860, dice: 97.032378, loss: 64.171619
IoU in current test batch is 0.8869968269549726
Epoch 30729: reducing learning rate of group 0 to 7.4591e-04.
training: bce: 0.026481, dice: 0.082100, loss: 0.054290
training IoU in current batch 30800 is 0.9567701332029818
training IoU uptillnow 30800 is 0.004346279897344753
testing: bce: 31.370288, dice: 97.260117, loss: 64.315202
IoU in current test batch is 0.8708806363942732
Epoch 30830: reducing learning rate of group 0 to 7.4517e-04.
training: bce: 0.026452, dice: 0.082042, loss: 0.054247
training IoU in current batch 30900 is 0.9378161014841224
training IoU uptillnow 30900 is 0.004347389248531045
testing: bce: 31.437807, dice: 97.507008, loss: 64.472408
IoU in current test batch is 0.9004080669554629
Epoch 30931: reducing learning rate of group 0 to 7.4442e-04.
training: bce: 0.026434, dice: 0.081996, loss: 0.054215
training IoU in current batch 31000 is 0.7346542937552293
training IoU uptillnow 31000 is 0.004345214745193233
testing: bce: 31.518661, dice: 97.767110, loss: 64.642885
IoU in current test batch is 0.890484496460108
Epoch 31032: reducing learning rate of group 0 to 7.4368e-04.
training: bce: 0.026406, dice: 0.081938, loss: 0.054172
training IoU in current batch 31100 is 0.9363290116569846
training IoU uptillnow 31100 is 0.004346296479906238
testing: bce: 31.587092, dice: 98.013085, loss: 64.800088
IoU in current test batch is 0.895697614241729
Epoch 31133: reducing learning rate of group 0 to 7.4293e-04.
training: bce: 0.026386, dice: 0.081884, loss: 0.054135
training IoU in current batch 31200 is 0.9448612807537952
training IoU uptillnow 31200 is 0.004347508011343894
testing: bce: 31.664428, dice: 98.264299, loss: 64.964364
IoU in current test batch is 0.8473031245559473
Epoch 31234: reducing learning rate of group 0 to 7.4219e-04.
training: bce: 0.026370, dice: 0.081831, loss: 0.054101
training IoU in current batch 31300 is 0.890869197012857
training IoU uptillnow 31300 is 0.004347849335818257
testing: bce: 31.746552, dice: 98.515640, loss: 65.131096
IoU in current test batch is 0.8775815688937046
Epoch 31335: reducing learning rate of group 0 to 7.4145e-04.
training: bce: 0.026440, dice: 0.081786, loss: 0.054113
training IoU in current batch 31400 is 0.86036997509783
training IoU uptillnow 31400 is 0.004347702845386968
testing: bce: 31.932344, dice: 98.775839, loss: 65.354092
IoU in current test batch is 0.8955061785818852
Epoch 31436: reducing learning rate of group 0 to 7.4071e-04.
training: bce: 0.026408, dice: 0.081728, loss: 0.054068
training IoU in current batch 31500 is 0.893569942912776
training IoU uptillnow 31500 is 0.004348084251911132
testing: bce: 31.995032, dice: 99.020001, loss: 65.507516
IoU in current test batch is 0.883880917782772
Epoch 31537: reducing learning rate of group 0 to 7.3997e-04.
training: bce: 0.026374, dice: 0.081644, loss: 0.054009
training IoU in current batch 31600 is 0.940938441670254
training IoU uptillnow 31600 is 0.004349212722391307
testing: bce: 32.056144, dice: 99.232349, loss: 65.644246
IoU in current test batch is 0.8838738182205286
Epoch 31638: reducing learning rate of group 0 to 7.3923e-04.
training: bce: 0.026338, dice: 0.081567, loss: 0.053953
training IoU in current batch 31700 is 0.9433612744871023
training IoU uptillnow 31700 is 0.004350372287231672
testing: bce: 32.113493, dice: 99.452321, loss: 65.782907
IoU in current test batch is 0.883916842176266
Epoch 31739: reducing learning rate of group 0 to 7.3849e-04.
training: bce: 0.026304, dice: 0.081495, loss: 0.053900
training IoU in current batch 31800 is 0.9166516910469755
training IoU uptillnow 31800 is 0.004351104610642895
testing: bce: 32.172918, dice: 99.678190, loss: 65.925554
IoU in current test batch is 0.8862304142149006
Epoch 31840: reducing learning rate of group 0 to 7.3775e-04.
training: bce: 0.026266, dice: 0.081403, loss: 0.053835
training IoU in current batch 31900 is 0.9173529266729129
training IoU uptillnow 31900 is 0.004351843333638168
testing: bce: 32.227980, dice: 99.878881, loss: 66.053431
IoU in current test batch is 0.8383706209030701
Epoch 31941: reducing learning rate of group 0 to 7.3701e-04.
training: bce: 0.026268, dice: 0.081382, loss: 0.053825
training IoU in current batch 32000 is 0.903267348590772
training IoU uptillnow 32000 is 0.004352357359478971
testing: bce: 32.331322, dice: 100.165811, loss: 66.248566
IoU in current test batch is 0.8743105174664731
Epoch 32042: reducing learning rate of group 0 to 7.3627e-04.
training: bce: 0.026234, dice: 0.081311, loss: 0.053772
training IoU in current batch 32100 is 0.8533414895243921
training IoU uptillnow 32100 is 0.004352090545635611
testing: bce: 32.389566, dice: 100.390681, loss: 66.390123
IoU in current test batch is 0.8733026081841861
Epoch 32143: reducing learning rate of group 0 to 7.3554e-04.
training: bce: 0.026208, dice: 0.081257, loss: 0.053733
training IoU in current batch 32200 is 0.6941433877779474
training IoU uptillnow 32200 is 0.004349353445524603
testing: bce: 32.458191, dice: 100.637302, loss: 66.547747
IoU in current test batch is 0.8869584621597884
Epoch 32244: reducing learning rate of group 0 to 7.3480e-04.
training: bce: 0.026222, dice: 0.081251, loss: 0.053736
training IoU in current batch 32300 is 0.9366245595943972
training IoU uptillnow 32300 is 0.004350386755182035
testing: bce: 32.576529, dice: 100.941423, loss: 66.758976
IoU in current test batch is 0.8585488179496255
Epoch 32345: reducing learning rate of group 0 to 7.3407e-04.
training: bce: 0.026194, dice: 0.081189, loss: 0.053691
training IoU in current batch 32400 is 0.8206962678861189
training IoU uptillnow 32400 is 0.004349624724949167
testing: bce: 32.642850, dice: 101.176880, loss: 66.909865
IoU in current test batch is 0.8906902158774016
Epoch 32446: reducing learning rate of group 0 to 7.3333e-04.
training: bce: 0.026169, dice: 0.081151, loss: 0.053660
training IoU in current batch 32500 is 0.8180187720451964
training IoU uptillnow 32500 is 0.00434882619301254
testing: bce: 32.711972, dice: 101.442352, loss: 67.077162
IoU in current test batch is 0.8911557023352491
Epoch 32547: reducing learning rate of group 0 to 7.3260e-04.
training: bce: 0.160405, dice: 0.262766, loss: 0.211586
training IoU in current batch 0 is 0.5869322534748823
training IoU uptillnow 0 is 9.01599492273126e-06
testing: bce: 0.006169, dice: 0.010106, loss: 0.008138
IoU in current test batch is 0.0
Epoch 32648: reducing learning rate of group 0 to 7.3187e-04.
training: bce: 0.080753, dice: 0.217804, loss: 0.149279
training IoU in current batch 100 is 0.8843531926908959
training IoU uptillnow 100 is 2.2531515737848636e-05
testing: bce: 0.313694, dice: 0.846086, loss: 0.579890
IoU in current test batch is 0.809023288401758
Epoch 32749: reducing learning rate of group 0 to 7.3114e-04.
training: bce: 0.062779, dice: 0.159817, loss: 0.111298
training IoU in current batch 200 is 0.9214722455341087
training IoU uptillnow 200 is 3.6531209510067125e-05
testing: bce: 0.485332, dice: 1.235512, loss: 0.860422
IoU in current test batch is 0.8050916924306816
training: bce: 0.049289, dice: 0.135710, loss: 0.092499
training IoU in current batch 300 is 0.9403291399034625
training IoU uptillnow 300 is 5.073268743212757e-05
testing: bce: 0.570616, dice: 1.571101, loss: 1.070859
IoU in current test batch is 0.8869141886298627
Epoch 32850: reducing learning rate of group 0 to 7.3040e-04.
training: bce: 0.041980, dice: 0.119862, loss: 0.080921
training IoU in current batch 400 is 0.9298192641159471
training IoU uptillnow 400 is 6.468847927463689e-05
testing: bce: 0.647459, dice: 1.848643, loss: 1.248051
IoU in current test batch is 0.8753058128718929
Epoch 32951: reducing learning rate of group 0 to 7.2967e-04.
training: bce: 0.036810, dice: 0.108730, loss: 0.072770
training IoU in current batch 500 is 0.8809620048504446
training IoU uptillnow 500 is 7.782066446647819e-05
testing: bce: 0.709297, dice: 2.095147, loss: 1.402222
IoU in current test batch is 0.8747508066435032
Epoch 33052: reducing learning rate of group 0 to 7.2894e-04.
training: bce: 0.035675, dice: 0.103052, loss: 0.069363
training IoU in current batch 600 is 0.9153659138155809
training IoU uptillnow 600 is 9.139254007428954e-05
testing: bce: 0.824637, dice: 2.382076, loss: 1.603357
IoU in current test batch is 0.8759080280432101
Epoch 33153: reducing learning rate of group 0 to 7.2822e-04.
training: bce: 0.033406, dice: 0.099566, loss: 0.066486
training IoU in current batch 700 is 0.8864348677766896
training IoU uptillnow 700 is 0.00010444771924633472
testing: bce: 0.900671, dice: 2.684451, loss: 1.792561
IoU in current test batch is 0.8821522402519439
Epoch 33254: reducing learning rate of group 0 to 7.2749e-04.
training: bce: 0.031208, dice: 0.095433, loss: 0.063320
training IoU in current batch 800 is 0.9406945985152316
training IoU uptillnow 800 is 0.00011823810672839538
testing: bce: 0.961432, dice: 2.940082, loss: 1.950757
IoU in current test batch is 0.8667948408301178
Epoch 33355: reducing learning rate of group 0 to 7.2676e-04.
training: bce: 0.029419, dice: 0.091539, loss: 0.060479
training IoU in current batch 900 is 0.9291392323751405
training IoU uptillnow 900 is 0.00013177331070796848
testing: bce: 1.019486, dice: 3.172170, loss: 2.095828
IoU in current test batch is 0.8925385007508067
Epoch 33456: reducing learning rate of group 0 to 7.2603e-04.
training: bce: 0.028342, dice: 0.088830, loss: 0.058586
training IoU in current batch 1000 is 0.9363903714192354
training IoU uptillnow 1000 is 0.00014533589300096304
testing: bce: 1.091173, dice: 3.419945, loss: 2.255559
IoU in current test batch is 0.8996509161465845
Epoch 33557: reducing learning rate of group 0 to 7.2531e-04.
training: bce: 0.027139, dice: 0.086118, loss: 0.056629
training IoU in current batch 1100 is 0.8990659837300392
training IoU uptillnow 1100 is 0.00015826325901130268
testing: bce: 1.149252, dice: 3.646776, loss: 2.398014
IoU in current test batch is 0.8884994330690081
Epoch 33658: reducing learning rate of group 0 to 7.2458e-04.
training: bce: 0.026378, dice: 0.084158, loss: 0.055268
training IoU in current batch 1200 is 0.9217926186291739
training IoU uptillnow 1200 is 0.00017145071314879975
testing: bce: 1.218456, dice: 3.887441, loss: 2.552949
IoU in current test batch is 0.8939977551299786
Epoch 33759: reducing learning rate of group 0 to 7.2386e-04.
training: bce: 0.025701, dice: 0.082701, loss: 0.054201
training IoU in current batch 1300 is 0.7978306718891233
training IoU uptillnow 1300 is 0.000182729174119558
testing: bce: 1.286017, dice: 4.138221, loss: 2.712119
IoU in current test batch is 0.8877478330509411
Epoch 33860: reducing learning rate of group 0 to 7.2313e-04.
training: bce: 0.024969, dice: 0.081120, loss: 0.053044
training IoU in current batch 1400 is 0.8717750826901874
training IoU uptillnow 1400 is 0.0001950302278591753
testing: bce: 1.345421, dice: 4.371117, loss: 2.858269
IoU in current test batch is 0.8927610576398768
Epoch 33961: reducing learning rate of group 0 to 7.2241e-04.
training: bce: 0.024996, dice: 0.080761, loss: 0.052879
training IoU in current batch 1500 is 0.94179534153363
training IoU uptillnow 1500 is 0.00020828724038449574
testing: bce: 1.443050, dice: 4.662398, loss: 3.052724
IoU in current test batch is 0.8910056003207193
Epoch 34062: reducing learning rate of group 0 to 7.2169e-04.
training: bce: 0.024740, dice: 0.080233, loss: 0.052487
training IoU in current batch 1600 is 0.8933710890353728
training IoU uptillnow 1600 is 0.00022075760804666464
testing: bce: 1.523437, dice: 4.940487, loss: 3.231962
IoU in current test batch is 0.8890595845283772
Epoch 34163: reducing learning rate of group 0 to 7.2097e-04.
training: bce: 0.024167, dice: 0.079149, loss: 0.051658
training IoU in current batch 1700 is 0.9666434003159325
training IoU uptillnow 1700 is 0.00023422483937422561
testing: bce: 1.581075, dice: 5.178146, loss: 3.379610
IoU in current test batch is 0.9062206453503296
Epoch 34264: reducing learning rate of group 0 to 7.2024e-04.
training: bce: 0.023695, dice: 0.078013, loss: 0.050854
training IoU in current batch 1800 is 0.9034538220604419
training IoU uptillnow 1800 is 0.00024669385426797367
testing: bce: 1.641351, dice: 5.403898, loss: 3.522625
IoU in current test batch is 0.8895342656513903
Epoch 34365: reducing learning rate of group 0 to 7.1952e-04.
training: bce: 0.023528, dice: 0.077474, loss: 0.050501
training IoU in current batch 1900 is 0.9293023926203516
training IoU uptillnow 1900 is 0.00025946564517592235
testing: bce: 1.720264, dice: 5.664536, loss: 3.692400
IoU in current test batch is 0.8798440744630214
Epoch 34466: reducing learning rate of group 0 to 7.1881e-04.
training: bce: 0.023354, dice: 0.077023, loss: 0.050189
training IoU in current batch 2000 is 0.9306501849328401
training IoU uptillnow 2000 is 0.0002721830080306331
testing: bce: 1.797341, dice: 5.927831, loss: 3.862586
IoU in current test batch is 0.863473631656001
Epoch 34567: reducing learning rate of group 0 to 7.1809e-04.
training: bce: 0.022967, dice: 0.076207, loss: 0.049587
training IoU in current batch 2100 is 0.9320786584630233
training IoU uptillnow 2100 is 0.00028484757832539774
testing: bce: 1.855893, dice: 6.158101, loss: 4.006997
IoU in current test batch is 0.8982611045439468
Epoch 34668: reducing learning rate of group 0 to 7.1737e-04.
training: bce: 0.022663, dice: 0.075626, loss: 0.049144
training IoU in current batch 2200 is 0.914015606242497
training IoU uptillnow 2200 is 0.00029717935418659597
testing: bce: 1.918470, dice: 6.402032, loss: 4.160251
IoU in current test batch is 0.848609656833938
Epoch 34769: reducing learning rate of group 0 to 7.1665e-04.
training: bce: 0.022336, dice: 0.074881, loss: 0.048609
training IoU in current batch 2300 is 0.9052917064751749
training IoU uptillnow 2300 is 0.00030931519308870155
testing: bce: 1.976774, dice: 6.626936, loss: 4.301855
IoU in current test batch is 0.8857235565290169
Epoch 34870: reducing learning rate of group 0 to 7.1593e-04.
training: bce: 0.022371, dice: 0.074566, loss: 0.048469
training IoU in current batch 2400 is 0.9072051902956093
training IoU uptillnow 2400 is 0.0003214089591179419
testing: bce: 2.065915, dice: 6.885904, loss: 4.475909
IoU in current test batch is 0.9039542511759511
Epoch 34971: reducing learning rate of group 0 to 7.1522e-04.
training: bce: 0.022215, dice: 0.074103, loss: 0.048159
training IoU in current batch 2500 is 0.8815312620949298
training IoU uptillnow 2500 is 0.00033306746309476527
testing: bce: 2.136948, dice: 7.128092, loss: 4.632520
IoU in current test batch is 0.7970636907259364
Epoch 35072: reducing learning rate of group 0 to 7.1450e-04.
training: bce: 0.021972, dice: 0.073706, loss: 0.047839
training IoU in current batch 2600 is 0.8770628175727326
training IoU uptillnow 2600 is 0.000344596066986055
testing: bce: 2.198052, dice: 7.373395, loss: 4.785724
IoU in current test batch is 0.89065701110847
Epoch 35173: reducing learning rate of group 0 to 7.1379e-04.
training: bce: 0.021756, dice: 0.073194, loss: 0.047475
training IoU in current batch 2700 is 0.7564038423053833
training IoU uptillnow 2700 is 0.00035434776032792044
testing: bce: 2.260099, dice: 7.603706, loss: 4.931902
IoU in current test batch is 0.873829274947099
Epoch 35274: reducing learning rate of group 0 to 7.1307e-04.
training: bce: 0.021542, dice: 0.072722, loss: 0.047132
training IoU in current batch 2800 is 0.9238083374854675
training IoU uptillnow 2800 is 0.00036641212878320105
testing: bce: 2.320718, dice: 7.834449, loss: 5.077583
IoU in current test batch is 0.8976735414201659
Epoch 35375: reducing learning rate of group 0 to 7.1236e-04.
training: bce: 0.021615, dice: 0.072771, loss: 0.047193
training IoU in current batch 2900 is 0.9135057720683969
training IoU uptillnow 2900 is 0.00037826311887208464
testing: bce: 2.411743, dice: 8.119529, loss: 5.265636
IoU in current test batch is 0.8720261638974158
Epoch 35476: reducing learning rate of group 0 to 7.1165e-04.
training: bce: 0.021455, dice: 0.072384, loss: 0.046919
training IoU in current batch 3000 is 0.9543898717472885
training IoU uptillnow 3000 is 0.0003906224663730744
testing: bce: 2.476376, dice: 8.354734, loss: 5.415555
IoU in current test batch is 0.8682499697888587
Epoch 35577: reducing learning rate of group 0 to 7.1094e-04.
training: bce: 0.021336, dice: 0.072020, loss: 0.046678
training IoU in current batch 3100 is 0.8813462635482031
training IoU uptillnow 3100 is 0.00040188800684732494
testing: bce: 2.544769, dice: 8.589761, loss: 5.567265
IoU in current test batch is 0.8993132949427095
Epoch 35678: reducing learning rate of group 0 to 7.1023e-04.
training: bce: 0.021258, dice: 0.071754, loss: 0.046506
training IoU in current batch 3200 is 0.9082284801278211
training IoU uptillnow 3200 is 0.0004134665027529789
testing: bce: 2.617150, dice: 8.833968, loss: 5.725559
IoU in current test batch is 0.8740421741891227
Epoch 35779: reducing learning rate of group 0 to 7.0952e-04.
training: bce: 0.021190, dice: 0.071304, loss: 0.046247
training IoU in current batch 3300 is 0.9474535443909153
training IoU uptillnow 3300 is 0.00042552748329441353
testing: bce: 2.690337, dice: 9.052891, loss: 5.871614
IoU in current test batch is 0.9138420353936283
Epoch 35880: reducing learning rate of group 0 to 7.0881e-04.
training: bce: 0.021069, dice: 0.070992, loss: 0.046030
training IoU in current batch 3400 is 0.8864780133652339
training IoU uptillnow 3400 is 0.0004366732922306484
testing: bce: 2.755930, dice: 9.286335, loss: 6.021132
IoU in current test batch is 0.8589428734337623
Epoch 35981: reducing learning rate of group 0 to 7.0810e-04.
training: bce: 0.020887, dice: 0.070585, loss: 0.045736
training IoU in current batch 3500 is 0.8899862482810351
training IoU uptillnow 3500 is 0.0004478059236102085
testing: bce: 2.812575, dice: 9.504563, loss: 6.158569
IoU in current test batch is 0.8735337315067709
Epoch 36082: reducing learning rate of group 0 to 7.0739e-04.
training: bce: 0.021307, dice: 0.070700, loss: 0.046003
training IoU in current batch 3600 is 0.9441241025259477
training IoU uptillnow 3600 is 0.0004596257678377069
testing: bce: 2.951016, dice: 9.791949, loss: 6.371483
IoU in current test batch is 0.8884907084389952
Epoch 36183: reducing learning rate of group 0 to 7.0668e-04.
training: bce: 0.021387, dice: 0.070880, loss: 0.046134
training IoU in current batch 3700 is 0.8952986384553346
training IoU uptillnow 3700 is 0.00047070693426604104
testing: bce: 3.044399, dice: 10.089466, loss: 6.566932
IoU in current test batch is 0.8909338964043451
Epoch 36284: reducing learning rate of group 0 to 7.0598e-04.
training: bce: 0.021292, dice: 0.070790, loss: 0.046041
training IoU in current batch 3800 is 0.9259789254585664
training IoU uptillnow 3800 is 0.00048214914858268025
testing: bce: 3.112668, dice: 10.348897, loss: 6.730782
IoU in current test batch is 0.8766882933335329
Epoch 36385: reducing learning rate of group 0 to 7.0527e-04.
training: bce: 0.021208, dice: 0.070605, loss: 0.045907
training IoU in current batch 3900 is 0.8808430291896305
training IoU uptillnow 3900 is 0.0004929094223789338
testing: bce: 3.182074, dice: 10.593496, loss: 6.887785
IoU in current test batch is 0.9035662885006099
Epoch 36486: reducing learning rate of group 0 to 7.0456e-04.
training: bce: 0.021166, dice: 0.070412, loss: 0.045789
training IoU in current batch 4000 is 0.9197271130356632
training IoU uptillnow 4000 is 0.0005041427529109504
testing: bce: 3.257088, dice: 10.835361, loss: 7.046225
IoU in current test batch is 0.7755010441149781
Epoch 36587: reducing learning rate of group 0 to 7.0386e-04.
training: bce: 0.021177, dice: 0.070424, loss: 0.045801
training IoU in current batch 4100 is 0.9322943887489904
training IoU uptillnow 4100 is 0.0005154862342431214
testing: bce: 3.340337, dice: 11.108108, loss: 7.224223
IoU in current test batch is 0.8933766334761433
Epoch 36688: reducing learning rate of group 0 to 7.0316e-04.
training: bce: 0.021024, dice: 0.070144, loss: 0.045584
training IoU in current batch 4200 is 0.910918516759666
training IoU uptillnow 4200 is 0.0005264771493564024
testing: bce: 3.396937, dice: 11.333661, loss: 7.365299
IoU in current test batch is 0.8853274442215677
Epoch 36789: reducing learning rate of group 0 to 7.0245e-04.
training: bce: 0.020863, dice: 0.069723, loss: 0.045293
training IoU in current batch 4300 is 0.8229322316955261
training IoU uptillnow 4300 is 0.0005362145515168692
testing: bce: 3.451209, dice: 11.533787, loss: 7.492498
IoU in current test batch is 0.9124829079640496
Epoch 36890: reducing learning rate of group 0 to 7.0175e-04.
training: bce: 0.020842, dice: 0.069644, loss: 0.045243
training IoU in current batch 4400 is 0.9439576115753006
training IoU uptillnow 4400 is 0.0005475369604976664
testing: bce: 3.527881, dice: 11.788578, loss: 7.658230
IoU in current test batch is 0.8831090287993695
Epoch 36991: reducing learning rate of group 0 to 7.0105e-04.
training: bce: 0.020740, dice: 0.069475, loss: 0.045108
training IoU in current batch 4500 is 0.9026741766884864
training IoU uptillnow 4500 is 0.0005582411101432615
testing: bce: 3.590431, dice: 12.027198, loss: 7.808815
IoU in current test batch is 0.8978954974159118
Epoch 37092: reducing learning rate of group 0 to 7.0035e-04.
training: bce: 0.020660, dice: 0.069233, loss: 0.044946
training IoU in current batch 4600 is 0.8951586699880862
training IoU uptillnow 4600 is 0.0005687864801746136
testing: bce: 3.655939, dice: 12.251521, loss: 7.953730
IoU in current test batch is 0.8826219290376933
Epoch 37193: reducing learning rate of group 0 to 6.9965e-04.
training: bce: 0.020605, dice: 0.068995, loss: 0.044800
training IoU in current batch 4700 is 0.8224405756920944
training IoU uptillnow 4700 is 0.0005782991351049773
testing: bce: 3.725593, dice: 12.474753, loss: 8.100173
IoU in current test batch is 0.8884301204969763
Epoch 37294: reducing learning rate of group 0 to 6.9895e-04.
training: bce: 0.020925, dice: 0.069007, loss: 0.044966
training IoU in current batch 4800 is 0.19242793050782742
training IoU uptillnow 4800 is 0.0005793268343176419
testing: bce: 3.863971, dice: 12.742330, loss: 8.303150
IoU in current test batch is 0.9071046135476231
Epoch 37395: reducing learning rate of group 0 to 6.9825e-04.
training: bce: 0.020907, dice: 0.069033, loss: 0.044970
training IoU in current batch 4900 is 0.9218084375429435
training IoU uptillnow 4900 is 0.0005900872325963828
testing: bce: 3.940886, dice: 13.012635, loss: 8.476760
IoU in current test batch is 0.8873606912887914
Epoch 37496: reducing learning rate of group 0 to 6.9755e-04.
training: bce: 0.020801, dice: 0.068891, loss: 0.044846
training IoU in current batch 5000 is 0.9404261155815655
training IoU uptillnow 5000 is 0.0006010382262056491
testing: bce: 4.001088, dice: 13.250888, loss: 8.625988
IoU in current test batch is 0.8792977668589277
Epoch 37597: reducing learning rate of group 0 to 6.9685e-04.
training: bce: 0.020704, dice: 0.068790, loss: 0.044747
training IoU in current batch 5100 is 0.9063121413556048
training IoU uptillnow 5100 is 0.0006114779995906141
testing: bce: 4.062039, dice: 13.496037, loss: 8.779038
IoU in current test batch is 0.878080401566337
Epoch 37698: reducing learning rate of group 0 to 6.9616e-04.
training: bce: 0.020586, dice: 0.068553, loss: 0.044570
training IoU in current batch 5200 is 0.9387315154494913
training IoU uptillnow 5200 is 0.0006222918635561151
testing: bce: 4.118010, dice: 13.713246, loss: 8.915628
IoU in current test batch is 0.8980112545178174
Epoch 37799: reducing learning rate of group 0 to 6.9546e-04.
training: bce: 0.020453, dice: 0.068269, loss: 0.044361
training IoU in current batch 5300 is 0.9256855665072704
training IoU uptillnow 5300 is 0.0006328762463590062
testing: bce: 4.169999, dice: 13.918940, loss: 9.044470
IoU in current test batch is 0.889843000338818
Epoch 37900: reducing learning rate of group 0 to 6.9476e-04.
training: bce: 0.020376, dice: 0.068099, loss: 0.044237
training IoU in current batch 5400 is 0.9509286138994788
training IoU uptillnow 5400 is 0.0006437374351049406
testing: bce: 4.232714, dice: 14.146250, loss: 9.189482
IoU in current test batch is 0.8537309147241563
Epoch 38001: reducing learning rate of group 0 to 6.9407e-04.
training: bce: 0.020317, dice: 0.068027, loss: 0.044172
training IoU in current batch 5500 is 0.8765133171912833
training IoU uptillnow 5500 is 0.0006535636592362733
testing: bce: 4.298651, dice: 14.392960, loss: 9.345805
IoU in current test batch is 0.8624428972468245
Epoch 38102: reducing learning rate of group 0 to 6.9338e-04.
training: bce: 0.020363, dice: 0.068060, loss: 0.044211
training IoU in current batch 5600 is 0.8980004300150505
training IoU uptillnow 5600 is 0.000663619986293873
testing: bce: 4.386693, dice: 14.661614, loss: 9.524154
IoU in current test batch is 0.8943867818799274
Epoch 38203: reducing learning rate of group 0 to 6.9268e-04.
training: bce: 0.020271, dice: 0.067860, loss: 0.044066
training IoU in current batch 5700 is 0.9354527580962273
training IoU uptillnow 5700 is 0.0006741133098776774
testing: bce: 4.444866, dice: 14.879569, loss: 9.662218
IoU in current test batch is 0.8962757910866586
Epoch 38304: reducing learning rate of group 0 to 6.9199e-04.
training: bce: 0.020183, dice: 0.067618, loss: 0.043901
training IoU in current batch 5800 is 0.93880751274101
training IoU uptillnow 5800 is 0.0006845956479885455
testing: bce: 4.503185, dice: 15.086648, loss: 9.794917
IoU in current test batch is 0.888965412467139
Epoch 38405: reducing learning rate of group 0 to 6.9130e-04.
training: bce: 0.020110, dice: 0.067454, loss: 0.043782
training IoU in current batch 5900 is 0.921422844230629
training IoU uptillnow 5900 is 0.0006947973894238427
testing: bce: 4.564169, dice: 15.309400, loss: 9.936784
IoU in current test batch is 0.9047654558119844
Epoch 38506: reducing learning rate of group 0 to 6.9061e-04.
training: bce: 0.020108, dice: 0.067574, loss: 0.043841
training IoU in current batch 6000 is 0.9027913914340507
training IoU uptillnow 6000 is 0.0007047045466314495
testing: bce: 4.641178, dice: 15.596559, loss: 10.118869
IoU in current test batch is 0.8640026837074951
Epoch 38607: reducing learning rate of group 0 to 6.8992e-04.
training: bce: 0.020051, dice: 0.067525, loss: 0.043788
training IoU in current batch 6100 is 0.8467733159212102
training IoU uptillnow 6100 is 0.0007138357437568319
testing: bce: 4.705070, dice: 15.844916, loss: 10.274993
IoU in current test batch is 0.8903485812484545
Epoch 38708: reducing learning rate of group 0 to 6.8923e-04.
training: bce: 0.020031, dice: 0.067479, loss: 0.043755
training IoU in current batch 6200 is 0.920425854233294
training IoU uptillnow 6200 is 0.0007238701791106031
testing: bce: 4.777433, dice: 16.093785, loss: 10.435609
IoU in current test batch is 0.9054038140629268
Epoch 38809: reducing learning rate of group 0 to 6.8854e-04.
training: bce: 0.020041, dice: 0.067392, loss: 0.043717
training IoU in current batch 6300 is 0.8777384377074574
training IoU uptillnow 6300 is 0.0007333035618038855
testing: bce: 4.856892, dice: 16.332231, loss: 10.594562
IoU in current test batch is 0.8859300186936273
Epoch 38910: reducing learning rate of group 0 to 6.8785e-04.
training: bce: 0.019990, dice: 0.067313, loss: 0.043651
training IoU in current batch 6400 is 0.9440747182938819
training IoU uptillnow 6400 is 0.00074354007326017
testing: bce: 4.921270, dice: 16.572054, loss: 10.746662
IoU in current test batch is 0.8987899037103066
Epoch 39011: reducing learning rate of group 0 to 6.8716e-04.
training: bce: 0.020076, dice: 0.067333, loss: 0.043704
training IoU in current batch 6500 is 0.9098877019994522
training IoU uptillnow 6500 is 0.0007532864168413607
testing: bce: 5.019797, dice: 16.835804, loss: 10.927800
IoU in current test batch is 0.8856432507233044
Epoch 39112: reducing learning rate of group 0 to 6.8647e-04.
training: bce: 0.020002, dice: 0.067147, loss: 0.043574
training IoU in current batch 6600 is 0.931382634205814
training IoU uptillnow 6600 is 0.0007632574937495912
testing: bce: 5.078079, dice: 17.047675, loss: 11.062877
IoU in current test batch is 0.8432838090385091
Epoch 39213: reducing learning rate of group 0 to 6.8579e-04.
training: bce: 0.020050, dice: 0.067105, loss: 0.043577
training IoU in current batch 6700 is 0.9294463922434316
training IoU uptillnow 6700 is 0.0007731530961584565
testing: bce: 5.167487, dice: 17.294955, loss: 11.231221
IoU in current test batch is 0.8768165136883819
Epoch 39314: reducing learning rate of group 0 to 6.8510e-04.
training: bce: 0.020002, dice: 0.067039, loss: 0.043521
training IoU in current batch 6800 is 0.9294117647058824
training IoU uptillnow 6800 is 0.0007829979626176769
testing: bce: 5.232037, dice: 17.535937, loss: 11.383987
IoU in current test batch is 0.8972474086468515
Epoch 39415: reducing learning rate of group 0 to 6.8442e-04.
training: bce: 0.019966, dice: 0.066950, loss: 0.043458
training IoU in current batch 6900 is 0.8800222438481857
training IoU uptillnow 6900 is 0.000792166933724087
testing: bce: 5.299552, dice: 17.770007, loss: 11.534779
IoU in current test batch is 0.8800614747319333
Epoch 39516: reducing learning rate of group 0 to 6.8373e-04.
training: bce: 0.019915, dice: 0.066942, loss: 0.043428
training IoU in current batch 7000 is 0.9433109504132231
training IoU uptillnow 7000 is 0.0008020896579515539
testing: bce: 5.362429, dice: 18.025320, loss: 11.693874
IoU in current test batch is 0.8861404446008118
Epoch 39617: reducing learning rate of group 0 to 6.8305e-04.
training: bce: 0.019893, dice: 0.066896, loss: 0.043395
training IoU in current batch 7100 is 0.87536844173708
training IoU uptillnow 7100 is 0.0008111055410036324
testing: bce: 5.433133, dice: 18.270377, loss: 11.851755
IoU in current test batch is 0.896181773487802
Epoch 39718: reducing learning rate of group 0 to 6.8236e-04.
training: bce: 0.019819, dice: 0.066747, loss: 0.043283
training IoU in current batch 7200 is 0.9238086484996094
training IoU uptillnow 7200 is 0.0008206853789927756
testing: bce: 5.489167, dice: 18.486451, loss: 11.987809
IoU in current test batch is 0.8821413563456566
Epoch 39819: reducing learning rate of group 0 to 6.8168e-04.
training: bce: 0.019817, dice: 0.066751, loss: 0.043284
training IoU in current batch 7300 is 0.9213400043506634
training IoU uptillnow 7300 is 0.0008301861622968586
testing: bce: 5.564640, dice: 18.744306, loss: 12.154473
IoU in current test batch is 0.8902834937438386
Epoch 39920: reducing learning rate of group 0 to 6.8100e-04.
training: bce: 0.019882, dice: 0.066798, loss: 0.043340
training IoU in current batch 7400 is 0.9384159807267617
training IoU uptillnow 7400 is 0.0008398531011605163
testing: bce: 5.659433, dice: 19.014278, loss: 12.336855
IoU in current test batch is 0.9000408125972451
Epoch 40021: reducing learning rate of group 0 to 6.8032e-04.
training: bce: 0.019878, dice: 0.066941, loss: 0.043410
training IoU in current batch 7500 is 0.9016195553767227
training IoU uptillnow 7500 is 0.0008490123782444327
testing: bce: 5.734891, dice: 19.312549, loss: 12.523720
IoU in current test batch is 0.8690476992850023
Epoch 40122: reducing learning rate of group 0 to 6.7964e-04.
training: bce: 0.019842, dice: 0.066887, loss: 0.043364
training IoU in current batch 7600 is 0.8863995111767402
training IoU uptillnow 7600 is 0.0008579364873308205
testing: bce: 5.800713, dice: 19.554087, loss: 12.677400
IoU in current test batch is 0.8763318067555699
Epoch 40223: reducing learning rate of group 0 to 6.7896e-04.
training: bce: 0.019779, dice: 0.066750, loss: 0.043264
training IoU in current batch 7700 is 0.9481308040320661
training IoU uptillnow 7700 is 0.0008675831103518007
testing: bce: 5.858448, dice: 19.770708, loss: 12.814578
IoU in current test batch is 0.9107863416064034
Epoch 40324: reducing learning rate of group 0 to 6.7828e-04.
training: bce: 0.019738, dice: 0.066674, loss: 0.043206
training IoU in current batch 7800 is 0.9007831516562599
training IoU uptillnow 7800 is 0.0008765951988483855
testing: bce: 5.922228, dice: 20.004773, loss: 12.963501
IoU in current test batch is 0.8901378114928167
Epoch 40425: reducing learning rate of group 0 to 6.7760e-04.
training: bce: 0.019711, dice: 0.066611, loss: 0.043161
training IoU in current batch 7900 is 0.7931264085228437
training IoU uptillnow 7900 is 0.0008842319727115132
testing: bce: 5.989844, dice: 20.241980, loss: 13.115912
IoU in current test batch is 0.8840297194872002
Epoch 40526: reducing learning rate of group 0 to 6.7692e-04.
training: bce: 0.019681, dice: 0.066607, loss: 0.043144
training IoU in current batch 8000 is 0.8960781602838841
training IoU uptillnow 8000 is 0.000893100537869426
testing: bce: 6.056528, dice: 20.497141, loss: 13.276835
IoU in current test batch is 0.8849850570877281
Epoch 40627: reducing learning rate of group 0 to 6.7625e-04.
training: bce: 0.020326, dice: 0.066862, loss: 0.043594
training IoU in current batch 8100 is 0.906079766536965
training IoU uptillnow 8100 is 0.0009020484912140315
testing: bce: 6.333010, dice: 20.832603, loss: 13.582806
IoU in current test batch is 0.8906667474842175
Epoch 40728: reducing learning rate of group 0 to 6.7557e-04.
training: bce: 0.020316, dice: 0.066867, loss: 0.043591
training IoU in current batch 8200 is 0.9229046867843875
training IoU uptillnow 8200 is 0.0009111589709566245
testing: bce: 6.408043, dice: 21.091276, loss: 13.749660
IoU in current test batch is 0.871780376793075
Epoch 40829: reducing learning rate of group 0 to 6.7490e-04.
training: bce: 0.020232, dice: 0.066675, loss: 0.043453
training IoU in current batch 8300 is 0.9182370137171126
training IoU uptillnow 8300 is 0.0009201677130406866
testing: bce: 6.459433, dice: 21.287172, loss: 13.873302
IoU in current test batch is 0.8880233048292274
Epoch 40930: reducing learning rate of group 0 to 6.7422e-04.
training: bce: 0.020401, dice: 0.066717, loss: 0.043559
training IoU in current batch 8400 is 0.9510248787357221
training IoU uptillnow 8400 is 0.0009295328009676159
testing: bce: 6.591915, dice: 21.557366, loss: 14.074640
IoU in current test batch is 0.8912491731797381
Epoch 41031: reducing learning rate of group 0 to 6.7355e-04.
training: bce: 0.020401, dice: 0.066654, loss: 0.043528
training IoU in current batch 8500 is 0.9403279984192847
training IoU uptillnow 8500 is 0.0009387219681709407
testing: bce: 6.670326, dice: 21.793362, loss: 14.231844
IoU in current test batch is 0.9068190431817489
Epoch 41132: reducing learning rate of group 0 to 6.7287e-04.
training: bce: 0.020322, dice: 0.066489, loss: 0.043406
training IoU in current batch 8600 is 0.8829588014981273
training IoU uptillnow 8600 is 0.000947169390470895
testing: bce: 6.722687, dice: 21.995071, loss: 14.358879
IoU in current test batch is 0.8797570790069791
Epoch 41233: reducing learning rate of group 0 to 6.7220e-04.
training: bce: 0.020270, dice: 0.066417, loss: 0.043344
training IoU in current batch 8700 is 0.9225948122054989
training IoU uptillnow 8700 is 0.0009560562973923283
testing: bce: 6.783402, dice: 22.226794, loss: 14.505098
IoU in current test batch is 0.8861203970663198
Epoch 41334: reducing learning rate of group 0 to 6.7153e-04.
training: bce: 0.020228, dice: 0.066331, loss: 0.043280
training IoU in current batch 8800 is 0.9651822154801892
training IoU uptillnow 8800 is 0.0009654151887453279
testing: bce: 6.847248, dice: 22.452984, loss: 14.650116
IoU in current test batch is 0.8364078562041588
Epoch 41435: reducing learning rate of group 0 to 6.7086e-04.
training: bce: 0.020192, dice: 0.066248, loss: 0.043220
training IoU in current batch 8900 is 0.9154929577464789
training IoU uptillnow 8900 is 0.0009741295269158416
testing: bce: 6.912645, dice: 22.679907, loss: 14.796276
IoU in current test batch is 0.8811413773566256
Epoch 41536: reducing learning rate of group 0 to 6.7019e-04.
training: bce: 0.020152, dice: 0.066222, loss: 0.043187
training IoU in current batch 9000 is 0.889632224168126
training IoU uptillnow 9000 is 0.0009824907144004677
testing: bce: 6.976424, dice: 22.925512, loss: 14.950968
IoU in current test batch is 0.8804791653385075
Epoch 41637: reducing learning rate of group 0 to 6.6952e-04.
training: bce: 0.020089, dice: 0.066106, loss: 0.043098
training IoU in current batch 9100 is 0.9316126601356444
training IoU uptillnow 9100 is 0.0009913157245117002
testing: bce: 7.032058, dice: 23.139704, loss: 15.085881
IoU in current test batch is 0.8908164275852701
Epoch 41738: reducing learning rate of group 0 to 6.6885e-04.
training: bce: 0.020043, dice: 0.066028, loss: 0.043036
training IoU in current batch 9200 is 0.9186273235408813
training IoU uptillnow 9200 is 0.0009999429437435297
testing: bce: 7.093030, dice: 23.366287, loss: 15.229658
IoU in current test batch is 0.8918253424819835
Epoch 41839: reducing learning rate of group 0 to 6.6818e-04.
training: bce: 0.020064, dice: 0.066049, loss: 0.043056
training IoU in current batch 9300 is 0.761576164065523
training IoU uptillnow 9300 is 0.00100665255288243
testing: bce: 7.177535, dice: 23.627709, loss: 15.402622
IoU in current test batch is 0.8737978751193574
Epoch 41940: reducing learning rate of group 0 to 6.6751e-04.
training: bce: 0.020023, dice: 0.065955, loss: 0.042989
training IoU in current batch 9400 is 0.9440851472707786
training IoU uptillnow 9400 is 0.0010155055146184972
testing: bce: 7.239756, dice: 23.847901, loss: 15.543828
IoU in current test batch is 0.8820346933197947
Epoch 42041: reducing learning rate of group 0 to 6.6684e-04.
training: bce: 0.019960, dice: 0.065839, loss: 0.042900
training IoU in current batch 9500 is 0.8724777352321129
training IoU uptillnow 9500 is 0.001023464903342601
testing: bce: 7.293875, dice: 24.059098, loss: 15.676486
IoU in current test batch is 0.8961740700004858
Epoch 42142: reducing learning rate of group 0 to 6.6617e-04.
training: bce: 0.019900, dice: 0.065718, loss: 0.042809
training IoU in current batch 9600 is 0.8868326165341675
training IoU uptillnow 9600 is 0.0010315568099591166
testing: bce: 7.348427, dice: 24.267558, loss: 15.807993
IoU in current test batch is 0.9127774752081887
Epoch 42243: reducing learning rate of group 0 to 6.6551e-04.
training: bce: 0.019833, dice: 0.065574, loss: 0.042703
training IoU in current batch 9700 is 0.923671497584541
training IoU uptillnow 9700 is 0.0010400463794876637
testing: bce: 7.399970, dice: 24.466664, loss: 15.933317
IoU in current test batch is 0.8880968089369498
Epoch 42344: reducing learning rate of group 0 to 6.6484e-04.
training: bce: 0.019793, dice: 0.065532, loss: 0.042663
training IoU in current batch 9800 is 0.9196648804967253
training IoU uptillnow 9800 is 0.0010484485519406939
testing: bce: 7.461236, dice: 24.703048, loss: 16.082142
IoU in current test batch is 0.8899414532969254
Epoch 42445: reducing learning rate of group 0 to 6.6418e-04.
training: bce: 0.019746, dice: 0.065429, loss: 0.042587
training IoU in current batch 9900 is 0.9242754087164383
training IoU uptillnow 9900 is 0.0010568654437571852
testing: bce: 7.519501, dice: 24.915744, loss: 16.217622
IoU in current test batch is 0.867232407369201
Epoch 42546: reducing learning rate of group 0 to 6.6351e-04.
training: bce: 0.019742, dice: 0.065411, loss: 0.042576
training IoU in current batch 10000 is 0.96405809215338
training IoU uptillnow 10000 is 0.001065710259834953
testing: bce: 7.593761, dice: 25.160598, loss: 16.377179
IoU in current test batch is 0.8787287250759065
Epoch 42647: reducing learning rate of group 0 to 6.6285e-04.
training: bce: 0.019770, dice: 0.065527, loss: 0.042648
training IoU in current batch 10100 is 0.8876860622462788
training IoU uptillnow 10100 is 0.001073618254187516
testing: bce: 7.680712, dice: 25.457151, loss: 16.568931
IoU in current test batch is 0.8876017129580489
Epoch 42748: reducing learning rate of group 0 to 6.6219e-04.
training: bce: 0.019769, dice: 0.065506, loss: 0.042637
training IoU in current batch 10200 is 0.8927861771058315
training IoU uptillnow 10200 is 0.0010815489028064276
testing: bce: 7.756133, dice: 25.701027, loss: 16.728580
IoU in current test batch is 0.8789563526937747
Epoch 42849: reducing learning rate of group 0 to 6.6153e-04.
training: bce: 0.019733, dice: 0.065457, loss: 0.042595
training IoU in current batch 10300 is 0.9504091451128747
training IoU uptillnow 10300 is 0.0010901149229997974
testing: bce: 7.817946, dice: 25.933610, loss: 16.875778
IoU in current test batch is 0.9050127251648822
training: bce: 0.019704, dice: 0.065399, loss: 0.042552
training IoU in current batch 10400 is 0.9470682368388659
training IoU uptillnow 10400 is 0.001098602160944813
testing: bce: 7.882472, dice: 26.162049, loss: 17.022260
IoU in current test batch is 0.9003692857955925
Epoch 42950: reducing learning rate of group 0 to 6.6086e-04.
training: bce: 0.019672, dice: 0.065331, loss: 0.042501
training IoU in current batch 10500 is 0.9285560267375835
training IoU uptillnow 10500 is 0.0011068349580103842
testing: bce: 7.945363, dice: 26.385988, loss: 17.165675
IoU in current test batch is 0.9121107520940586
Epoch 43051: reducing learning rate of group 0 to 6.6020e-04.
training: bce: 0.019615, dice: 0.065240, loss: 0.042428
training IoU in current batch 10600 is 0.9242568597560976
training IoU uptillnow 10600 is 0.0011149797785547013
testing: bce: 7.997783, dice: 26.600425, loss: 17.299104
IoU in current test batch is 0.8905153248926462
Epoch 43152: reducing learning rate of group 0 to 6.5954e-04.
training: bce: 0.019565, dice: 0.065128, loss: 0.042347
training IoU in current batch 10700 is 0.937902224229423
training IoU uptillnow 10700 is 0.0011232446864555842
testing: bce: 8.052465, dice: 26.805317, loss: 17.428891
IoU in current test batch is 0.898684355633126
Epoch 43253: reducing learning rate of group 0 to 6.5888e-04.
training: bce: 0.019529, dice: 0.065058, loss: 0.042294
training IoU in current batch 10800 is 0.9308098923919199
training IoU uptillnow 10800 is 0.0011313896587747667
testing: bce: 8.112966, dice: 27.026523, loss: 17.569744
IoU in current test batch is 0.9011203361548833
Epoch 43354: reducing learning rate of group 0 to 6.5822e-04.
training: bce: 0.019501, dice: 0.065004, loss: 0.042252
training IoU in current batch 10900 is 0.9340842882113032
training IoU uptillnow 10900 is 0.001139534819898098
testing: bce: 8.175977, dice: 27.254293, loss: 17.715135
IoU in current test batch is 0.8834654048860828
Epoch 43455: reducing learning rate of group 0 to 6.5757e-04.
training: bce: 0.019492, dice: 0.065028, loss: 0.042260
training IoU in current batch 11000 is 0.8549509134646642
training IoU uptillnow 11000 is 0.0011467340294123868
testing: bce: 8.247387, dice: 27.514209, loss: 17.880798
IoU in current test batch is 0.8467279963460673
Epoch 43556: reducing learning rate of group 0 to 6.5691e-04.
training: bce: 0.019541, dice: 0.065092, loss: 0.042316
training IoU in current batch 11100 is 0.9110200610722431
training IoU uptillnow 11100 is 0.0011545425181143166
testing: bce: 8.343063, dice: 27.791850, loss: 18.067456
IoU in current test batch is 0.8961892822883121
Epoch 43657: reducing learning rate of group 0 to 6.5625e-04.
training: bce: 0.019525, dice: 0.065063, loss: 0.042294
training IoU in current batch 11200 is 0.8553569528673263
training IoU uptillnow 11200 is 0.0011616791533815135
testing: bce: 8.411545, dice: 28.029503, loss: 18.220524
IoU in current test batch is 0.8677789448887311
Epoch 43758: reducing learning rate of group 0 to 6.5560e-04.
training: bce: 0.019476, dice: 0.064954, loss: 0.042215
training IoU in current batch 11300 is 0.9392751790981879
training IoU uptillnow 11300 is 0.0011697401272628791
testing: bce: 8.465311, dice: 28.232606, loss: 18.348958
IoU in current test batch is 0.8753963352272947
Epoch 43859: reducing learning rate of group 0 to 6.5494e-04.
training: bce: 0.019475, dice: 0.064948, loss: 0.042212
training IoU in current batch 11400 is 0.8465721819380356
training IoU uptillnow 11400 is 0.001176709764647667
testing: bce: 8.539904, dice: 28.479634, loss: 18.509769
IoU in current test batch is 0.9013524671509389
Epoch 43960: reducing learning rate of group 0 to 6.5429e-04.
training: bce: 0.019427, dice: 0.064878, loss: 0.042153
training IoU in current batch 11500 is 0.9243829296424452
training IoU uptillnow 11500 is 0.0011845309768829127
testing: bce: 8.593532, dice: 28.698696, loss: 18.646114
IoU in current test batch is 0.8981675329021436
Epoch 44061: reducing learning rate of group 0 to 6.5363e-04.
training: bce: 0.019395, dice: 0.064798, loss: 0.042096
training IoU in current batch 11600 is 0.9177998284944261
training IoU uptillnow 11600 is 0.0011922422038856855
testing: bce: 8.653883, dice: 28.912299, loss: 18.783091
IoU in current test batch is 0.8638085467049083
Epoch 44162: reducing learning rate of group 0 to 6.5298e-04.
training: bce: 0.019355, dice: 0.064715, loss: 0.042035
training IoU in current batch 11700 is 0.944498618560052
training IoU uptillnow 11700 is 0.0012002202621437777
testing: bce: 8.710288, dice: 29.124198, loss: 18.917243
IoU in current test batch is 0.866834170454996
Epoch 44263: reducing learning rate of group 0 to 6.5232e-04.
training: bce: 0.019304, dice: 0.064627, loss: 0.041966
training IoU in current batch 11800 is 0.9118350609408816
training IoU uptillnow 11800 is 0.0012077940905805373
testing: bce: 8.761817, dice: 29.333331, loss: 19.047574
IoU in current test batch is 0.8948764144330463
Epoch 44364: reducing learning rate of group 0 to 6.5167e-04.
training: bce: 0.019268, dice: 0.064554, loss: 0.041911
training IoU in current batch 11900 is 0.9202005352632968
training IoU uptillnow 11900 is 0.0012154279415478957
testing: bce: 8.819713, dice: 29.548521, loss: 19.184117
IoU in current test batch is 0.8840270812482575
Epoch 44465: reducing learning rate of group 0 to 6.5102e-04.
training: bce: 0.019237, dice: 0.064491, loss: 0.041864
training IoU in current batch 12000 is 0.9438454273571542
training IoU uptillnow 12000 is 0.0012232928989441354
testing: bce: 8.879237, dice: 29.767682, loss: 19.323460
IoU in current test batch is 0.8876120520984413
Epoch 44566: reducing learning rate of group 0 to 6.5037e-04.
training: bce: 0.019242, dice: 0.064509, loss: 0.041876
training IoU in current batch 12100 is 0.5876831356630938
training IoU uptillnow 12100 is 0.0012271342023839754
testing: bce: 8.955888, dice: 30.024192, loss: 19.490040
IoU in current test batch is 0.8922850899722964
Epoch 44667: reducing learning rate of group 0 to 6.4972e-04.
training: bce: 0.019219, dice: 0.064466, loss: 0.041843
training IoU in current batch 12200 is 0.9165314498425122
training IoU uptillnow 12200 is 0.001234632661689283
testing: bce: 9.018965, dice: 30.252028, loss: 19.635497
IoU in current test batch is 0.9123641330803968
Epoch 44768: reducing learning rate of group 0 to 6.4907e-04.
training: bce: 0.019194, dice: 0.064415, loss: 0.041805
training IoU in current batch 12300 is 0.9429738373213116
training IoU uptillnow 12300 is 0.0012423924728910073
testing: bce: 9.080941, dice: 30.475814, loss: 19.778377
IoU in current test batch is 0.8451721329476689
Epoch 44869: reducing learning rate of group 0 to 6.4842e-04.
training: bce: 0.019148, dice: 0.064326, loss: 0.041737
training IoU in current batch 12400 is 0.9089720115092859
training IoU uptillnow 12400 is 0.0012497395347819192
testing: bce: 9.133079, dice: 30.681166, loss: 19.907123
IoU in current test batch is 0.9060244472401175
Epoch 44970: reducing learning rate of group 0 to 6.4777e-04.
training: bce: 0.019112, dice: 0.064285, loss: 0.041699
training IoU in current batch 12500 is 0.8992805755395683
training IoU uptillnow 12500 is 0.0012569464146427742
testing: bce: 9.189293, dice: 30.908687, loss: 20.048990
IoU in current test batch is 0.9011895051153823
Epoch 45071: reducing learning rate of group 0 to 6.4712e-04.
training: bce: 0.019098, dice: 0.064245, loss: 0.041672
training IoU in current batch 12600 is 0.8555555555555555
training IoU uptillnow 12600 is 0.0012636371451339979
testing: bce: 9.256082, dice: 31.136609, loss: 20.196345
IoU in current test batch is 0.8952993051347918
Epoch 45172: reducing learning rate of group 0 to 6.4648e-04.
training: bce: 0.019088, dice: 0.064255, loss: 0.041672
training IoU in current batch 12700 is 0.9471355894713559
training IoU uptillnow 12700 is 0.0012713102482671214
testing: bce: 9.324726, dice: 31.388443, loss: 20.356584
IoU in current test batch is 0.8870336192182713
Epoch 45273: reducing learning rate of group 0 to 6.4583e-04.
training: bce: 0.019154, dice: 0.064260, loss: 0.041707
training IoU in current batch 12800 is 0.7520729116699788
training IoU uptillnow 12800 is 0.0012767988519123278
testing: bce: 9.430480, dice: 31.638171, loss: 20.534326
IoU in current test batch is 0.8798661758632513
Epoch 45374: reducing learning rate of group 0 to 6.4518e-04.
training: bce: 0.019124, dice: 0.064204, loss: 0.041664
training IoU in current batch 12900 is 0.8582758876287896
training IoU uptillnow 12900 is 0.0012834316654443394
testing: bce: 9.489014, dice: 31.857762, loss: 20.673388
IoU in current test batch is 0.8905429320451062
Epoch 45475: reducing learning rate of group 0 to 6.4454e-04.
training: bce: 0.019124, dice: 0.064197, loss: 0.041660
training IoU in current batch 13000 is 0.9138937655656718
training IoU uptillnow 13000 is 0.0012906458767142413
testing: bce: 9.562836, dice: 32.100735, loss: 20.831786
IoU in current test batch is 0.8845470768453116
Epoch 45576: reducing learning rate of group 0 to 6.4389e-04.
training: bce: 0.019082, dice: 0.064104, loss: 0.041593
training IoU in current batch 13100 is 0.919345473739773
training IoU uptillnow 13100 is 0.0012978881936990597
testing: bce: 9.615068, dice: 32.301075, loss: 20.958072
IoU in current test batch is 0.8772702832780882
Epoch 45677: reducing learning rate of group 0 to 6.4325e-04.
training: bce: 0.019087, dice: 0.064106, loss: 0.041596
training IoU in current batch 13200 is 0.7372624621316441
training IoU uptillnow 13200 is 0.001303108849918164
testing: bce: 9.691006, dice: 32.548357, loss: 21.119681
IoU in current test batch is 0.9188665281334291
Epoch 45778: reducing learning rate of group 0 to 6.4261e-04.
training: bce: 0.019095, dice: 0.064082, loss: 0.041589
training IoU in current batch 13300 is 0.9372037417676916
training IoU uptillnow 13300 is 0.0013104871416310948
testing: bce: 9.768731, dice: 32.782711, loss: 21.275721
IoU in current test batch is 0.9019375284155078
Epoch 45879: reducing learning rate of group 0 to 6.4196e-04.
training: bce: 0.019076, dice: 0.064056, loss: 0.041566
training IoU in current batch 13400 is 0.7866506360533664
training IoU uptillnow 13400 is 0.0013161950732487092
testing: bce: 9.832205, dice: 33.016137, loss: 21.424171
IoU in current test batch is 0.8850265370615923
Epoch 45980: reducing learning rate of group 0 to 6.4132e-04.
training: bce: 0.019121, dice: 0.064174, loss: 0.041648
training IoU in current batch 13500 is 0.8117658070856929
training IoU uptillnow 13500 is 0.0013221509119921914
testing: bce: 9.929136, dice: 33.323626, loss: 21.626381
IoU in current test batch is 0.8848122266275585
Epoch 46081: reducing learning rate of group 0 to 6.4068e-04.
training: bce: 0.019131, dice: 0.064159, loss: 0.041645
training IoU in current batch 13600 is 0.9104029516024018
training IoU uptillnow 13600 is 0.0013291496093692373
testing: bce: 10.007878, dice: 33.562400, loss: 21.785139
IoU in current test batch is 0.8690544139962457
Epoch 46182: reducing learning rate of group 0 to 6.4004e-04.
training: bce: 0.019183, dice: 0.064297, loss: 0.041740
training IoU in current batch 13700 is 0.9383365689179524
training IoU uptillnow 13700 is 0.0013364200300985869
testing: bce: 10.108580, dice: 33.882235, loss: 21.995408
IoU in current test batch is 0.8823181070381885
Epoch 46283: reducing learning rate of group 0 to 6.3940e-04.
training: bce: 0.019146, dice: 0.064244, loss: 0.041695
training IoU in current batch 13800 is 0.9289867991001316
training IoU uptillnow 13800 is 0.0013435582170593999
testing: bce: 10.163082, dice: 34.101156, loss: 22.132119
IoU in current test batch is 0.8901032117499326
Epoch 46384: reducing learning rate of group 0 to 6.3876e-04.
training: bce: 0.019118, dice: 0.064191, loss: 0.041654
training IoU in current batch 13900 is 0.9341048759700354
training IoU uptillnow 13900 is 0.0013507207616783748
testing: bce: 10.221482, dice: 34.319835, loss: 22.270658
IoU in current test batch is 0.8922107238364293
Epoch 46485: reducing learning rate of group 0 to 6.3812e-04.
training: bce: 0.019085, dice: 0.064136, loss: 0.041611
training IoU in current batch 14000 is 0.9252174665685723
training IoU uptillnow 14000 is 0.001357757070491927
testing: bce: 10.277518, dice: 34.537248, loss: 22.407383
IoU in current test batch is 0.8906372754931892
Epoch 46586: reducing learning rate of group 0 to 6.3748e-04.
training: bce: 0.019062, dice: 0.064094, loss: 0.041578
training IoU in current batch 14100 is 0.9490205926670016
training IoU uptillnow 14100 is 0.0013650183399435676
testing: bce: 10.338140, dice: 34.761120, loss: 22.549630
IoU in current test batch is 0.8908108721787393
Epoch 46687: reducing learning rate of group 0 to 6.3685e-04.
training: bce: 0.019037, dice: 0.064058, loss: 0.041548
training IoU in current batch 14200 is 0.8984701521961991
training IoU uptillnow 14200 is 0.0013717078926041041
testing: bce: 10.398119, dice: 34.988094, loss: 22.693106
IoU in current test batch is 0.8952862300287787
Epoch 46788: reducing learning rate of group 0 to 6.3621e-04.
training: bce: 0.019004, dice: 0.064003, loss: 0.041503
training IoU in current batch 14300 is 0.9445355970894564
training IoU uptillnow 14300 is 0.0013788605198313813
testing: bce: 10.452759, dice: 35.204141, loss: 22.828450
IoU in current test batch is 0.8842799432747339
Epoch 46889: reducing learning rate of group 0 to 6.3557e-04.
training: bce: 0.018984, dice: 0.063980, loss: 0.041482
training IoU in current batch 14400 is 0.9391919466739219
training IoU uptillnow 14400 is 0.0013859257691173977
testing: bce: 10.514785, dice: 35.437629, loss: 22.976207
IoU in current test batch is 0.8909353977494401
Epoch 46990: reducing learning rate of group 0 to 6.3494e-04.
training: bce: 0.018970, dice: 0.063981, loss: 0.041475
training IoU in current batch 14500 is 0.9010670658104389
training IoU uptillnow 14500 is 0.001392555828012678
testing: bce: 10.580271, dice: 35.683981, loss: 23.132126
IoU in current test batch is 0.8822690914532436
Epoch 47091: reducing learning rate of group 0 to 6.3430e-04.
training: bce: 0.018976, dice: 0.063979, loss: 0.041478
training IoU in current batch 14600 is 0.9194739504299444
training IoU uptillnow 14600 is 0.0013993529603770444
testing: bce: 10.656624, dice: 35.929227, loss: 23.292925
IoU in current test batch is 0.8913050377773585
Epoch 47192: reducing learning rate of group 0 to 6.3367e-04.
training: bce: 0.018947, dice: 0.063925, loss: 0.041436
training IoU in current batch 14700 is 0.9488408955815336
training IoU uptillnow 14700 is 0.0014064320861191806
testing: bce: 10.712859, dice: 36.144736, loss: 23.428797
IoU in current test batch is 0.8967882827951481
Epoch 47293: reducing learning rate of group 0 to 6.3304e-04.
training: bce: 0.018942, dice: 0.063947, loss: 0.041445
training IoU in current batch 14800 is 0.9313916928677809
training IoU uptillnow 14800 is 0.001413297050645141
testing: bce: 10.783127, dice: 36.403202, loss: 23.593165
IoU in current test batch is 0.880941796499034
Epoch 47394: reducing learning rate of group 0 to 6.3240e-04.
training: bce: 0.018932, dice: 0.063932, loss: 0.041432
training IoU in current batch 14900 is 0.9207357416964403
training IoU uptillnow 14900 is 0.0014200207920077202
testing: bce: 10.850023, dice: 36.640165, loss: 23.745094
IoU in current test batch is 0.8839020400352301
Epoch 47495: reducing learning rate of group 0 to 6.3177e-04.
training: bce: 0.018947, dice: 0.063946, loss: 0.041447
training IoU in current batch 15000 is 0.8532888465204957
training IoU uptillnow 15000 is 0.0014260070241249763
testing: bce: 10.931721, dice: 36.894443, loss: 23.913082
IoU in current test batch is 0.8754116975881902
Epoch 47596: reducing learning rate of group 0 to 6.3114e-04.
training: bce: 0.018925, dice: 0.063904, loss: 0.041414
training IoU in current batch 15100 is 0.9197587454764777
training IoU uptillnow 15100 is 0.0014326656180310142
testing: bce: 10.991722, dice: 37.115784, loss: 24.053753
IoU in current test batch is 0.8900257620996181
Epoch 47697: reducing learning rate of group 0 to 6.3051e-04.
training: bce: 0.018891, dice: 0.063851, loss: 0.041371
training IoU in current batch 15200 is 0.9546502690238279
training IoU uptillnow 15200 is 0.0014396616823397256
testing: bce: 11.044507, dice: 37.330843, loss: 24.187675
IoU in current test batch is 0.8843470813031716
Epoch 47798: reducing learning rate of group 0 to 6.2988e-04.
training: bce: 0.018997, dice: 0.063858, loss: 0.041427
training IoU in current batch 15300 is 0.905739347254998
training IoU uptillnow 15300 is 0.0014461174134423186
testing: bce: 11.179843, dice: 37.580193, loss: 24.380018
IoU in current test batch is 0.8853766155889904
Epoch 47899: reducing learning rate of group 0 to 6.2925e-04.
training: bce: 0.018962, dice: 0.063793, loss: 0.041378
training IoU in current batch 15400 is 0.9470287288469107
training IoU uptillnow 15400 is 0.001452976768035781
testing: bce: 11.231925, dice: 37.787819, loss: 24.509872
IoU in current test batch is 0.9133838556730202
Epoch 48000: reducing learning rate of group 0 to 6.2862e-04.
training: bce: 0.018984, dice: 0.063831, loss: 0.041407
training IoU in current batch 15500 is 0.9171921160544998
training IoU uptillnow 15500 is 0.0014594970935589118
testing: bce: 11.318168, dice: 38.055403, loss: 24.686786
IoU in current test batch is 0.9070835294575352
Epoch 48101: reducing learning rate of group 0 to 6.2799e-04.
training: bce: 0.018990, dice: 0.063831, loss: 0.041411
training IoU in current batch 15600 is 0.9123830356057844
training IoU uptillnow 15600 is 0.0014659403963646939
testing: bce: 11.394494, dice: 38.301332, loss: 24.847913
IoU in current test batch is 0.9111415883090802
Epoch 48202: reducing learning rate of group 0 to 6.2736e-04.
training: bce: 0.018972, dice: 0.063816, loss: 0.041394
training IoU in current batch 15700 is 0.9298157606712879
training IoU uptillnow 15700 is 0.0014725376427755204
testing: bce: 11.456899, dice: 38.537781, loss: 24.997340
IoU in current test batch is 0.875273795250114
Epoch 48303: reducing learning rate of group 0 to 6.2673e-04.
training: bce: 0.019091, dice: 0.063911, loss: 0.041501
training IoU in current batch 15800 is 0.8372745066920595
training IoU uptillnow 15800 is 0.0014781505961477058
testing: bce: 11.602064, dice: 38.840848, loss: 25.221456
IoU in current test batch is 0.8987500467851987
Epoch 48404: reducing learning rate of group 0 to 6.2611e-04.
training: bce: 0.019067, dice: 0.063902, loss: 0.041485
training IoU in current batch 15900 is 0.937475709288768
training IoU uptillnow 15900 is 0.0014847744580044767
testing: bce: 11.660886, dice: 39.081156, loss: 25.371021
IoU in current test batch is 0.8819448471959164
Epoch 48505: reducing learning rate of group 0 to 6.2548e-04.
training: bce: 0.019077, dice: 0.063898, loss: 0.041487
training IoU in current batch 16000 is 0.9110341924318256
training IoU uptillnow 16000 is 0.001491098717789139
testing: bce: 11.740171, dice: 39.324198, loss: 25.532185
IoU in current test batch is 0.8999538221011801
Epoch 48606: reducing learning rate of group 0 to 6.2486e-04.
training: bce: 0.019049, dice: 0.063851, loss: 0.041450
training IoU in current batch 16100 is 0.8722491527604993
training IoU uptillnow 16100 is 0.0014969983612510727
testing: bce: 11.796302, dice: 39.540883, loss: 25.668592
IoU in current test batch is 0.8861214862707423
Epoch 48707: reducing learning rate of group 0 to 6.2423e-04.
training: bce: 0.019027, dice: 0.063813, loss: 0.041420
training IoU in current batch 16200 is 0.9452488329772925
training IoU uptillnow 16200 is 0.0015036225231473696
testing: bce: 11.856322, dice: 39.762763, loss: 25.809543
IoU in current test batch is 0.8976879345694793
Epoch 48808: reducing learning rate of group 0 to 6.2361e-04.
training: bce: 0.019035, dice: 0.063848, loss: 0.041441
training IoU in current batch 16300 is 0.8995761045317484
training IoU uptillnow 16300 is 0.0015097520802554493
testing: bce: 11.934197, dice: 40.030140, loss: 25.982169
IoU in current test batch is 0.9087301996904931
Epoch 48909: reducing learning rate of group 0 to 6.2298e-04.
training: bce: 0.019002, dice: 0.063774, loss: 0.041388
training IoU in current batch 16400 is 0.9224442988204457
training IoU uptillnow 16400 is 0.0015160901826136895
testing: bce: 11.986477, dice: 40.229166, loss: 26.107821
IoU in current test batch is 0.8924726947078937
Epoch 49010: reducing learning rate of group 0 to 6.2236e-04.
training: bce: 0.018992, dice: 0.063777, loss: 0.041385
training IoU in current batch 16500 is 0.9396284189183598
training IoU uptillnow 16500 is 0.0015225776124793925
testing: bce: 12.053476, dice: 40.476405, loss: 26.264941
IoU in current test batch is 0.8963468226490309
Epoch 49111: reducing learning rate of group 0 to 6.2174e-04.
training: bce: 0.019009, dice: 0.063764, loss: 0.041386
training IoU in current batch 16600 is 0.9405165114309907
training IoU uptillnow 16600 is 0.0015290476781864202
testing: bce: 12.137140, dice: 40.713045, loss: 26.425092
IoU in current test batch is 0.9058915160322981
Epoch 49212: reducing learning rate of group 0 to 6.2112e-04.
training: bce: 0.019050, dice: 0.063775, loss: 0.041413
training IoU in current batch 16700 is 0.9420884800058644
training IoU uptillnow 16700 is 0.001535507428482043
testing: bce: 12.237008, dice: 40.965934, loss: 26.601471
IoU in current test batch is 0.857015762364074
Epoch 49313: reducing learning rate of group 0 to 6.2049e-04.
training: bce: 0.019043, dice: 0.063807, loss: 0.041425
training IoU in current batch 16800 is 0.8700808151791989
training IoU uptillnow 16800 is 0.0015412114308476475
testing: bce: 12.305335, dice: 41.231728, loss: 26.768532
IoU in current test batch is 0.885667906316888
Epoch 49414: reducing learning rate of group 0 to 6.1987e-04.
training: bce: 0.019036, dice: 0.063797, loss: 0.041416
training IoU in current batch 16900 is 0.8715448739823527
training IoU uptillnow 16900 is 0.001546907166778373
testing: bce: 12.374208, dice: 41.470287, loss: 26.922247
IoU in current test batch is 0.8734623042950873
Epoch 49515: reducing learning rate of group 0 to 6.1925e-04.
training: bce: 0.019019, dice: 0.063786, loss: 0.041402
training IoU in current batch 17000 is 0.8280139552913813
training IoU uptillnow 17000 is 0.0015521406456422938
testing: bce: 12.435968, dice: 41.708845, loss: 27.072406
IoU in current test batch is 0.8918874219284026
Epoch 49616: reducing learning rate of group 0 to 6.1863e-04.
training: bce: 0.019014, dice: 0.063771, loss: 0.041392
training IoU in current batch 17100 is 0.9151018249140439
training IoU uptillnow 17100 is 0.0015582300694611197
testing: bce: 12.506053, dice: 41.944018, loss: 27.225035
IoU in current test batch is 0.8503046104114665
Epoch 49717: reducing learning rate of group 0 to 6.1802e-04.
training: bce: 0.019051, dice: 0.063806, loss: 0.041429
training IoU in current batch 17200 is 0.9052675080268303
training IoU uptillnow 17200 is 0.0015641961745891573
testing: bce: 12.603544, dice: 42.212899, loss: 27.408222
IoU in current test batch is 0.8443011094824324
Epoch 49818: reducing learning rate of group 0 to 6.1740e-04.
training: bce: 0.019036, dice: 0.063798, loss: 0.041417
training IoU in current batch 17300 is 0.930747606746695
training IoU uptillnow 17300 is 0.0015703939135015724
testing: bce: 12.666804, dice: 42.452333, loss: 27.559569
IoU in current test batch is 0.9181376894876553
Epoch 49919: reducing learning rate of group 0 to 6.1678e-04.
training: bce: 0.019045, dice: 0.063840, loss: 0.041443
training IoU in current batch 17400 is 0.7868245743893413
training IoU uptillnow 17400 is 0.0015751261509783139
testing: bce: 12.746522, dice: 42.725936, loss: 27.736229
IoU in current test batch is 0.8599327050390251
Maximum training samples requirement meet, I have been training for more than  100001  samples.
Making network now
ResNetUNet(
  (base_model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer0_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer1_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2_1x1): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3_1x1): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4_1x1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (upsample): Upsample(scale_factor=2.0, mode=bilinear)
  (conv_up3): Sequential(
    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up2): Sequential(
    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up1): Sequential(
    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up0): Sequential(
    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size2): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 512, 512]           1,792
              ReLU-2         [-1, 64, 512, 512]               0
            Conv2d-3         [-1, 64, 512, 512]          36,928
              ReLU-4         [-1, 64, 512, 512]               0
            Conv2d-5         [-1, 64, 256, 256]           9,408
            Conv2d-6         [-1, 64, 256, 256]           9,408
       BatchNorm2d-7         [-1, 64, 256, 256]             128
       BatchNorm2d-8         [-1, 64, 256, 256]             128
              ReLU-9         [-1, 64, 256, 256]               0
             ReLU-10         [-1, 64, 256, 256]               0
        MaxPool2d-11         [-1, 64, 128, 128]               0
        MaxPool2d-12         [-1, 64, 128, 128]               0
           Conv2d-13         [-1, 64, 128, 128]          36,864
           Conv2d-14         [-1, 64, 128, 128]          36,864
      BatchNorm2d-15         [-1, 64, 128, 128]             128
      BatchNorm2d-16         [-1, 64, 128, 128]             128
             ReLU-17         [-1, 64, 128, 128]               0
             ReLU-18         [-1, 64, 128, 128]               0
           Conv2d-19         [-1, 64, 128, 128]          36,864
           Conv2d-20         [-1, 64, 128, 128]          36,864
      BatchNorm2d-21         [-1, 64, 128, 128]             128
      BatchNorm2d-22         [-1, 64, 128, 128]             128
             ReLU-23         [-1, 64, 128, 128]               0
             ReLU-24         [-1, 64, 128, 128]               0
       BasicBlock-25         [-1, 64, 128, 128]               0
       BasicBlock-26         [-1, 64, 128, 128]               0
           Conv2d-27         [-1, 64, 128, 128]          36,864
           Conv2d-28         [-1, 64, 128, 128]          36,864
      BatchNorm2d-29         [-1, 64, 128, 128]             128
      BatchNorm2d-30         [-1, 64, 128, 128]             128
             ReLU-31         [-1, 64, 128, 128]               0
             ReLU-32         [-1, 64, 128, 128]               0
           Conv2d-33         [-1, 64, 128, 128]          36,864
           Conv2d-34         [-1, 64, 128, 128]          36,864
      BatchNorm2d-35         [-1, 64, 128, 128]             128
      BatchNorm2d-36         [-1, 64, 128, 128]             128
             ReLU-37         [-1, 64, 128, 128]               0
             ReLU-38         [-1, 64, 128, 128]               0
       BasicBlock-39         [-1, 64, 128, 128]               0
       BasicBlock-40         [-1, 64, 128, 128]               0
           Conv2d-41          [-1, 128, 64, 64]          73,728
           Conv2d-42          [-1, 128, 64, 64]          73,728
      BatchNorm2d-43          [-1, 128, 64, 64]             256
      BatchNorm2d-44          [-1, 128, 64, 64]             256
             ReLU-45          [-1, 128, 64, 64]               0
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]         147,456
           Conv2d-48          [-1, 128, 64, 64]         147,456
      BatchNorm2d-49          [-1, 128, 64, 64]             256
      BatchNorm2d-50          [-1, 128, 64, 64]             256
           Conv2d-51          [-1, 128, 64, 64]           8,192
           Conv2d-52          [-1, 128, 64, 64]           8,192
      BatchNorm2d-53          [-1, 128, 64, 64]             256
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
             ReLU-56          [-1, 128, 64, 64]               0
       BasicBlock-57          [-1, 128, 64, 64]               0
       BasicBlock-58          [-1, 128, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]         147,456
           Conv2d-60          [-1, 128, 64, 64]         147,456
      BatchNorm2d-61          [-1, 128, 64, 64]             256
      BatchNorm2d-62          [-1, 128, 64, 64]             256
             ReLU-63          [-1, 128, 64, 64]               0
             ReLU-64          [-1, 128, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]         147,456
           Conv2d-66          [-1, 128, 64, 64]         147,456
      BatchNorm2d-67          [-1, 128, 64, 64]             256
      BatchNorm2d-68          [-1, 128, 64, 64]             256
             ReLU-69          [-1, 128, 64, 64]               0
             ReLU-70          [-1, 128, 64, 64]               0
       BasicBlock-71          [-1, 128, 64, 64]               0
       BasicBlock-72          [-1, 128, 64, 64]               0
           Conv2d-73          [-1, 256, 32, 32]         294,912
           Conv2d-74          [-1, 256, 32, 32]         294,912
      BatchNorm2d-75          [-1, 256, 32, 32]             512
      BatchNorm2d-76          [-1, 256, 32, 32]             512
             ReLU-77          [-1, 256, 32, 32]               0
             ReLU-78          [-1, 256, 32, 32]               0
           Conv2d-79          [-1, 256, 32, 32]         589,824
           Conv2d-80          [-1, 256, 32, 32]         589,824
      BatchNorm2d-81          [-1, 256, 32, 32]             512
      BatchNorm2d-82          [-1, 256, 32, 32]             512
           Conv2d-83          [-1, 256, 32, 32]          32,768
           Conv2d-84          [-1, 256, 32, 32]          32,768
      BatchNorm2d-85          [-1, 256, 32, 32]             512
      BatchNorm2d-86          [-1, 256, 32, 32]             512
             ReLU-87          [-1, 256, 32, 32]               0
             ReLU-88          [-1, 256, 32, 32]               0
       BasicBlock-89          [-1, 256, 32, 32]               0
       BasicBlock-90          [-1, 256, 32, 32]               0
           Conv2d-91          [-1, 256, 32, 32]         589,824
           Conv2d-92          [-1, 256, 32, 32]         589,824
      BatchNorm2d-93          [-1, 256, 32, 32]             512
      BatchNorm2d-94          [-1, 256, 32, 32]             512
             ReLU-95          [-1, 256, 32, 32]               0
             ReLU-96          [-1, 256, 32, 32]               0
           Conv2d-97          [-1, 256, 32, 32]         589,824
           Conv2d-98          [-1, 256, 32, 32]         589,824
      BatchNorm2d-99          [-1, 256, 32, 32]             512
     BatchNorm2d-100          [-1, 256, 32, 32]             512
            ReLU-101          [-1, 256, 32, 32]               0
            ReLU-102          [-1, 256, 32, 32]               0
      BasicBlock-103          [-1, 256, 32, 32]               0
      BasicBlock-104          [-1, 256, 32, 32]               0
          Conv2d-105          [-1, 512, 16, 16]       1,179,648
          Conv2d-106          [-1, 512, 16, 16]       1,179,648
     BatchNorm2d-107          [-1, 512, 16, 16]           1,024
     BatchNorm2d-108          [-1, 512, 16, 16]           1,024
            ReLU-109          [-1, 512, 16, 16]               0
            ReLU-110          [-1, 512, 16, 16]               0
          Conv2d-111          [-1, 512, 16, 16]       2,359,296
          Conv2d-112          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-113          [-1, 512, 16, 16]           1,024
     BatchNorm2d-114          [-1, 512, 16, 16]           1,024
          Conv2d-115          [-1, 512, 16, 16]         131,072
          Conv2d-116          [-1, 512, 16, 16]         131,072
     BatchNorm2d-117          [-1, 512, 16, 16]           1,024
     BatchNorm2d-118          [-1, 512, 16, 16]           1,024
            ReLU-119          [-1, 512, 16, 16]               0
            ReLU-120          [-1, 512, 16, 16]               0
      BasicBlock-121          [-1, 512, 16, 16]               0
      BasicBlock-122          [-1, 512, 16, 16]               0
          Conv2d-123          [-1, 512, 16, 16]       2,359,296
          Conv2d-124          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-125          [-1, 512, 16, 16]           1,024
     BatchNorm2d-126          [-1, 512, 16, 16]           1,024
            ReLU-127          [-1, 512, 16, 16]               0
            ReLU-128          [-1, 512, 16, 16]               0
          Conv2d-129          [-1, 512, 16, 16]       2,359,296
          Conv2d-130          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-131          [-1, 512, 16, 16]           1,024
     BatchNorm2d-132          [-1, 512, 16, 16]           1,024
            ReLU-133          [-1, 512, 16, 16]               0
            ReLU-134          [-1, 512, 16, 16]               0
      BasicBlock-135          [-1, 512, 16, 16]               0
      BasicBlock-136          [-1, 512, 16, 16]               0
          Conv2d-137          [-1, 512, 16, 16]         262,656
            ReLU-138          [-1, 512, 16, 16]               0
        Upsample-139          [-1, 512, 32, 32]               0
          Conv2d-140          [-1, 256, 32, 32]          65,792
            ReLU-141          [-1, 256, 32, 32]               0
          Conv2d-142          [-1, 512, 32, 32]       3,539,456
            ReLU-143          [-1, 512, 32, 32]               0
        Upsample-144          [-1, 512, 64, 64]               0
          Conv2d-145          [-1, 128, 64, 64]          16,512
            ReLU-146          [-1, 128, 64, 64]               0
          Conv2d-147          [-1, 256, 64, 64]       1,474,816
            ReLU-148          [-1, 256, 64, 64]               0
        Upsample-149        [-1, 256, 128, 128]               0
          Conv2d-150         [-1, 64, 128, 128]           4,160
            ReLU-151         [-1, 64, 128, 128]               0
          Conv2d-152        [-1, 256, 128, 128]         737,536
            ReLU-153        [-1, 256, 128, 128]               0
        Upsample-154        [-1, 256, 256, 256]               0
          Conv2d-155         [-1, 64, 256, 256]           4,160
            ReLU-156         [-1, 64, 256, 256]               0
          Conv2d-157        [-1, 128, 256, 256]         368,768
            ReLU-158        [-1, 128, 256, 256]               0
        Upsample-159        [-1, 128, 512, 512]               0
          Conv2d-160         [-1, 64, 512, 512]         110,656
            ReLU-161         [-1, 64, 512, 512]               0
          Conv2d-162          [-1, 1, 512, 512]              65
================================================================
Total params: 28,976,321
Trainable params: 28,976,321
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 2172.00
Params size (MB): 110.54
Estimated Total Size (MB): 2285.54
----------------------------------------------------------------
Start training now...
training: bce: 1.298882, dice: 0.917993, loss: 1.298882
training IoU in current batch 0 is 0.04152837003116597
training IoU uptillnow 0 is 0.020764185015582984
testing: bce: 0.049957, dice: 0.035307, loss: 0.049957
IoU in current test batch is 0.0
training: bce: 0.322982, dice: 0.757634, loss: 0.322982
training IoU in current batch 100 is 0.5279131759329779
training IoU uptillnow 100 is 0.00281901755427794
testing: bce: 1.254662, dice: 2.943116, loss: 1.254662
IoU in current test batch is 0.09422019600087338
training: bce: 0.184063, dice: 0.528148, loss: 0.184063
training IoU in current batch 200 is 0.5212890396537607
training IoU uptillnow 200 is 0.0027132601632286184
testing: bce: 1.422945, dice: 4.082992, loss: 1.422945
IoU in current test batch is 0.6889167523220585
training: bce: 0.132275, dice: 0.417536, loss: 0.132275
training IoU in current batch 300 is 0.7822014051522248
training IoU uptillnow 300 is 0.0031111827089204806
testing: bce: 1.531339, dice: 4.833786, loss: 1.531339
IoU in current test batch is 0.7952928156522805
training: bce: 0.105017, dice: 0.351927, loss: 0.105017
training IoU in current batch 400 is 0.8006358803294069
training IoU uptillnow 400 is 0.0033336257744383244
testing: bce: 1.619688, dice: 5.427801, loss: 1.619688
IoU in current test batch is 0.8254883092498715
Epoch   446: reducing learning rate of group 0 to 9.9900e-04.
training: bce: 0.213500, dice: 0.427585, loss: 0.213500
training IoU in current batch 500 is 0.0
training IoU uptillnow 500 is 0.0026682314082829704
testing: bce: 4.113979, dice: 8.239239, loss: 4.113979
IoU in current test batch is 0.0
Epoch   547: reducing learning rate of group 0 to 9.9800e-04.
training: bce: 0.219485, dice: 0.492094, loss: 0.219485
training IoU in current batch 600 is 0.0
training IoU uptillnow 600 is 0.0022242661157234078
testing: bce: 5.073488, dice: 11.374935, loss: 5.073488
IoU in current test batch is 0.0
Epoch   648: reducing learning rate of group 0 to 9.9700e-04.
training: bce: 0.209566, dice: 0.523967, loss: 0.209566
training IoU in current batch 700 is 0.0
training IoU uptillnow 700 is 0.0019069670977885423
testing: bce: 5.650217, dice: 14.126952, loss: 5.650217
IoU in current test batch is 0.0
Epoch   749: reducing learning rate of group 0 to 9.9601e-04.
training: bce: 0.194132, dice: 0.535978, loss: 0.194132
training IoU in current batch 800 is 0.0
training IoU uptillnow 800 is 0.0016688938021844795
testing: bce: 5.980748, dice: 16.512248, loss: 5.980748
IoU in current test batch is 0.0
Epoch   850: reducing learning rate of group 0 to 9.9501e-04.
training: bce: 0.181751, dice: 0.543718, loss: 0.181751
training IoU in current batch 900 is 0.0
training IoU uptillnow 900 is 0.0014836669650940824
testing: bce: 6.298368, dice: 18.841917, loss: 6.298368
IoU in current test batch is 0.0
Epoch   951: reducing learning rate of group 0 to 9.9401e-04.
training: bce: 0.170902, dice: 0.548709, loss: 0.170902
training IoU in current batch 1000 is 0.0
training IoU uptillnow 1000 is 0.0013354484870627054
testing: bce: 6.579743, dice: 21.125291, loss: 6.579743
IoU in current test batch is 0.0
Epoch  1052: reducing learning rate of group 0 to 9.9302e-04.
training: bce: 0.162717, dice: 0.549291, loss: 0.162717
training IoU in current batch 1100 is 0.3773319253783879
training IoU uptillnow 1100 is 0.001385513077419584
testing: bce: 6.890458, dice: 23.260351, loss: 6.890458
IoU in current test batch is 0.505117155392107
Epoch  1153: reducing learning rate of group 0 to 9.9203e-04.
training: bce: 0.154879, dice: 0.540428, loss: 0.154879
training IoU in current batch 1200 is 0.3447164948453608
training IoU uptillnow 1200 is 0.0014136620696599855
testing: bce: 7.154206, dice: 24.963609, loss: 7.154206
IoU in current test batch is 0.4834156436291208
Epoch  1254: reducing learning rate of group 0 to 9.9104e-04.
training: bce: 0.147224, dice: 0.528327, loss: 0.147224
training IoU in current batch 1300 is 0.5329295987887964
training IoU uptillnow 1300 is 0.0015098177902044894
testing: bce: 7.366886, dice: 26.436649, loss: 7.366886
IoU in current test batch is 0.6234835937730161
Epoch  1355: reducing learning rate of group 0 to 9.9004e-04.
training: bce: 0.140613, dice: 0.517537, loss: 0.140613
training IoU in current batch 1400 is 0.57384945329255
training IoU uptillnow 1400 is 0.001606850586511289
testing: bce: 7.576857, dice: 27.887270, loss: 7.576857
IoU in current test batch is 0.4953468991303488
Epoch  1456: reducing learning rate of group 0 to 9.8905e-04.
training: bce: 0.134773, dice: 0.507968, loss: 0.134773
training IoU in current batch 1500 is 0.5672539256989659
training IoU uptillnow 1500 is 0.0016887572515335103
testing: bce: 7.780553, dice: 29.325397, loss: 7.780553
IoU in current test batch is 0.6096192097101911
Epoch  1557: reducing learning rate of group 0 to 9.8807e-04.
training: bce: 0.129805, dice: 0.499634, loss: 0.129805
training IoU in current batch 1600 is 0.7469451425600139
training IoU uptillnow 1600 is 0.0018165504096388544
testing: bce: 7.992962, dice: 30.765911, loss: 7.992962
IoU in current test batch is 0.61036330036678
Epoch  1658: reducing learning rate of group 0 to 9.8708e-04.
training: bce: 0.125258, dice: 0.491595, loss: 0.125258
training IoU in current batch 1700 is 0.698728813559322
training IoU uptillnow 1700 is 0.0019151449809591222
testing: bce: 8.194752, dice: 32.161690, loss: 8.194752
IoU in current test batch is 0.5176448699306155
Epoch  1759: reducing learning rate of group 0 to 9.8609e-04.
training: bce: 0.120863, dice: 0.482704, loss: 0.120863
training IoU in current batch 1800 is 0.7304759400473311
training IoU uptillnow 1800 is 0.0020116044323348877
testing: bce: 8.372106, dice: 33.436520, loss: 8.372106
IoU in current test batch is 0.660330977798801
Epoch  1860: reducing learning rate of group 0 to 9.8510e-04.
training: bce: 0.117256, dice: 0.475554, loss: 0.117256
training IoU in current batch 1900 is 0.5972216661996077
training IoU uptillnow 1900 is 0.002062867130844259
testing: bce: 8.573184, dice: 34.770323, loss: 8.573184
IoU in current test batch is 0.5827894883489887
Epoch  1961: reducing learning rate of group 0 to 9.8412e-04.
training: bce: 0.113625, dice: 0.468339, loss: 0.113625
training IoU in current batch 2000 is 0.6483476521660075
training IoU uptillnow 2000 is 0.0021217812302938227
testing: bce: 8.744784, dice: 36.044070, loss: 8.744784
IoU in current test batch is 0.662623527826068
Epoch  2062: reducing learning rate of group 0 to 9.8314e-04.
training: bce: 0.110276, dice: 0.460441, loss: 0.110276
training IoU in current batch 2100 is 0.6528604118993135
training IoU uptillnow 2100 is 0.0021761610888946198
testing: bce: 8.911158, dice: 37.207212, loss: 8.911158
IoU in current test batch is 0.6060783034242515
Epoch  2163: reducing learning rate of group 0 to 9.8215e-04.
training: bce: 0.107161, dice: 0.452286, loss: 0.107161
training IoU in current batch 2200 is 0.8115936598343763
training IoU uptillnow 2200 is 0.0022616589176214378
testing: bce: 9.071569, dice: 38.287790, loss: 9.071569
IoU in current test batch is 0.6554753706299437
Epoch  2264: reducing learning rate of group 0 to 9.8117e-04.
training: bce: 0.104476, dice: 0.446376, loss: 0.104476
training IoU in current batch 2300 is 0.822375230637685
training IoU uptillnow 2300 is 0.00234206818470388
testing: bce: 9.246166, dice: 39.504285, loss: 9.246166
IoU in current test batch is 0.6488705218020255
Epoch  2365: reducing learning rate of group 0 to 9.8019e-04.
training: bce: 0.101693, dice: 0.438956, loss: 0.101693
training IoU in current batch 2400 is 0.848511851719836
training IoU uptillnow 2400 is 0.0024212223318881903
testing: bce: 9.390954, dice: 40.535888, loss: 9.390954
IoU in current test batch is 0.7069887187130781
Epoch  2466: reducing learning rate of group 0 to 9.7921e-04.
training: bce: 0.099098, dice: 0.432038, loss: 0.099098
training IoU in current batch 2500 is 0.6483552631578947
training IoU uptillnow 2500 is 0.002454031367629945
testing: bce: 9.532440, dice: 41.558737, loss: 9.532440
IoU in current test batch is 0.73633440173491
Epoch  2567: reducing learning rate of group 0 to 9.7823e-04.
training: bce: 0.096656, dice: 0.425273, loss: 0.096656
training IoU in current batch 2600 is 0.6592509439738748
training IoU uptillnow 2600 is 0.0024864121193500306
testing: bce: 9.669331, dice: 42.543655, loss: 9.669331
IoU in current test batch is 0.7161333024298494
Epoch  2668: reducing learning rate of group 0 to 9.7725e-04.
training: bce: 0.094382, dice: 0.418830, loss: 0.094382
training IoU in current batch 2700 is 0.7406884377758164
training IoU uptillnow 2700 is 0.002531470618777245
testing: bce: 9.804876, dice: 43.509959, loss: 9.804876
IoU in current test batch is 0.7578982461077
Epoch  2769: reducing learning rate of group 0 to 9.7627e-04.
training: bce: 0.092489, dice: 0.413567, loss: 0.092489
training IoU in current batch 2800 is 0.7414120871833414
training IoU uptillnow 2800 is 0.002573440979974655
testing: bce: 9.963877, dice: 44.553842, loss: 9.963877
IoU in current test batch is 0.74512012383758
Epoch  2870: reducing learning rate of group 0 to 9.7530e-04.
training: bce: 0.091041, dice: 0.409290, loss: 0.091041
training IoU in current batch 2900 is 0.7045045045045045
training IoU uptillnow 2900 is 0.0026061566484526927
testing: bce: 10.158097, dice: 45.667291, loss: 10.158097
IoU in current test batch is 0.6779726855510849
Epoch  2971: reducing learning rate of group 0 to 9.7432e-04.
training: bce: 0.089148, dice: 0.404105, loss: 0.089148
training IoU in current batch 3000 is 0.7541848108345651
training IoU uptillnow 3000 is 0.002644969291095816
testing: bce: 10.289727, dice: 46.643019, loss: 10.289727
IoU in current test batch is 0.7503464461039127
Epoch  3072: reducing learning rate of group 0 to 9.7335e-04.
training: bce: 0.087374, dice: 0.398948, loss: 0.087374
training IoU in current batch 3100 is 0.6960375009155497
training IoU uptillnow 3100 is 0.002671903125777594
testing: bce: 10.421085, dice: 47.582228, loss: 10.421085
IoU in current test batch is 0.7600262155130123
Epoch  3173: reducing learning rate of group 0 to 9.7237e-04.
training: bce: 0.085608, dice: 0.393544, loss: 0.085608
training IoU in current batch 3200 is 0.7800448430493273
training IoU uptillnow 3200 is 0.0027102761682477293
testing: bce: 10.539702, dice: 48.451359, loss: 10.539702
IoU in current test batch is 0.7603785567413012
Epoch  3274: reducing learning rate of group 0 to 9.7140e-04.
training: bce: 0.083986, dice: 0.388220, loss: 0.083986
training IoU in current batch 3300 is 0.7736742788924404
training IoU uptillnow 3300 is 0.002745359331719843
testing: bce: 10.662957, dice: 49.288985, loss: 10.662957
IoU in current test batch is 0.7350105852202489
Epoch  3375: reducing learning rate of group 0 to 9.7043e-04.
training: bce: 0.082373, dice: 0.382976, loss: 0.082373
training IoU in current batch 3400 is 0.5044191343963553
training IoU uptillnow 3400 is 0.002738794684270914
testing: bce: 10.774982, dice: 50.096142, loss: 10.774982
IoU in current test batch is 0.7454966851391026
Epoch  3476: reducing learning rate of group 0 to 9.6946e-04.
training: bce: 0.080940, dice: 0.378492, loss: 0.080940
training IoU in current batch 3500 is 0.7461460397235781
training IoU uptillnow 3500 is 0.002767127603846663
testing: bce: 10.898823, dice: 50.965443, loss: 10.898823
IoU in current test batch is 0.7577681211731644
Epoch  3577: reducing learning rate of group 0 to 9.6849e-04.
training: bce: 0.079503, dice: 0.373953, loss: 0.079503
training IoU in current batch 3600 is 0.7712785508007352
training IoU uptillnow 3600 is 0.0027973765666391374
testing: bce: 11.011168, dice: 51.792490, loss: 11.011168
IoU in current test batch is 0.7690728442224203
Epoch  3678: reducing learning rate of group 0 to 9.6752e-04.
training: bce: 0.078343, dice: 0.370144, loss: 0.078343
training IoU in current batch 3700 is 0.7512419366797657
training IoU uptillnow 3700 is 0.0028232839731984374
testing: bce: 11.151782, dice: 52.688514, loss: 11.151782
IoU in current test batch is 0.7681248237539062
Epoch  3779: reducing learning rate of group 0 to 9.6656e-04.
training: bce: 0.076931, dice: 0.365061, loss: 0.076931
training IoU in current batch 3800 is 0.7509498332945647
training IoU uptillnow 3800 is 0.0028477897662338067
testing: bce: 11.246691, dice: 53.369085, loss: 11.246691
IoU in current test batch is 0.7485706111910273
Epoch  3880: reducing learning rate of group 0 to 9.6559e-04.
training: bce: 0.075631, dice: 0.360077, loss: 0.075631
training IoU in current batch 3900 is 0.8138625642769113
training IoU uptillnow 3900 is 0.0028791028412184453
testing: bce: 11.347605, dice: 54.025444, loss: 11.347605
IoU in current test batch is 0.7457189224693419
Epoch  3981: reducing learning rate of group 0 to 9.6462e-04.
training: bce: 0.074330, dice: 0.355441, loss: 0.074330
training IoU in current batch 4000 is 0.8866359073984135
training IoU uptillnow 4000 is 0.002917945048061075
testing: bce: 11.438302, dice: 54.696896, loss: 11.438302
IoU in current test batch is 0.7846732023745413
Epoch  4082: reducing learning rate of group 0 to 9.6366e-04.
training: bce: 0.073039, dice: 0.350575, loss: 0.073039
training IoU in current batch 4100 is 0.8911379906196001
training IoU uptillnow 4100 is 0.002955441875786921
testing: bce: 11.520541, dice: 55.296495, loss: 11.520541
IoU in current test batch is 0.7601297921744993
Epoch  4183: reducing learning rate of group 0 to 9.6269e-04.
training: bce: 0.071980, dice: 0.346913, loss: 0.071980
training IoU in current batch 4200 is 0.5888761181350257
training IoU uptillnow 4200 is 0.002955178574546459
testing: bce: 11.630240, dice: 56.053204, loss: 11.630240
IoU in current test batch is 0.7549050663664688
Epoch  4284: reducing learning rate of group 0 to 9.6173e-04.
training: bce: 0.070859, dice: 0.342651, loss: 0.070859
training IoU in current batch 4300 is 0.8186150693418481
training IoU uptillnow 4300 is 0.0029816351374890955
testing: bce: 11.721673, dice: 56.682410, loss: 11.721673
IoU in current test batch is 0.8182345724313731
Epoch  4385: reducing learning rate of group 0 to 9.6077e-04.
training: bce: 0.069728, dice: 0.338417, loss: 0.069728
training IoU in current batch 4400 is 0.8122110685510501
training IoU uptillnow 4400 is 0.0030061618406307944
testing: bce: 11.802817, dice: 57.283531, loss: 11.802817
IoU in current test batch is 0.8125156535525204
training: bce: 0.068574, dice: 0.333812, loss: 0.068574
training IoU in current batch 4500 is 0.861763085399449
training IoU uptillnow 4500 is 0.003035103266677594
testing: bce: 11.871261, dice: 57.788003, loss: 11.871261
IoU in current test batch is 0.8344261966486222
Epoch  4541: reducing learning rate of group 0 to 9.5981e-04.
training: bce: 0.067480, dice: 0.329496, loss: 0.067480
training IoU in current batch 4600 is 0.866182953710507
training IoU uptillnow 4600 is 0.0030632669593938498
testing: bce: 11.941314, dice: 58.308065, loss: 11.941314
IoU in current test batch is 0.8216850520478918
Epoch  4660: reducing learning rate of group 0 to 9.5885e-04.
training: bce: 0.066434, dice: 0.325303, loss: 0.066434
training IoU in current batch 4700 is 0.9114944775681751
training IoU uptillnow 4700 is 0.003095051801522057
testing: bce: 12.011804, dice: 58.817223, loss: 12.011804
IoU in current test batch is 0.8342192418234203
Epoch  4761: reducing learning rate of group 0 to 9.5789e-04.
training: bce: 0.065461, dice: 0.321480, loss: 0.065461
training IoU in current batch 4800 is 0.9235186524309839
training IoU uptillnow 4800 is 0.003126764808408807
testing: bce: 12.087536, dice: 59.362601, loss: 12.087536
IoU in current test batch is 0.7990241451593652
Epoch  4862: reducing learning rate of group 0 to 9.5693e-04.
training: bce: 0.064537, dice: 0.317791, loss: 0.064537
training IoU in current batch 4900 is 0.6434558349451966
training IoU uptillnow 4900 is 0.0031286116634652687
testing: bce: 12.165224, dice: 59.903646, loss: 12.165224
IoU in current test batch is 0.8458132696105877
Epoch  4963: reducing learning rate of group 0 to 9.5598e-04.
training: bce: 0.063763, dice: 0.314869, loss: 0.063763
training IoU in current batch 5000 is 0.9255683207039843
training IoU uptillnow 5000 is 0.0031585902665457457
testing: bce: 12.264625, dice: 60.563874, loss: 12.264625
IoU in current test batch is 0.819464285299778
Epoch  5064: reducing learning rate of group 0 to 9.5502e-04.
training: bce: 0.063090, dice: 0.311957, loss: 0.063090
training IoU in current batch 5100 is 0.7978456604859598
training IoU uptillnow 5100 is 0.003174874093949864
testing: bce: 12.377709, dice: 61.203577, loss: 12.377709
IoU in current test batch is 0.6430457060391989
Epoch  5165: reducing learning rate of group 0 to 9.5406e-04.
training: bce: 0.062352, dice: 0.309272, loss: 0.062352
training IoU in current batch 5200 is 0.8600657810430994
training IoU uptillnow 5200 is 0.0031965132943202854
testing: bce: 12.472791, dice: 61.866218, loss: 12.472791
IoU in current test batch is 0.8220942514682587
Epoch  5266: reducing learning rate of group 0 to 9.5311e-04.
training: bce: 0.061523, dice: 0.306048, loss: 0.061523
training IoU in current batch 5300 is 0.8971674236383178
training IoU uptillnow 5300 is 0.003220835569813047
testing: bce: 12.543659, dice: 62.398539, loss: 12.543659
IoU in current test batch is 0.8583541815119347
Epoch  5367: reducing learning rate of group 0 to 9.5216e-04.
training: bce: 0.061132, dice: 0.304340, loss: 0.061132
training IoU in current batch 5400 is 0.7485208416744518
training IoU uptillnow 5400 is 0.003230496163009847
testing: bce: 12.698921, dice: 63.220869, loss: 12.698921
IoU in current test batch is 0.829282088320894
Epoch  5468: reducing learning rate of group 0 to 9.5121e-04.
training: bce: 0.060890, dice: 0.301842, loss: 0.060890
training IoU in current batch 5500 is 0.24044245642641518
training IoU uptillnow 5500 is 0.0031936249781184135
testing: bce: 12.883006, dice: 63.862789, loss: 12.883006
IoU in current test batch is 0.3242681056168098
Epoch  5569: reducing learning rate of group 0 to 9.5025e-04.
training: bce: 0.061032, dice: 0.305348, loss: 0.061032
training IoU in current batch 5600 is 0.6889795153049266
training IoU uptillnow 5600 is 0.003198111187695386
testing: bce: 13.147669, dice: 65.778987, loss: 13.147669
IoU in current test batch is 0.5832204912620956
Epoch  5670: reducing learning rate of group 0 to 9.4930e-04.
training: bce: 0.060670, dice: 0.305126, loss: 0.060670
training IoU in current batch 5700 is 0.7368261742934977
training IoU uptillnow 5700 is 0.003206636353171129
testing: bce: 13.303082, dice: 66.904726, loss: 13.303082
IoU in current test batch is 0.6858872364929167
Epoch  5771: reducing learning rate of group 0 to 9.4835e-04.
training: bce: 0.060197, dice: 0.303641, loss: 0.060197
training IoU in current batch 5800 is 0.8550348057613961
training IoU uptillnow 5800 is 0.0032250562407014835
testing: bce: 13.430950, dice: 67.747086, loss: 13.430950
IoU in current test batch is 0.7195980221716126
Epoch  5872: reducing learning rate of group 0 to 9.4741e-04.
training: bce: 0.059624, dice: 0.301433, loss: 0.059624
training IoU in current batch 5900 is 0.827829594151752
training IoU uptillnow 5900 is 0.0032405466953711543
testing: bce: 13.532430, dice: 68.413691, loss: 13.532430
IoU in current test batch is 0.8093690674951721
Epoch  5973: reducing learning rate of group 0 to 9.4646e-04.
training: bce: 0.059047, dice: 0.299027, loss: 0.059047
training IoU in current batch 6000 is 0.8930355427473583
training IoU uptillnow 6000 is 0.0032609538111579503
testing: bce: 13.628519, dice: 69.017675, loss: 13.628519
IoU in current test batch is 0.8509588456222885
Epoch  6074: reducing learning rate of group 0 to 9.4551e-04.
training: bce: 0.058453, dice: 0.296587, loss: 0.058453
training IoU in current batch 6100 is 0.7960043643477934
training IoU uptillnow 6100 is 0.0032727398791891093
testing: bce: 13.716331, dice: 69.595310, loss: 13.716331
IoU in current test batch is 0.6792786062249944
Epoch  6175: reducing learning rate of group 0 to 9.4457e-04.
training: bce: 0.057823, dice: 0.294200, loss: 0.057823
training IoU in current batch 6200 is 0.8839214029156983
training IoU uptillnow 6200 is 0.0032912347531673285
testing: bce: 13.790673, dice: 70.166790, loss: 13.790673
IoU in current test batch is 0.7914685501537685
Epoch  6276: reducing learning rate of group 0 to 9.4362e-04.
training: bce: 0.057184, dice: 0.291590, loss: 0.057184
training IoU in current batch 6300 is 0.9340029139154965
training IoU uptillnow 6300 is 0.0033131166737578725
testing: bce: 13.858396, dice: 70.665617, loss: 13.858396
IoU in current test batch is 0.8322733522540601
Epoch  6377: reducing learning rate of group 0 to 9.4268e-04.
training: bce: 0.056552, dice: 0.288967, loss: 0.056552
training IoU in current batch 6400 is 0.877314512264192
training IoU uptillnow 6400 is 0.00332988680166856
testing: bce: 13.922613, dice: 71.141561, loss: 13.922613
IoU in current test batch is 0.8430001840797053
Epoch  6479: reducing learning rate of group 0 to 9.4174e-04.
training: bce: 0.055927, dice: 0.286465, loss: 0.055927
training IoU in current batch 6500 is 0.6409399005874379
training IoU uptillnow 6500 is 0.00332796113948226
testing: bce: 13.983924, dice: 71.627320, loss: 13.983924
IoU in current test batch is 0.8680439592204117
Epoch  6580: reducing learning rate of group 0 to 9.4079e-04.
training: bce: 0.055352, dice: 0.283963, loss: 0.055352
training IoU in current batch 6600 is 0.8614639758400844
training IoU uptillnow 6600 is 0.0033427976603081673
testing: bce: 14.053015, dice: 72.093846, loss: 14.053015
IoU in current test batch is 0.8234607710721147
Epoch  6681: reducing learning rate of group 0 to 9.3985e-04.
training: bce: 0.054783, dice: 0.281578, loss: 0.054783
training IoU in current batch 6700 is 0.917487462568099
training IoU uptillnow 6700 is 0.0033613715993102908
testing: bce: 14.119318, dice: 72.571243, loss: 14.119318
IoU in current test batch is 0.8510801931467703
Epoch  6782: reducing learning rate of group 0 to 9.3891e-04.
training: bce: 0.054236, dice: 0.279472, loss: 0.054236
training IoU in current batch 6800 is 0.9516713970356354
training IoU uptillnow 6800 is 0.0033819124813256984
testing: bce: 14.186960, dice: 73.103350, loss: 14.186960
IoU in current test batch is 0.8421833082064001
Epoch  6883: reducing learning rate of group 0 to 9.3797e-04.
training: bce: 0.053685, dice: 0.277185, loss: 0.053685
training IoU in current batch 6900 is 0.846378439079169
training IoU uptillnow 6900 is 0.0033942292428685203
testing: bce: 14.249322, dice: 73.571380, loss: 14.249322
IoU in current test batch is 0.8691127694491907
Epoch  6984: reducing learning rate of group 0 to 9.3704e-04.
training: bce: 0.053910, dice: 0.277105, loss: 0.053910
training IoU in current batch 7000 is 0.8171306045870506
training IoU uptillnow 7000 is 0.003404105314573516
testing: bce: 14.516198, dice: 74.615792, loss: 14.516198
IoU in current test batch is 0.7700259827592053
Epoch  7085: reducing learning rate of group 0 to 9.3610e-04.
training: bce: 0.053483, dice: 0.275576, loss: 0.053483
training IoU in current batch 7100 is 0.8304071487005223
training IoU uptillnow 7100 is 0.0034146380624812626
testing: bce: 14.606944, dice: 75.263937, loss: 14.606944
IoU in current test batch is 0.6826183816679984
Epoch  7186: reducing learning rate of group 0 to 9.3516e-04.
training: bce: 0.053225, dice: 0.274422, loss: 0.053225
training IoU in current batch 7200 is 0.8394339805511583
training IoU uptillnow 7200 is 0.003425505050958898
testing: bce: 14.741410, dice: 76.004369, loss: 14.741410
IoU in current test batch is 0.7766681179689314
Epoch  7287: reducing learning rate of group 0 to 9.3423e-04.
training: bce: 0.052779, dice: 0.272719, loss: 0.052779
training IoU in current batch 7300 is 0.8501619765036493
training IoU uptillnow 7300 is 0.003436809048103938
testing: bce: 14.820876, dice: 76.581464, loss: 14.820876
IoU in current test batch is 0.7992067553498993
Epoch  7388: reducing learning rate of group 0 to 9.3329e-04.
training: bce: 0.052325, dice: 0.270930, loss: 0.052325
training IoU in current batch 7400 is 0.851465578872114
training IoU uptillnow 7400 is 0.0034478956424324964
testing: bce: 14.894374, dice: 77.121342, loss: 14.894374
IoU in current test batch is 0.847046985964809
Epoch  7489: reducing learning rate of group 0 to 9.3236e-04.
training: bce: 0.051954, dice: 0.269420, loss: 0.051954
training IoU in current batch 7500 is 0.8547516443638014
training IoU uptillnow 7500 is 0.003458905675486576
testing: bce: 14.988727, dice: 77.727759, loss: 14.988727
IoU in current test batch is 0.7831318095307955
training: bce: 0.051480, dice: 0.267481, loss: 0.051480
training IoU in current batch 7600 is 0.8871439759284449
training IoU uptillnow 7600 is 0.0034717568030244745
testing: bce: 15.049864, dice: 78.197147, loss: 15.049864
IoU in current test batch is 0.8449353614012408
Epoch  7661: reducing learning rate of group 0 to 9.3143e-04.
training: bce: 0.051062, dice: 0.265621, loss: 0.051062
training IoU in current batch 7700 is 0.7937115674672236
training IoU uptillnow 7700 is 0.0034782079267007718
testing: bce: 15.124287, dice: 78.674823, loss: 15.124287
IoU in current test batch is 0.8213762772303487
Epoch  7762: reducing learning rate of group 0 to 9.3050e-04.
training: bce: 0.050634, dice: 0.263807, loss: 0.050634
training IoU in current batch 7800 is 0.8603718068744148
training IoU uptillnow 7800 is 0.003488766202661178
testing: bce: 15.192003, dice: 79.152224, loss: 15.192003
IoU in current test batch is 0.8574670144407223
Epoch  7863: reducing learning rate of group 0 to 9.2957e-04.
training: bce: 0.050188, dice: 0.261877, loss: 0.050188
training IoU in current batch 7900 is 0.7304604843314413
training IoU uptillnow 7900 is 0.00349083601938053
testing: bce: 15.251220, dice: 79.580277, loss: 15.251220
IoU in current test batch is 0.8734395942946918
Epoch  7973: reducing learning rate of group 0 to 9.2864e-04.
training: bce: 0.049749, dice: 0.259978, loss: 0.049749
training IoU in current batch 8000 is 0.8865920418334832
training IoU uptillnow 8000 is 0.003502611099867805
testing: bce: 15.309227, dice: 80.003315, loss: 15.309227
IoU in current test batch is 0.8731225668789082
Epoch  8074: reducing learning rate of group 0 to 9.2771e-04.
training: bce: 0.049347, dice: 0.258211, loss: 0.049347
training IoU in current batch 8100 is 0.9281020782396088
training IoU uptillnow 8100 is 0.003516657505142836
testing: bce: 15.375405, dice: 80.452496, loss: 15.375405
IoU in current test batch is 0.8494806555704689
Epoch  8175: reducing learning rate of group 0 to 9.2678e-04.
training: bce: 0.048964, dice: 0.256540, loss: 0.048964
training IoU in current batch 8200 is 0.9039586144849303
training IoU uptillnow 8200 is 0.003528889374028116
testing: bce: 15.444499, dice: 80.918614, loss: 15.444499
IoU in current test batch is 0.8343121999601377
Epoch  8276: reducing learning rate of group 0 to 9.2585e-04.
training: bce: 0.048546, dice: 0.254758, loss: 0.048546
training IoU in current batch 8300 is 0.8865974646334742
training IoU uptillnow 8300 is 0.003539780808182305
testing: bce: 15.499101, dice: 81.336536, loss: 15.499101
IoU in current test batch is 0.848750430754969
Epoch  8377: reducing learning rate of group 0 to 9.2493e-04.
training: bce: 0.048229, dice: 0.253437, loss: 0.048229
training IoU in current batch 8400 is 0.9182637440478533
training IoU uptillnow 8400 is 0.003552297626561748
testing: bce: 15.583498, dice: 81.889450, loss: 15.583498
IoU in current test batch is 0.8714473253977069
Epoch  8478: reducing learning rate of group 0 to 9.2400e-04.
training: bce: 0.047871, dice: 0.251940, loss: 0.047871
training IoU in current batch 8500 is 0.9287275111078739
training IoU uptillnow 8500 is 0.0035651354095164313
testing: bce: 15.651975, dice: 82.374709, loss: 15.651975
IoU in current test batch is 0.8202339711246468
Epoch  8579: reducing learning rate of group 0 to 9.2308e-04.
training: bce: 0.047507, dice: 0.250445, loss: 0.047507
training IoU in current batch 8600 is 0.8686491470723836
training IoU uptillnow 8600 is 0.0035741821520561996
testing: bce: 15.715712, dice: 82.849015, loss: 15.715712
IoU in current test batch is 0.8728811923979316
training: bce: 0.047128, dice: 0.248755, loss: 0.047128
training IoU in current batch 8700 is 0.8697767723761151
training IoU uptillnow 8700 is 0.003583085746008899
testing: bce: 15.771539, dice: 83.246821, loss: 15.771539
IoU in current test batch is 0.8744860492150585
Epoch  8723: reducing learning rate of group 0 to 9.2216e-04.
training: bce: 0.046748, dice: 0.247097, loss: 0.046748
training IoU in current batch 8800 is 0.8850243902439024
training IoU uptillnow 8800 is 0.0035926532520333346
testing: bce: 15.824179, dice: 83.642230, loss: 15.824179
IoU in current test batch is 0.875837660842794
Epoch  8824: reducing learning rate of group 0 to 9.2123e-04.
training: bce: 0.046393, dice: 0.245553, loss: 0.046393
training IoU in current batch 8900 is 0.8345648210344496
training IoU uptillnow 8900 is 0.003599171293299922
testing: bce: 15.882356, dice: 84.064137, loss: 15.882356
IoU in current test batch is 0.8437878264544978
Epoch  8925: reducing learning rate of group 0 to 9.2031e-04.
training: bce: 0.046031, dice: 0.244022, loss: 0.046031
training IoU in current batch 9000 is 0.8873248448397211
training IoU uptillnow 9000 is 0.00360847529208782
testing: bce: 15.935495, dice: 84.478394, loss: 15.935495
IoU in current test batch is 0.8796585484543561
Epoch  9026: reducing learning rate of group 0 to 9.1939e-04.
training: bce: 0.045695, dice: 0.242594, loss: 0.045695
training IoU in current batch 9100 is 0.8979804455842283
training IoU uptillnow 9100 is 0.0036181602380919217
testing: bce: 15.995067, dice: 84.917104, loss: 15.995067
IoU in current test batch is 0.8673498001963711
Epoch  9127: reducing learning rate of group 0 to 9.1847e-04.
training: bce: 0.045427, dice: 0.241345, loss: 0.045427
training IoU in current batch 9200 is 0.9081793555169207
training IoU uptillnow 9200 is 0.00362818889301522
testing: bce: 16.075782, dice: 85.408268, loss: 16.075782
IoU in current test batch is 0.8233916515375951
Epoch  9228: reducing learning rate of group 0 to 9.1755e-04.
training: bce: 0.045158, dice: 0.240255, loss: 0.045158
training IoU in current batch 9300 is 0.8077165125623276
training IoU uptillnow 9300 is 0.0036326012537269328
testing: bce: 16.154436, dice: 85.946616, loss: 16.154436
IoU in current test batch is 0.854342801326431
Epoch  9329: reducing learning rate of group 0 to 9.1664e-04.
training: bce: 0.044827, dice: 0.238858, loss: 0.044827
training IoU in current batch 9400 is 0.7688523703983999
training IoU uptillnow 9400 is 0.0036348527227011386
testing: bce: 16.208367, dice: 86.365506, loss: 16.208367
IoU in current test batch is 0.8709935464741239
Epoch  9430: reducing learning rate of group 0 to 9.1572e-04.
training: bce: 0.044518, dice: 0.237523, loss: 0.044518
training IoU in current batch 9500 is 0.9122570408568028
training IoU uptillnow 9500 is 0.0036446036171499637
testing: bce: 16.267845, dice: 86.796545, loss: 16.267845
IoU in current test batch is 0.8767082855116975
Epoch  9531: reducing learning rate of group 0 to 9.1480e-04.
training: bce: 0.044209, dice: 0.236219, loss: 0.044209
training IoU in current batch 9600 is 0.828752642706131
training IoU uptillnow 9600 is 0.0036498026547125166
testing: bce: 16.324996, dice: 87.228515, loss: 16.324996
IoU in current test batch is 0.8430625983024184
Epoch  9632: reducing learning rate of group 0 to 9.1389e-04.
training: bce: 0.043902, dice: 0.234884, loss: 0.043902
training IoU in current batch 9700 is 0.8651730080869681
training IoU uptillnow 9700 is 0.003656771651575957
testing: bce: 16.380454, dice: 87.638711, loss: 16.380454
IoU in current test batch is 0.8693308229872534
Epoch  9733: reducing learning rate of group 0 to 9.1298e-04.
training: bce: 0.043731, dice: 0.234017, loss: 0.043731
training IoU in current batch 9800 is 0.9009626366454561
training IoU uptillnow 9800 is 0.0036654242536742257
testing: bce: 16.484766, dice: 88.215362, loss: 16.484766
IoU in current test batch is 0.8445700598785707
Epoch  9834: reducing learning rate of group 0 to 9.1206e-04.
training: bce: 0.043541, dice: 0.233350, loss: 0.043541
training IoU in current batch 9900 is 0.8592734051743737
training IoU uptillnow 9900 is 0.0036717967693009066
testing: bce: 16.580937, dice: 88.861488, loss: 16.580937
IoU in current test batch is 0.8304249141216573
Epoch  9935: reducing learning rate of group 0 to 9.1115e-04.
training: bce: 0.043362, dice: 0.232540, loss: 0.043362
training IoU in current batch 10000 is 0.8728722771098679
training IoU uptillnow 10000 is 0.003678721722968024
testing: bce: 16.679440, dice: 89.447513, loss: 16.679440
IoU in current test batch is 0.8700008894329184
Epoch 10036: reducing learning rate of group 0 to 9.1024e-04.
training: bce: 0.043150, dice: 0.231531, loss: 0.043150
training IoU in current batch 10100 is 0.7872843807803157
training IoU uptillnow 10100 is 0.0036812729573105006
testing: bce: 16.763583, dice: 89.949847, loss: 16.763583
IoU in current test batch is 0.8462567647123879
Epoch 10137: reducing learning rate of group 0 to 9.0933e-04.
training: bce: 0.042896, dice: 0.230431, loss: 0.042896
training IoU in current batch 10200 is 0.914075317638979
training IoU uptillnow 10200 is 0.003689988805079194
testing: bce: 16.829986, dice: 90.408649, loss: 16.829986
IoU in current test batch is 0.8513720244708624
Epoch 10238: reducing learning rate of group 0 to 9.0842e-04.
training: bce: 0.042743, dice: 0.229641, loss: 0.042743
training IoU in current batch 10300 is 0.8741199061233198
training IoU uptillnow 10300 is 0.0036965960347223104
testing: bce: 16.934484, dice: 90.982042, loss: 16.934484
IoU in current test batch is 0.8348862225154032
Epoch 10339: reducing learning rate of group 0 to 9.0751e-04.
training: bce: 0.042473, dice: 0.228544, loss: 0.042473
training IoU in current batch 10400 is 0.8762577862961188
training IoU uptillnow 10400 is 0.0037031789872918547
testing: bce: 16.990634, dice: 91.426556, loss: 16.990634
IoU in current test batch is 0.8607004341686089
Epoch 10440: reducing learning rate of group 0 to 9.0660e-04.
training: bce: 0.042217, dice: 0.227472, loss: 0.042217
training IoU in current batch 10500 is 0.7363074442354206
training IoU uptillnow 10500 is 0.0037029728948614693
testing: bce: 17.050823, dice: 91.872310, loss: 17.050823
IoU in current test batch is 0.8672983456115024
Epoch 10541: reducing learning rate of group 0 to 9.0570e-04.
training: bce: 0.041960, dice: 0.226327, loss: 0.041960
training IoU in current batch 10600 is 0.7905307040762308
training IoU uptillnow 10600 is 0.003705328150266806
testing: bce: 17.108263, dice: 92.280313, loss: 17.108263
IoU in current test batch is 0.8078957124353064
Epoch 10642: reducing learning rate of group 0 to 9.0479e-04.
training: bce: 0.041759, dice: 0.225520, loss: 0.041759
training IoU in current batch 10700 is 0.9059416118421053
training IoU uptillnow 10700 is 0.0037130319154190694
testing: bce: 17.186974, dice: 92.819027, loss: 17.186974
IoU in current test batch is 0.8581712070657672
Epoch 10743: reducing learning rate of group 0 to 9.0389e-04.
training: bce: 0.041540, dice: 0.224614, loss: 0.041540
training IoU in current batch 10800 is 0.8972089020041805
training IoU uptillnow 10800 is 0.003720188776770813
testing: bce: 17.256527, dice: 93.309685, loss: 17.256527
IoU in current test batch is 0.8724162798128241
Epoch 10844: reducing learning rate of group 0 to 9.0298e-04.
training: bce: 0.041303, dice: 0.223580, loss: 0.041303
training IoU in current batch 10900 is 0.85977212971078
training IoU uptillnow 10900 is 0.0037254972060138465
testing: bce: 17.317226, dice: 93.740275, loss: 17.317226
IoU in current test batch is 0.8267300986614744
Epoch 10945: reducing learning rate of group 0 to 9.0208e-04.
training: bce: 0.041095, dice: 0.222646, loss: 0.041095
training IoU in current batch 11000 is 0.7893383928024971
training IoU uptillnow 11000 is 0.0037275078846612294
testing: bce: 17.388034, dice: 94.204758, loss: 17.388034
IoU in current test batch is 0.8726207889492685
Epoch 11046: reducing learning rate of group 0 to 9.0118e-04.
training: bce: 0.040870, dice: 0.221652, loss: 0.040870
training IoU in current batch 11100 is 0.8747515628952408
training IoU uptillnow 11100 is 0.0037333294316373128
testing: bce: 17.449971, dice: 94.637046, loss: 17.449971
IoU in current test batch is 0.843458504744504
training: bce: 0.040657, dice: 0.220637, loss: 0.040657
training IoU in current batch 11200 is 0.7926663237830152
training IoU uptillnow 11200 is 0.0037353828392551838
testing: bce: 17.515229, dice: 95.052117, loss: 17.515229
IoU in current test batch is 0.8542782398560977
Epoch 11229: reducing learning rate of group 0 to 9.0028e-04.
training: bce: 0.040431, dice: 0.219606, loss: 0.040431
training IoU in current batch 11300 is 0.8890821514583891
training IoU uptillnow 11300 is 0.003741665716151359
testing: bce: 17.573574, dice: 95.452648, loss: 17.573574
IoU in current test batch is 0.8860309863788666
Epoch 11330: reducing learning rate of group 0 to 8.9938e-04.
training: bce: 0.040229, dice: 0.218693, loss: 0.040229
training IoU in current batch 11400 is 0.9303874501318548
training IoU uptillnow 11400 is 0.003749649853810406
testing: bce: 17.640360, dice: 95.896969, loss: 17.640360
IoU in current test batch is 0.8328362119064157
Epoch 11431: reducing learning rate of group 0 to 8.9848e-04.
training: bce: 0.040024, dice: 0.217830, loss: 0.040024
training IoU in current batch 11500 is 0.9333751793400287
training IoU uptillnow 11500 is 0.00375762503894987
testing: bce: 17.704325, dice: 96.356208, loss: 17.704325
IoU in current test batch is 0.843968631403233
Epoch 11532: reducing learning rate of group 0 to 8.9758e-04.
training: bce: 0.039787, dice: 0.216735, loss: 0.039787
training IoU in current batch 11600 is 0.9032783178837893
training IoU uptillnow 11600 is 0.0037641655660636455
testing: bce: 17.752768, dice: 96.705578, loss: 17.752768
IoU in current test batch is 0.8404356322915867
Epoch 11633: reducing learning rate of group 0 to 8.9668e-04.
training: bce: 0.039596, dice: 0.215921, loss: 0.039596
training IoU in current batch 11700 is 0.9224826851358551
training IoU uptillnow 11700 is 0.0037714149281661636
testing: bce: 17.819924, dice: 97.172749, loss: 17.819924
IoU in current test batch is 0.8807480565091923
Epoch 11734: reducing learning rate of group 0 to 8.9578e-04.
training: bce: 0.039465, dice: 0.215235, loss: 0.039465
training IoU in current batch 11800 is 0.9154238696150956
training IoU uptillnow 11800 is 0.0037782423531293814
testing: bce: 17.912370, dice: 97.691864, loss: 17.912370
IoU in current test batch is 0.8199118016760641
Epoch 11835: reducing learning rate of group 0 to 8.9489e-04.
training: bce: 0.039305, dice: 0.214627, loss: 0.039305
training IoU in current batch 11900 is 0.8103057757644394
training IoU uptillnow 11900 is 0.003780538685586257
testing: bce: 17.991091, dice: 98.241482, loss: 17.991091
IoU in current test batch is 0.6783620233714969
Epoch 11936: reducing learning rate of group 0 to 8.9399e-04.
training: bce: 0.039150, dice: 0.213879, loss: 0.039150
training IoU in current batch 12000 is 0.9139093242087254
training IoU uptillnow 12000 is 0.003787113203838548
testing: bce: 18.070793, dice: 98.721549, loss: 18.070793
IoU in current test batch is 0.8442991729528035
Epoch 12037: reducing learning rate of group 0 to 8.9310e-04.
training: bce: 0.038950, dice: 0.213066, loss: 0.038950
training IoU in current batch 12100 is 0.8745408538078524
training IoU uptillnow 12100 is 0.003791952399485195
testing: bce: 18.128202, dice: 99.166007, loss: 18.128202
IoU in current test batch is 0.8466373578295584
Epoch 12138: reducing learning rate of group 0 to 8.9221e-04.
training: bce: 0.038839, dice: 0.212458, loss: 0.038839
training IoU in current batch 12200 is 0.8797399310555483
training IoU uptillnow 12200 is 0.003796925330030171
testing: bce: 18.226025, dice: 99.699925, loss: 18.226025
IoU in current test batch is 0.8132017825880626
Epoch 12239: reducing learning rate of group 0 to 8.9131e-04.
training: bce: 0.038649, dice: 0.211646, loss: 0.038649
training IoU in current batch 12300 is 0.7439760645402576
training IoU uptillnow 12300 is 0.0037962989987780056
testing: bce: 18.285581, dice: 100.132849, loss: 18.285581
IoU in current test batch is 0.8492810467006446
Epoch 12340: reducing learning rate of group 0 to 8.9042e-04.
training: bce: 0.038454, dice: 0.210807, loss: 0.038454
training IoU in current batch 12400 is 0.79783041041403
training IoU uptillnow 12400 is 0.003797854139922205
testing: bce: 18.341101, dice: 100.547068, loss: 18.341101
IoU in current test batch is 0.8228211756027891
Epoch 12441: reducing learning rate of group 0 to 8.8953e-04.
training: bce: 0.038361, dice: 0.210300, loss: 0.038361
training IoU in current batch 12500 is 0.9058651921706978
training IoU uptillnow 12500 is 0.003803705446385138
testing: bce: 18.444238, dice: 101.113846, loss: 18.444238
IoU in current test batch is 0.8732287104640731
Epoch 12542: reducing learning rate of group 0 to 8.8864e-04.
training: bce: 0.038182, dice: 0.209532, loss: 0.038182
training IoU in current batch 12600 is 0.9365519307839245
training IoU uptillnow 12600 is 0.003810681513423742
testing: bce: 18.505165, dice: 101.550294, loss: 18.505165
IoU in current test batch is 0.8532917471132814
Epoch 12643: reducing learning rate of group 0 to 8.8775e-04.
training: bce: 0.037996, dice: 0.208714, loss: 0.037996
training IoU in current batch 12700 is 0.8885559287005942
training IoU uptillnow 12700 is 0.0038156582721835188
testing: bce: 18.561050, dice: 101.956983, loss: 18.561050
IoU in current test batch is 0.8745174865718114
training: bce: 0.037805, dice: 0.207837, loss: 0.037805
training IoU in current batch 12800 is 0.9451303155006858
training IoU uptillnow 12800 is 0.0038227670395088835
testing: bce: 18.612939, dice: 102.327844, loss: 18.612939
IoU in current test batch is 0.88678834494008
Epoch 12841: reducing learning rate of group 0 to 8.8687e-04.
training: bce: 0.037614, dice: 0.206993, loss: 0.037614
training IoU in current batch 12900 is 0.9160373831775701
training IoU uptillnow 12900 is 0.0038286380563012173
testing: bce: 18.663880, dice: 102.708298, loss: 18.663880
IoU in current test batch is 0.8586825697352041
Epoch 12942: reducing learning rate of group 0 to 8.8598e-04.
training: bce: 0.037437, dice: 0.206239, loss: 0.037437
training IoU in current batch 13000 is 0.7276526250060041
training IoU uptillnow 13000 is 0.0038271737463922008
testing: bce: 18.719838, dice: 103.127637, loss: 18.719838
IoU in current test batch is 0.8409217459728497
Epoch 13043: reducing learning rate of group 0 to 8.8509e-04.
training: bce: 0.037307, dice: 0.205681, loss: 0.037307
training IoU in current batch 13100 is 0.668646080760095
training IoU uptillnow 13100 is 0.0038234798043832573
testing: bce: 18.798229, dice: 103.639412, loss: 18.798229
IoU in current test batch is 0.7823680621504028
Epoch 13144: reducing learning rate of group 0 to 8.8421e-04.
training: bce: 0.037189, dice: 0.205134, loss: 0.037189
training IoU in current batch 13200 is 0.8902414179484568
training IoU uptillnow 13200 is 0.0038282349538822273
testing: bce: 18.881916, dice: 104.153065, loss: 18.881916
IoU in current test batch is 0.8943659551357798
Epoch 13245: reducing learning rate of group 0 to 8.8333e-04.
training: bce: 0.037014, dice: 0.204380, loss: 0.037014
training IoU in current batch 13300 is 0.8692608051703244
training IoU uptillnow 13300 is 0.003832129917208063
testing: bce: 18.935635, dice: 104.555954, loss: 18.935635
IoU in current test batch is 0.8582505854639619
Epoch 13346: reducing learning rate of group 0 to 8.8244e-04.
training: bce: 0.036839, dice: 0.203639, loss: 0.036839
training IoU in current batch 13400 is 0.9084066901408451
training IoU uptillnow 13400 is 0.0038374273094436886
testing: bce: 18.987523, dice: 104.960407, loss: 18.987523
IoU in current test batch is 0.8968939956464532
Epoch 13477: reducing learning rate of group 0 to 8.8156e-04.
training: bce: 0.036680, dice: 0.202929, loss: 0.036680
training IoU in current batch 13500 is 0.8103523587652883
training IoU uptillnow 13500 is 0.0038390148546950235
testing: bce: 19.046824, dice: 105.375031, loss: 19.046824
IoU in current test batch is 0.8749586349672991
Epoch 13578: reducing learning rate of group 0 to 8.8068e-04.
training: bce: 0.036501, dice: 0.202105, loss: 0.036501
training IoU in current batch 13600 is 0.8038852361028094
training IoU uptillnow 13600 is 0.003840341311027786
testing: bce: 19.094135, dice: 105.724018, loss: 19.094135
IoU in current test batch is 0.8476555505738391
Epoch 13679: reducing learning rate of group 0 to 8.7980e-04.
training: bce: 0.036336, dice: 0.201380, loss: 0.036336
training IoU in current batch 13700 is 0.8841625693536018
training IoU uptillnow 13700 is 0.0038445780202879883
testing: bce: 19.147750, dice: 106.119331, loss: 19.147750
IoU in current test batch is 0.8505919681984332
Epoch 13780: reducing learning rate of group 0 to 8.7892e-04.
training: bce: 0.036226, dice: 0.200902, loss: 0.036226
training IoU in current batch 13800 is 0.9247195213163799
training IoU uptillnow 13800 is 0.003850222680720521
testing: bce: 19.229244, dice: 106.640406, loss: 19.229244
IoU in current test batch is 0.8357482632922947
Epoch 13881: reducing learning rate of group 0 to 8.7804e-04.
training: bce: 0.036062, dice: 0.200142, loss: 0.036062
training IoU in current batch 13900 is 0.9203875054337701
training IoU uptillnow 13900 is 0.0038556303121603337
testing: bce: 19.280584, dice: 107.006538, loss: 19.280584
IoU in current test batch is 0.8751325376318088
Epoch 13982: reducing learning rate of group 0 to 8.7716e-04.
training: bce: 0.035907, dice: 0.199475, loss: 0.035907
training IoU in current batch 14000 is 0.8901822775672261
training IoU uptillnow 14000 is 0.00385988201615059
testing: bce: 19.336006, dice: 107.417197, loss: 19.336006
IoU in current test batch is 0.8741213818041417
Epoch 14083: reducing learning rate of group 0 to 8.7628e-04.
training: bce: 0.035755, dice: 0.198798, loss: 0.035755
training IoU in current batch 14100 is 0.8169652048589664
training IoU uptillnow 14100 is 0.0038614772505888863
testing: bce: 19.391336, dice: 107.817401, loss: 19.391336
IoU in current test batch is 0.8652687956295059
Epoch 14184: reducing learning rate of group 0 to 8.7541e-04.
training: bce: 0.035651, dice: 0.198293, loss: 0.035651
training IoU in current batch 14200 is 0.9182208379230448
training IoU uptillnow 14200 is 0.003866615106648504
testing: bce: 19.472439, dice: 108.306293, loss: 19.472439
IoU in current test batch is 0.816669969430788
Epoch 14285: reducing learning rate of group 0 to 8.7453e-04.
training: bce: 0.035511, dice: 0.197719, loss: 0.035511
training IoU in current batch 14300 is 0.8714345763526091
training IoU uptillnow 14300 is 0.0038700453407238453
testing: bce: 19.532658, dice: 108.753142, loss: 19.532658
IoU in current test batch is 0.8840427121089781
Epoch 14386: reducing learning rate of group 0 to 8.7366e-04.
training: bce: 0.035364, dice: 0.197099, loss: 0.035364
training IoU in current batch 14400 is 0.854208140963192
training IoU uptillnow 14400 is 0.0038728298373844395
testing: bce: 19.587819, dice: 109.170146, loss: 19.587819
IoU in current test batch is 0.8827430412809915
Epoch 14487: reducing learning rate of group 0 to 8.7278e-04.
training: bce: 0.035232, dice: 0.196524, loss: 0.035232
training IoU in current batch 14500 is 0.8353989695028547
training IoU uptillnow 14500 is 0.0038749273824511925
testing: bce: 19.649936, dice: 109.607660, loss: 19.649936
IoU in current test batch is 0.8399141712246748
Epoch 14588: reducing learning rate of group 0 to 8.7191e-04.
training: bce: 0.035110, dice: 0.195945, loss: 0.035110
training IoU in current batch 14600 is 0.9306370833108945
training IoU uptillnow 14600 is 0.0038802575518512563
testing: bce: 19.716964, dice: 110.038435, loss: 19.716964
IoU in current test batch is 0.8554758680517621
Epoch 14689: reducing learning rate of group 0 to 8.7104e-04.
training: bce: 0.034971, dice: 0.195299, loss: 0.034971
training IoU in current batch 14700 is 0.9296585290482077
training IoU uptillnow 14700 is 0.003885481924978185
testing: bce: 19.773438, dice: 110.426442, loss: 19.773438
IoU in current test batch is 0.8045037959627207
Epoch 14790: reducing learning rate of group 0 to 8.7017e-04.
training: bce: 0.034921, dice: 0.195112, loss: 0.034921
training IoU in current batch 14800 is 0.8396681936362511
training IoU uptillnow 14800 is 0.0038875956946099873
testing: bce: 19.879627, dice: 111.071135, loss: 19.879627
IoU in current test batch is 0.8463424008398339
Epoch 14891: reducing learning rate of group 0 to 8.6930e-04.
training: bce: 0.034891, dice: 0.194805, loss: 0.034891
training IoU in current batch 14900 is 0.85704854539853
training IoU uptillnow 14900 is 0.0038902642875392047
testing: bce: 19.996381, dice: 111.645788, loss: 19.996381
IoU in current test batch is 0.8604693508987326
Epoch 14992: reducing learning rate of group 0 to 8.6843e-04.
training: bce: 0.034753, dice: 0.194204, loss: 0.034753
training IoU in current batch 15000 is 0.9127601889102677
training IoU uptillnow 15000 is 0.003894754232589616
testing: bce: 20.051068, dice: 112.048473, loss: 20.051068
IoU in current test batch is 0.8687809683731603
Epoch 15093: reducing learning rate of group 0 to 8.6756e-04.
training: bce: 0.034625, dice: 0.193630, loss: 0.034625
training IoU in current batch 15100 is 0.8958653759160409
training IoU uptillnow 15100 is 0.0038986253182593765
testing: bce: 20.110209, dice: 112.461664, loss: 20.110209
IoU in current test batch is 0.8722526510674504
Epoch 15194: reducing learning rate of group 0 to 8.6669e-04.
training: bce: 0.034573, dice: 0.193347, loss: 0.034573
training IoU in current batch 15200 is 0.8672386636823344
training IoU uptillnow 15200 is 0.003901503865724361
testing: bce: 20.212999, dice: 113.041212, loss: 20.212999
IoU in current test batch is 0.8906823107774697
Epoch 15295: reducing learning rate of group 0 to 8.6583e-04.
training: bce: 0.034457, dice: 0.192834, loss: 0.034457
training IoU in current batch 15300 is 0.8419237961664329
training IoU uptillnow 15300 is 0.0039035175583922114
testing: bce: 20.278147, dice: 113.482840, loss: 20.278147
IoU in current test batch is 0.8491093386286273
Epoch 15396: reducing learning rate of group 0 to 8.6496e-04.
training: bce: 0.034504, dice: 0.192640, loss: 0.034504
training IoU in current batch 15400 is 0.8549044585987261
training IoU uptillnow 15400 is 0.0039059265236191538
testing: bce: 20.438180, dice: 114.109540, loss: 20.438180
IoU in current test batch is 0.8159700503464978
Epoch 15497: reducing learning rate of group 0 to 8.6409e-04.
training: bce: 0.034376, dice: 0.192127, loss: 0.034376
training IoU in current batch 15500 is 0.9219903283278188
training IoU uptillnow 15500 is 0.00391046832813512
testing: bce: 20.494993, dice: 114.544724, loss: 20.494993
IoU in current test batch is 0.8631851763439664
Epoch 15598: reducing learning rate of group 0 to 8.6323e-04.
training: bce: 0.034258, dice: 0.191621, loss: 0.034258
training IoU in current batch 15600 is 0.9104770813844715
training IoU uptillnow 15600 is 0.003914582917448544
testing: bce: 20.555843, dice: 114.979960, loss: 20.555843
IoU in current test batch is 0.7997611578369098
training: bce: 0.034140, dice: 0.191111, loss: 0.034140
training IoU in current batch 15700 is 0.8934918396131372
training IoU uptillnow 15700 is 0.003918104198135233
testing: bce: 20.616632, dice: 115.409244, loss: 20.616632
IoU in current test batch is 0.8644262363680402
training: bce: 0.034006, dice: 0.190507, loss: 0.034006
training IoU in current batch 15800 is 0.842093580751132
training IoU uptillnow 15800 is 0.003919954484228647
testing: bce: 20.666267, dice: 115.777150, loss: 20.666267
IoU in current test batch is 0.806654670477415
Epoch 15824: reducing learning rate of group 0 to 8.6237e-04.
training: bce: 0.033883, dice: 0.189966, loss: 0.033883
training IoU in current batch 15900 is 0.9113358342847823
training IoU uptillnow 15900 is 0.003923958790166609
testing: bce: 20.721817, dice: 116.178848, loss: 20.721817
IoU in current test batch is 0.9020942173511717
Epoch 15925: reducing learning rate of group 0 to 8.6150e-04.
training: bce: 0.033744, dice: 0.189342, loss: 0.033744
training IoU in current batch 16000 is 0.8882699604423993
training IoU uptillnow 16000 is 0.00392719228189866
testing: bce: 20.766786, dice: 116.525434, loss: 20.766786
IoU in current test batch is 0.8683042833844041
Epoch 16026: reducing learning rate of group 0 to 8.6064e-04.
training: bce: 0.033646, dice: 0.188876, loss: 0.033646
training IoU in current batch 16100 is 0.9387174985369117
training IoU uptillnow 16100 is 0.003931952204951799
testing: bce: 20.835677, dice: 116.965285, loss: 20.835677
IoU in current test batch is 0.8896614026947828
Epoch 16127: reducing learning rate of group 0 to 8.5978e-04.
training: bce: 0.033515, dice: 0.188309, loss: 0.033515
training IoU in current batch 16200 is 0.9189505739048958
training IoU uptillnow 16200 is 0.0039360433145411615
testing: bce: 20.883629, dice: 117.338103, loss: 20.883629
IoU in current test batch is 0.8679407472808763
Epoch 16228: reducing learning rate of group 0 to 8.5892e-04.
training: bce: 0.033389, dice: 0.187760, loss: 0.033389
training IoU in current batch 16300 is 0.9109904702390251
training IoU uptillnow 16300 is 0.003939840069566338
testing: bce: 20.933857, dice: 117.718293, loss: 20.933857
IoU in current test batch is 0.8741453881814168
Epoch 16329: reducing learning rate of group 0 to 8.5806e-04.
training: bce: 0.033282, dice: 0.187319, loss: 0.033282
training IoU in current batch 16400 is 0.8699869697408427
training IoU uptillnow 16400 is 0.003942340495022943
testing: bce: 20.994844, dice: 118.162243, loss: 20.994844
IoU in current test batch is 0.8701241147854871
Epoch 16430: reducing learning rate of group 0 to 8.5721e-04.
training: bce: 0.033251, dice: 0.187069, loss: 0.033251
training IoU in current batch 16500 is 0.907180622712278
training IoU uptillnow 16500 is 0.003945937626218255
testing: bce: 21.103014, dice: 118.723831, loss: 21.103014
IoU in current test batch is 0.8605944788243449
Epoch 16531: reducing learning rate of group 0 to 8.5635e-04.
training: bce: 0.033153, dice: 0.186607, loss: 0.033153
training IoU in current batch 16600 is 0.9388012121689165
training IoU uptillnow 16600 is 0.003950443791115709
testing: bce: 21.168461, dice: 119.148306, loss: 21.168461
IoU in current test batch is 0.8777209531553046
Epoch 16632: reducing learning rate of group 0 to 8.5549e-04.
training: bce: 0.033115, dice: 0.186207, loss: 0.033115
training IoU in current batch 16700 is 0.8044179104477612
training IoU uptillnow 16700 is 0.003950872781961306
testing: bce: 21.271233, dice: 119.609518, loss: 21.271233
IoU in current test batch is 0.8862835030082276
Epoch 16733: reducing learning rate of group 0 to 8.5464e-04.
training: bce: 0.033008, dice: 0.185763, loss: 0.033008
training IoU in current batch 16800 is 0.9109532319225908
training IoU uptillnow 16800 is 0.003954467171447954
testing: bce: 21.329463, dice: 120.038662, loss: 21.329463
IoU in current test batch is 0.8744864921848842
Epoch 16834: reducing learning rate of group 0 to 8.5378e-04.
training: bce: 0.032882, dice: 0.185224, loss: 0.032882
training IoU in current batch 16900 is 0.8612246958453228
training IoU uptillnow 16900 is 0.003956547854885494
testing: bce: 21.374435, dice: 120.402641, loss: 21.374435
IoU in current test batch is 0.8588807207812107
Epoch 16935: reducing learning rate of group 0 to 8.5293e-04.
training: bce: 0.032765, dice: 0.184698, loss: 0.032765
training IoU in current batch 17000 is 0.8355161787365177
training IoU uptillnow 17000 is 0.003957847972753838
testing: bce: 21.424742, dice: 120.771287, loss: 21.424742
IoU in current test batch is 0.8642338413016281
Epoch 17036: reducing learning rate of group 0 to 8.5208e-04.
training: bce: 0.032670, dice: 0.184204, loss: 0.032670
training IoU in current batch 17100 is 0.8965929698653393
training IoU uptillnow 17100 is 0.003960918652109273
testing: bce: 21.487993, dice: 121.156646, loss: 21.487993
IoU in current test batch is 0.8917464939277558
Epoch 17137: reducing learning rate of group 0 to 8.5122e-04.
training: bce: 0.032617, dice: 0.183874, loss: 0.032617
training IoU in current batch 17200 is 0.9087110022335904
training IoU uptillnow 17200 is 0.00396430587586986
testing: bce: 21.578867, dice: 121.646553, loss: 21.578867
IoU in current test batch is 0.8665219581142598
Epoch 17238: reducing learning rate of group 0 to 8.5037e-04.
training: bce: 0.032521, dice: 0.183472, loss: 0.032521
training IoU in current batch 17300 is 0.9503127393413326
training IoU uptillnow 17300 is 0.00396885623608509
testing: bce: 21.640445, dice: 122.086603, loss: 21.640445
IoU in current test batch is 0.8551773287329018
Epoch 17339: reducing learning rate of group 0 to 8.4952e-04.
training: bce: 0.032410, dice: 0.183004, loss: 0.032410
training IoU in current batch 17400 is 0.7176213060926669
training IoU uptillnow 17400 is 0.003966668145138467
testing: bce: 21.691096, dice: 122.478654, loss: 21.691096
IoU in current test batch is 0.8120151173394945
Epoch 17440: reducing learning rate of group 0 to 8.4867e-04.
training: bce: 0.032329, dice: 0.182579, loss: 0.032329
training IoU in current batch 17500 is 0.8817089558137696
training IoU uptillnow 17500 is 0.003969193010197209
testing: bce: 21.761453, dice: 122.896723, loss: 21.761453
IoU in current test batch is 0.8536167561056568
Epoch 17541: reducing learning rate of group 0 to 8.4782e-04.
training: bce: 0.032213, dice: 0.182039, loss: 0.032213
training IoU in current batch 17600 is 0.9375814284022267
training IoU uptillnow 17600 is 0.003973276381209163
testing: bce: 21.806983, dice: 123.233606, loss: 21.806983
IoU in current test batch is 0.8731676733480818
Epoch 17642: reducing learning rate of group 0 to 8.4698e-04.
training: bce: 0.032109, dice: 0.181586, loss: 0.032109
training IoU in current batch 17700 is 0.8255520644333318
training IoU uptillnow 17700 is 0.0039741491225286215
testing: bce: 21.860193, dice: 123.625011, loss: 21.860193
IoU in current test batch is 0.8385023436807459
Epoch 17743: reducing learning rate of group 0 to 8.4613e-04.
training: bce: 0.032044, dice: 0.181285, loss: 0.032044
training IoU in current batch 17800 is 0.8886139769385183
training IoU uptillnow 17800 is 0.003976783360841998
testing: bce: 21.939108, dice: 124.117365, loss: 21.939108
IoU in current test batch is 0.8597102063350164
Epoch 17844: reducing learning rate of group 0 to 8.4528e-04.
training: bce: 0.031931, dice: 0.180767, loss: 0.031931
training IoU in current batch 17900 is 0.9252126267582598
training IoU uptillnow 17900 is 0.003980410419514414
testing: bce: 21.984504, dice: 124.457920, loss: 21.984504
IoU in current test batch is 0.8307916861101305
Epoch 17945: reducing learning rate of group 0 to 8.4444e-04.
training: bce: 0.031832, dice: 0.180321, loss: 0.031832
training IoU in current batch 18000 is 0.9274316274620635
training IoU uptillnow 18000 is 0.003984058815257961
testing: bce: 22.038771, dice: 124.844426, loss: 22.038771
IoU in current test batch is 0.8637865541646281
Epoch 18046: reducing learning rate of group 0 to 8.4359e-04.
training: bce: 0.031932, dice: 0.180267, loss: 0.031932
training IoU in current batch 18100 is 0.865966252785737
training IoU uptillnow 18100 is 0.003985969054740148
testing: bce: 22.231132, dice: 125.500725, loss: 22.231132
IoU in current test batch is 0.8521837689133519
Epoch 18147: reducing learning rate of group 0 to 8.4275e-04.
training: bce: 0.031835, dice: 0.179892, loss: 0.031835
training IoU in current batch 18200 is 0.9279174606125389
training IoU uptillnow 18200 is 0.003989560166483034
testing: bce: 22.285494, dice: 125.931356, loss: 22.285494
IoU in current test batch is 0.829650601167851
Epoch 18248: reducing learning rate of group 0 to 8.4191e-04.
training: bce: 0.031731, dice: 0.179464, loss: 0.031731
training IoU in current batch 18300 is 0.9026741038471003
training IoU uptillnow 18300 is 0.003992422361733306
testing: bce: 22.334697, dice: 126.321747, loss: 22.334697
IoU in current test batch is 0.8706814621725035
Epoch 18349: reducing learning rate of group 0 to 8.4106e-04.
training: bce: 0.031634, dice: 0.179046, loss: 0.031634
training IoU in current batch 18400 is 0.9088783832926386
training IoU uptillnow 18400 is 0.0039954220332442565
testing: bce: 22.388448, dice: 126.716319, loss: 22.388448
IoU in current test batch is 0.8855953680303214
Epoch 18450: reducing learning rate of group 0 to 8.4022e-04.
training: bce: 0.031546, dice: 0.178676, loss: 0.031546
training IoU in current batch 18500 is 0.8828489415141729
training IoU uptillnow 18500 is 0.003997685817225266
testing: bce: 22.447667, dice: 127.141795, loss: 22.447667
IoU in current test batch is 0.8595625083084875
Epoch 18551: reducing learning rate of group 0 to 8.3938e-04.
training: bce: 0.031541, dice: 0.178428, loss: 0.031541
training IoU in current batch 18600 is 0.8471140118294922
training IoU uptillnow 18600 is 0.0039989646960055586
testing: bce: 22.565078, dice: 127.651781, loss: 22.565078
IoU in current test batch is 0.7969907293607474
Epoch 18652: reducing learning rate of group 0 to 8.3854e-04.
training: bce: 0.031492, dice: 0.178242, loss: 0.031492
training IoU in current batch 18700 is 0.9238159070598749
training IoU uptillnow 18700 is 0.004002280640817567
testing: bce: 22.650874, dice: 128.203812, loss: 22.650874
IoU in current test batch is 0.8742822224870466
Epoch 18753: reducing learning rate of group 0 to 8.3771e-04.
training: bce: 0.031411, dice: 0.177924, loss: 0.031411
training IoU in current batch 18800 is 0.9132653061224489
training IoU uptillnow 18800 is 0.004005280725333256
testing: bce: 22.713597, dice: 128.659341, loss: 22.713597
IoU in current test batch is 0.8716735515224899
Epoch 18854: reducing learning rate of group 0 to 8.3687e-04.
training: bce: 0.031329, dice: 0.177547, loss: 0.031329
training IoU in current batch 18900 is 0.9064828614008942
training IoU uptillnow 18900 is 0.004008069644341092
testing: bce: 22.775256, dice: 129.069765, loss: 22.775256
IoU in current test batch is 0.8693687074894884
Epoch 18955: reducing learning rate of group 0 to 8.3603e-04.
training: bce: 0.031577, dice: 0.177725, loss: 0.031577
training IoU in current batch 19000 is 0.8542062798704969
training IoU uptillnow 19000 is 0.004009453580739237
testing: bce: 23.076750, dice: 129.882698, loss: 23.076750
IoU in current test batch is 0.8587120305143412
Epoch 19056: reducing learning rate of group 0 to 8.3519e-04.
training: bce: 0.031484, dice: 0.177356, loss: 0.031484
training IoU in current batch 19100 is 0.9152161706906232
training IoU uptillnow 19100 is 0.004012420060361842
testing: bce: 23.130047, dice: 130.294947, loss: 23.130047
IoU in current test batch is 0.8321497626707907
Epoch 19157: reducing learning rate of group 0 to 8.3436e-04.
training: bce: 0.031392, dice: 0.176986, loss: 0.031392
training IoU in current batch 19200 is 0.9140032002625856
training IoU uptillnow 19200 is 0.004015324054637928
testing: bce: 23.183077, dice: 130.703862, loss: 23.183077
IoU in current test batch is 0.8853130539727012
Epoch 19258: reducing learning rate of group 0 to 8.3353e-04.
training: bce: 0.031295, dice: 0.176537, loss: 0.031295
training IoU in current batch 19300 is 0.7114877066259203
training IoU uptillnow 19300 is 0.004012951713715134
testing: bce: 23.231613, dice: 131.051431, loss: 23.231613
IoU in current test batch is 0.8929778504682678
Epoch 19359: reducing learning rate of group 0 to 8.3269e-04.
training: bce: 0.031244, dice: 0.176303, loss: 0.031244
training IoU in current batch 19400 is 0.9048664162410839
training IoU uptillnow 19400 is 0.0040155875591225375
testing: bce: 23.313680, dice: 131.555602, loss: 23.313680
IoU in current test batch is 0.8057495291407176
Epoch 19460: reducing learning rate of group 0 to 8.3186e-04.
training: bce: 0.031160, dice: 0.175924, loss: 0.031160
training IoU in current batch 19500 is 0.9209520093640265
training IoU uptillnow 19500 is 0.004018608801559836
testing: bce: 23.371546, dice: 131.949733, loss: 23.371546
IoU in current test batch is 0.8609348644800676
Epoch 19561: reducing learning rate of group 0 to 8.3103e-04.
training: bce: 0.031077, dice: 0.175573, loss: 0.031077
training IoU in current batch 19600 is 0.8835002176752286
training IoU uptillnow 19600 is 0.004020643862458852
testing: bce: 23.428424, dice: 132.361922, loss: 23.428424
IoU in current test batch is 0.8760713353355827
Epoch 19662: reducing learning rate of group 0 to 8.3020e-04.
training: bce: 0.030994, dice: 0.175207, loss: 0.030994
training IoU in current batch 19700 is 0.9115834133845662
training IoU uptillnow 19700 is 0.0040233709991750796
testing: bce: 23.485155, dice: 132.759465, loss: 23.485155
IoU in current test batch is 0.8687366713318995
Epoch 19763: reducing learning rate of group 0 to 8.2937e-04.
training: bce: 0.030901, dice: 0.174784, loss: 0.030901
training IoU in current batch 19800 is 0.8965728274173806
training IoU uptillnow 19800 is 0.004025691554389017
testing: bce: 23.533225, dice: 133.111115, loss: 23.533225
IoU in current test batch is 0.8659971760398413
Epoch 19864: reducing learning rate of group 0 to 8.2854e-04.
training: bce: 0.030888, dice: 0.174655, loss: 0.030888
training IoU in current batch 19900 is 0.9542995327485859
training IoU uptillnow 19900 is 0.004029439135462098
testing: bce: 23.642110, dice: 133.684609, loss: 23.642110
IoU in current test batch is 0.8836802018751198
Epoch 19965: reducing learning rate of group 0 to 8.2771e-04.
training: bce: 0.030821, dice: 0.174298, loss: 0.030821
training IoU in current batch 20000 is 0.9368272585554154
training IoU uptillnow 20000 is 0.004032712457582567
testing: bce: 23.709440, dice: 134.082072, loss: 23.709440
IoU in current test batch is 0.8882419222096172
Epoch 20066: reducing learning rate of group 0 to 8.2688e-04.
training: bce: 0.030758, dice: 0.173976, loss: 0.030758
training IoU in current batch 20100 is 0.9178382326090926
training IoU uptillnow 20100 is 0.004035480870624023
testing: bce: 23.779460, dice: 134.503371, loss: 23.779460
IoU in current test batch is 0.870499382494698
Epoch 20167: reducing learning rate of group 0 to 8.2605e-04.
training: bce: 0.030704, dice: 0.173715, loss: 0.030704
training IoU in current batch 20200 is 0.9137361488605478
training IoU uptillnow 20200 is 0.004038120343292101
testing: bce: 23.856199, dice: 134.969694, loss: 23.856199
IoU in current test batch is 0.8847814337084288
Epoch 20268: reducing learning rate of group 0 to 8.2523e-04.
training: bce: 0.030614, dice: 0.173307, loss: 0.030614
training IoU in current batch 20300 is 0.9521307647402218
training IoU uptillnow 20300 is 0.0040416794461954515
testing: bce: 23.903935, dice: 135.319665, loss: 23.903935
IoU in current test batch is 0.8728655724911122
Epoch 20369: reducing learning rate of group 0 to 8.2440e-04.
training: bce: 0.030526, dice: 0.172932, loss: 0.030526
training IoU in current batch 20400 is 0.9233363494283129
training IoU uptillnow 20400 is 0.004044497946763787
testing: bce: 23.952244, dice: 135.691455, loss: 23.952244
IoU in current test batch is 0.8739929854831611
Epoch 20470: reducing learning rate of group 0 to 8.2358e-04.
training: bce: 0.030441, dice: 0.172550, loss: 0.030441
training IoU in current batch 20500 is 0.8932214616913225
training IoU uptillnow 20500 is 0.004046554477477864
testing: bce: 24.002562, dice: 136.055475, loss: 24.002562
IoU in current test batch is 0.84887560753767
Epoch 20571: reducing learning rate of group 0 to 8.2275e-04.
training: bce: 0.030355, dice: 0.172188, loss: 0.030355
training IoU in current batch 20600 is 0.8659595461272817
training IoU uptillnow 20600 is 0.004047929377983463
testing: bce: 24.052006, dice: 136.432697, loss: 24.052006
IoU in current test batch is 0.8891865406430899
Epoch 20672: reducing learning rate of group 0 to 8.2193e-04.
training: bce: 0.030281, dice: 0.171846, loss: 0.030281
training IoU in current batch 20700 is 0.8069467341254051
training IoU uptillnow 20700 is 0.004047865633684364
testing: bce: 24.109557, dice: 136.822202, loss: 24.109557
IoU in current test batch is 0.8717922393415596
Epoch 20773: reducing learning rate of group 0 to 8.2111e-04.
training: bce: 0.030199, dice: 0.171524, loss: 0.030199
training IoU in current batch 20800 is 0.936789912655152
training IoU uptillnow 20800 is 0.004050923582482938
testing: bce: 24.160210, dice: 137.225488, loss: 24.160210
IoU in current test batch is 0.8751296925434561
Epoch 20887: reducing learning rate of group 0 to 8.2029e-04.
training: bce: 0.030109, dice: 0.171113, loss: 0.030109
training IoU in current batch 20900 is 0.9332138672548074
training IoU uptillnow 20900 is 0.0040538667227814455
testing: bce: 24.204485, dice: 137.555018, loss: 24.204485
IoU in current test batch is 0.8721341566792038
Epoch 20988: reducing learning rate of group 0 to 8.1947e-04.
training: bce: 0.030021, dice: 0.170745, loss: 0.030021
training IoU in current batch 21000 is 0.9192583436341162
training IoU uptillnow 21000 is 0.004056449575956957
testing: bce: 24.248535, dice: 137.915670, loss: 24.248535
IoU in current test batch is 0.8842029320720115
Epoch 21089: reducing learning rate of group 0 to 8.1865e-04.
training: bce: 0.029957, dice: 0.170409, loss: 0.029957
training IoU in current batch 21100 is 0.8173654087757666
training IoU uptillnow 21100 is 0.004056593538176387
testing: bce: 24.312129, dice: 138.299850, loss: 24.312129
IoU in current test batch is 0.8557465469844868
Epoch 21190: reducing learning rate of group 0 to 8.1783e-04.
training: bce: 0.029915, dice: 0.170200, loss: 0.029915
training IoU in current batch 21200 is 0.9407993966817496
training IoU uptillnow 21200 is 0.004059647183972492
testing: bce: 24.393755, dice: 138.785067, loss: 24.393755
IoU in current test batch is 0.8917488615505869
Epoch 21291: reducing learning rate of group 0 to 8.1701e-04.
training: bce: 0.029854, dice: 0.169920, loss: 0.029854
training IoU in current batch 21300 is 0.9139744753564373
training IoU uptillnow 21300 is 0.004062042494956998
testing: bce: 24.458598, dice: 139.210410, loss: 24.458598
IoU in current test batch is 0.8778987926256978
Epoch 21392: reducing learning rate of group 0 to 8.1620e-04.
training: bce: 0.029790, dice: 0.169667, loss: 0.029790
training IoU in current batch 21400 is 0.9104818214822883
training IoU uptillnow 21400 is 0.0040643338206541825
testing: bce: 24.520218, dice: 139.655210, loss: 24.520218
IoU in current test batch is 0.8810908593510042
Epoch 21493: reducing learning rate of group 0 to 8.1538e-04.
training: bce: 0.029706, dice: 0.169312, loss: 0.029706
training IoU in current batch 21500 is 0.9223853370396108
training IoU uptillnow 21500 is 0.004066880645753219
testing: bce: 24.565374, dice: 140.014586, loss: 24.565374
IoU in current test batch is 0.8848874891735197
Epoch 21594: reducing learning rate of group 0 to 8.1456e-04.
training: bce: 0.029645, dice: 0.169013, loss: 0.029645
training IoU in current batch 21600 is 0.9226780021253985
training IoU uptillnow 21600 is 0.004069410664571208
testing: bce: 24.629221, dice: 140.417201, loss: 24.629221
IoU in current test batch is 0.851595133101382
Epoch 21695: reducing learning rate of group 0 to 8.1375e-04.
training: bce: 0.029586, dice: 0.168797, loss: 0.029586
training IoU in current batch 21700 is 0.9187993406784072
training IoU uptillnow 21700 is 0.0040718280003567515
testing: bce: 24.693870, dice: 140.886779, loss: 24.693870
IoU in current test batch is 0.8776748995116831
Epoch 21796: reducing learning rate of group 0 to 8.1294e-04.
training: bce: 0.029507, dice: 0.168436, loss: 0.029507
training IoU in current batch 21800 is 0.9167329237441272
training IoU uptillnow 21800 is 0.004074175767057196
testing: bce: 24.741498, dice: 141.233584, loss: 24.741498
IoU in current test batch is 0.8672581173089587
Epoch 21897: reducing learning rate of group 0 to 8.1212e-04.
training: bce: 0.029511, dice: 0.168413, loss: 0.029511
training IoU in current batch 21900 is 0.9038797325497211
training IoU uptillnow 21900 is 0.004076208655490105
testing: bce: 24.858561, dice: 141.862074, loss: 24.858561
IoU in current test batch is 0.8648663106351547
training: bce: 0.029444, dice: 0.168126, loss: 0.029444
training IoU in current batch 22000 is 0.884817579644622
training IoU uptillnow 22000 is 0.004077789852902645
testing: bce: 24.915444, dice: 142.266960, loss: 24.915444
IoU in current test batch is 0.8601716589161671
Epoch 22061: reducing learning rate of group 0 to 8.1131e-04.
training: bce: 0.029384, dice: 0.167875, loss: 0.029384
training IoU in current batch 22100 is 0.8736741973840666
training IoU uptillnow 22100 is 0.00407910464017027
testing: bce: 24.977307, dice: 142.700466, loss: 24.977307
IoU in current test batch is 0.887165044141468
Epoch 22162: reducing learning rate of group 0 to 8.1050e-04.
training: bce: 0.029311, dice: 0.167571, loss: 0.029311
training IoU in current batch 22200 is 0.9123963097914567
training IoU uptillnow 22200 is 0.004081279663407003
testing: bce: 25.027845, dice: 143.086551, loss: 25.027845
IoU in current test batch is 0.8305744661565695
Epoch 22263: reducing learning rate of group 0 to 8.0969e-04.
training: bce: 0.029251, dice: 0.167262, loss: 0.029251
training IoU in current batch 22300 is 0.9292140475864273
training IoU uptillnow 22300 is 0.004083812242997717
testing: bce: 25.089234, dice: 143.466145, loss: 25.089234
IoU in current test batch is 0.8531653737770243
Epoch 22364: reducing learning rate of group 0 to 8.0888e-04.
training: bce: 0.029186, dice: 0.166998, loss: 0.029186
training IoU in current batch 22400 is 0.8807483181273615
training IoU uptillnow 22400 is 0.00408524043525538
testing: bce: 25.146044, dice: 143.881249, loss: 25.146044
IoU in current test batch is 0.8819278184107848
Epoch 22465: reducing learning rate of group 0 to 8.0807e-04.
training: bce: 0.029138, dice: 0.166802, loss: 0.029138
training IoU in current batch 22500 is 0.8528498871331829
training IoU uptillnow 22500 is 0.004086035995454529
testing: bce: 25.216342, dice: 144.353997, loss: 25.216342
IoU in current test batch is 0.8883261587598595
Epoch 22566: reducing learning rate of group 0 to 8.0726e-04.
training: bce: 0.029066, dice: 0.166509, loss: 0.029066
training IoU in current batch 22600 is 0.866747609652451
training IoU uptillnow 22600 is 0.00408713197374225
testing: bce: 25.266330, dice: 144.741041, loss: 25.266330
IoU in current test batch is 0.868784862249015
Epoch 22667: reducing learning rate of group 0 to 8.0645e-04.
training: bce: 0.029011, dice: 0.166292, loss: 0.029011
training IoU in current batch 22700 is 0.850993098804915
training IoU uptillnow 22700 is 0.004087871295887892
testing: bce: 25.329683, dice: 145.192107, loss: 25.329683
IoU in current test batch is 0.8586290703571868
Epoch 22768: reducing learning rate of group 0 to 8.0565e-04.
training: bce: 0.028937, dice: 0.165979, loss: 0.028937
training IoU in current batch 22800 is 0.8985540509524902
training IoU uptillnow 22800 is 0.0040896470906288005
testing: bce: 25.377043, dice: 145.557528, loss: 25.377043
IoU in current test batch is 0.8522773435377312
Epoch 22869: reducing learning rate of group 0 to 8.0484e-04.
training: bce: 0.028902, dice: 0.165775, loss: 0.028902
training IoU in current batch 22900 is 0.9242314792519464
training IoU uptillnow 22900 is 0.004091967994980711
testing: bce: 25.456880, dice: 146.016299, loss: 25.456880
IoU in current test batch is 0.8461612342267243
Epoch 22970: reducing learning rate of group 0 to 8.0404e-04.
training: bce: 0.028851, dice: 0.165541, loss: 0.028851
training IoU in current batch 23000 is 0.7394119233881845
training IoU uptillnow 23000 is 0.004090251076681334
testing: bce: 25.523422, dice: 146.446411, loss: 25.523422
IoU in current test batch is 0.8734258559107785
Epoch 23071: reducing learning rate of group 0 to 8.0323e-04.
training: bce: 0.028780, dice: 0.165234, loss: 0.028780
training IoU in current batch 23100 is 0.9079665610566096
training IoU uptillnow 23100 is 0.0040921972336814706
testing: bce: 25.570892, dice: 146.810530, loss: 25.570892
IoU in current test batch is 0.8663081633584012
training: bce: 0.028716, dice: 0.164954, loss: 0.028716
training IoU in current batch 23200 is 0.8884678047743855
training IoU uptillnow 23200 is 0.004093706400485447
testing: bce: 25.624691, dice: 147.195648, loss: 25.624691
IoU in current test batch is 0.8161112006261059
Epoch 23261: reducing learning rate of group 0 to 8.0243e-04.
training: bce: 0.028653, dice: 0.164713, loss: 0.028653
training IoU in current batch 23300 is 0.8509916935073741
training IoU uptillnow 23300 is 0.004094398439741494
testing: bce: 25.678347, dice: 147.614587, loss: 25.678347
IoU in current test batch is 0.8888115692548949
Epoch 23362: reducing learning rate of group 0 to 8.0163e-04.
training: bce: 0.028639, dice: 0.164569, loss: 0.028639
training IoU in current batch 23400 is 0.8495666284085962
training IoU uptillnow 23400 is 0.004095054115577148
testing: bce: 25.776611, dice: 148.118406, loss: 25.776611
IoU in current test batch is 0.8082935644155465
Epoch 23463: reducing learning rate of group 0 to 8.0083e-04.
training: bce: 0.028586, dice: 0.164367, loss: 0.028586
training IoU in current batch 23500 is 0.9130801348964188
training IoU uptillnow 23500 is 0.004097055505130381
testing: bce: 25.838192, dice: 148.569220, loss: 25.838192
IoU in current test batch is 0.8814940076282111
Epoch 23564: reducing learning rate of group 0 to 8.0003e-04.
training: bce: 0.028543, dice: 0.164184, loss: 0.028543
training IoU in current batch 23600 is 0.9433923844430869
training IoU uptillnow 23600 is 0.004099682115939605
testing: bce: 25.909662, dice: 149.034620, loss: 25.909662
IoU in current test batch is 0.8468204914347933
Epoch 23665: reducing learning rate of group 0 to 7.9923e-04.
training: bce: 0.028484, dice: 0.163946, loss: 0.028484
training IoU in current batch 23700 is 0.9214615860537905
training IoU uptillnow 23700 is 0.004101823906641809
testing: bce: 25.965751, dice: 149.449066, loss: 25.965751
IoU in current test batch is 0.8921126551740386
Epoch 23766: reducing learning rate of group 0 to 7.9843e-04.
training: bce: 0.028442, dice: 0.163788, loss: 0.028442
training IoU in current batch 23800 is 0.7458852733257785
training IoU uptillnow 23800 is 0.004100259276836284
testing: bce: 26.036561, dice: 149.934961, loss: 26.036561
IoU in current test batch is 0.8695031051400518
Epoch 23867: reducing learning rate of group 0 to 7.9763e-04.
training: bce: 0.028381, dice: 0.163527, loss: 0.028381
training IoU in current batch 23900 is 0.9536616010530095
training IoU uptillnow 23900 is 0.004103054342852052
testing: bce: 26.089403, dice: 150.325707, loss: 26.089403
IoU in current test batch is 0.9001818689636351
Epoch 23968: reducing learning rate of group 0 to 7.9683e-04.
training: bce: 0.028321, dice: 0.163263, loss: 0.028321
training IoU in current batch 24000 is 0.8834816835890326
training IoU uptillnow 24000 is 0.004104364096925187
testing: bce: 26.143530, dice: 150.710741, loss: 26.143530
IoU in current test batch is 0.8306258048259522
Epoch 24069: reducing learning rate of group 0 to 7.9603e-04.
training: bce: 0.028260, dice: 0.163003, loss: 0.028260
training IoU in current batch 24100 is 0.9582213438735178
training IoU uptillnow 24100 is 0.004107213533141287
testing: bce: 26.196198, dice: 151.097177, loss: 26.196198
IoU in current test batch is 0.8446998012378598
Epoch 24170: reducing learning rate of group 0 to 7.9524e-04.
training: bce: 0.028197, dice: 0.162737, loss: 0.028197
training IoU in current batch 24200 is 0.8053001900261928
training IoU uptillnow 24200 is 0.004106880023852373
testing: bce: 26.245818, dice: 151.476673, loss: 26.245818
IoU in current test batch is 0.8411875736691881
Epoch 24285: reducing learning rate of group 0 to 7.9444e-04.
training: bce: 0.028133, dice: 0.162436, loss: 0.028133
training IoU in current batch 24300 is 0.9691864650827934
training IoU uptillnow 24300 is 0.004109921266194505
testing: bce: 26.294665, dice: 151.821521, loss: 26.294665
IoU in current test batch is 0.8705125629100864
Epoch 24386: reducing learning rate of group 0 to 7.9365e-04.
training: bce: 0.028077, dice: 0.162184, loss: 0.028077
training IoU in current batch 24400 is 0.7141523960617797
training IoU uptillnow 24400 is 0.004107711687546558
testing: bce: 26.349986, dice: 152.209878, loss: 26.349986
IoU in current test batch is 0.8865312269555125
Epoch 24487: reducing learning rate of group 0 to 7.9285e-04.
training: bce: 0.028032, dice: 0.161988, loss: 0.028032
training IoU in current batch 24500 is 0.874601441362743
training IoU uptillnow 24500 is 0.004108794482205009
testing: bce: 26.415991, dice: 152.648745, loss: 26.415991
IoU in current test batch is 0.8632571386201555
Epoch 24588: reducing learning rate of group 0 to 7.9206e-04.
training: bce: 0.027971, dice: 0.161717, loss: 0.027971
training IoU in current batch 24600 is 0.9072847682119205
training IoU uptillnow 24600 is 0.004110532742271082
testing: bce: 26.465620, dice: 153.015600, loss: 26.465620
IoU in current test batch is 0.8701955845312448
Epoch 24689: reducing learning rate of group 0 to 7.9127e-04.
training: bce: 0.027909, dice: 0.161436, loss: 0.027909
training IoU in current batch 24700 is 0.8626507050653943
training IoU uptillnow 24700 is 0.00411135344095962
testing: bce: 26.514312, dice: 153.369940, loss: 26.514312
IoU in current test batch is 0.8910732999341694
Epoch 24790: reducing learning rate of group 0 to 7.9048e-04.
training: bce: 0.027849, dice: 0.161176, loss: 0.027849
training IoU in current batch 24800 is 0.8949528108329914
training IoU uptillnow 24800 is 0.004112818747250517
testing: bce: 26.564734, dice: 153.743056, loss: 26.564734
IoU in current test batch is 0.8613664636549976
Epoch 24891: reducing learning rate of group 0 to 7.8969e-04.
training: bce: 0.027782, dice: 0.160887, loss: 0.027782
training IoU in current batch 24900 is 0.916636658264314
training IoU uptillnow 24900 is 0.004114707685622756
testing: bce: 26.608087, dice: 154.086373, loss: 26.608087
IoU in current test batch is 0.8709195169171485
Epoch 24992: reducing learning rate of group 0 to 7.8890e-04.
training: bce: 0.027726, dice: 0.160639, loss: 0.027726
training IoU in current batch 25000 is 0.8773669635506325
training IoU uptillnow 25000 is 0.004115796150612677
testing: bce: 26.660391, dice: 154.466834, loss: 26.660391
IoU in current test batch is 0.8579785497963998
Epoch 25093: reducing learning rate of group 0 to 7.8811e-04.
training: bce: 0.027683, dice: 0.160473, loss: 0.027683
training IoU in current batch 25100 is 0.9115216149304894
training IoU uptillnow 25100 is 0.004117556287356392
testing: bce: 26.725747, dice: 154.924299, loss: 26.725747
IoU in current test batch is 0.805533325742377
Epoch 25194: reducing learning rate of group 0 to 7.8732e-04.
training: bce: 0.027629, dice: 0.160251, loss: 0.027629
training IoU in current batch 25200 is 0.8442494105318313
training IoU uptillnow 25200 is 0.004117967742319698
testing: bce: 26.779943, dice: 155.326771, loss: 26.779943
IoU in current test batch is 0.8921947848427567
Epoch 25295: reducing learning rate of group 0 to 7.8653e-04.
training: bce: 0.027572, dice: 0.160033, loss: 0.027572
training IoU in current batch 25300 is 0.9218434173292009
training IoU uptillnow 25300 is 0.004119909362588961
testing: bce: 26.830517, dice: 155.730250, loss: 26.830517
IoU in current test batch is 0.872370850257852
Epoch 25396: reducing learning rate of group 0 to 7.8575e-04.
training: bce: 0.027525, dice: 0.159865, loss: 0.027525
training IoU in current batch 25400 is 0.9006238859180036
training IoU uptillnow 25400 is 0.004121418004244806
testing: bce: 26.890646, dice: 156.182304, loss: 26.890646
IoU in current test batch is 0.8779350504146245
Epoch 25497: reducing learning rate of group 0 to 7.8496e-04.
training: bce: 0.027467, dice: 0.159636, loss: 0.027467
training IoU in current batch 25500 is 0.8905301583259052
training IoU uptillnow 25500 is 0.004122716905414896
testing: bce: 26.939605, dice: 156.572280, loss: 26.939605
IoU in current test batch is 0.8547092922400398
Epoch 25598: reducing learning rate of group 0 to 7.8418e-04.
training: bce: 0.027432, dice: 0.159403, loss: 0.027432
training IoU in current batch 25600 is 0.29494563028594445
training IoU uptillnow 25600 is 0.004112373603379877
testing: bce: 27.011378, dice: 156.956532, loss: 27.011378
IoU in current test batch is 0.8643304118729463
Epoch 25699: reducing learning rate of group 0 to 7.8339e-04.
training: bce: 0.027391, dice: 0.159263, loss: 0.027391
training IoU in current batch 25700 is 0.9510752116767633
training IoU uptillnow 25700 is 0.004114875461109164
testing: bce: 27.075804, dice: 157.431635, loss: 27.075804
IoU in current test batch is 0.8676597844055234
Epoch 25800: reducing learning rate of group 0 to 7.8261e-04.
training: bce: 0.027330, dice: 0.159000, loss: 0.027330
training IoU in current batch 25800 is 0.8773904491312425
training IoU uptillnow 25800 is 0.004115929981416699
testing: bce: 27.120856, dice: 157.783322, loss: 27.120856
IoU in current test batch is 0.8577440197391637
training: bce: 0.027321, dice: 0.158910, loss: 0.027321
training IoU in current batch 25900 is 0.9574368568755847
training IoU uptillnow 25900 is 0.004118521596809777
testing: bce: 27.217174, dice: 158.304738, loss: 27.217174
IoU in current test batch is 0.871773143089456
Epoch 25901: reducing learning rate of group 0 to 7.8183e-04.
training: bce: 0.027263, dice: 0.158660, loss: 0.027263
training IoU in current batch 26000 is 0.6267170695346609
training IoU uptillnow 26000 is 0.00411473352616197
testing: bce: 27.264251, dice: 158.666314, loss: 27.264251
IoU in current test batch is 0.9024549538024197
Epoch 26002: reducing learning rate of group 0 to 7.8104e-04.
training: bce: 0.027206, dice: 0.158394, loss: 0.027206
training IoU in current batch 26100 is 0.9034021041125837
training IoU uptillnow 26100 is 0.0041162747582772176
testing: bce: 27.312174, dice: 159.009523, loss: 27.312174
IoU in current test batch is 0.8706972384324361
Epoch 26103: reducing learning rate of group 0 to 7.8026e-04.
training: bce: 0.027170, dice: 0.158239, loss: 0.027170
training IoU in current batch 26200 is 0.6927144759368461
training IoU uptillnow 26200 is 0.004113783622906075
testing: bce: 27.379848, dice: 159.462384, loss: 27.379848
IoU in current test batch is 0.8708065701666821
Epoch 26204: reducing learning rate of group 0 to 7.7948e-04.
training: bce: 0.027137, dice: 0.158067, loss: 0.027137
training IoU in current batch 26300 is 0.9440812112869924
training IoU uptillnow 26300 is 0.004116090084384836
testing: bce: 27.451168, dice: 159.897126, loss: 27.451168
IoU in current test batch is 0.8687100311759457
Epoch 26305: reducing learning rate of group 0 to 7.7870e-04.
training: bce: 0.027099, dice: 0.157879, loss: 0.027099
training IoU in current batch 26400 is 0.7451908671420866
training IoU uptillnow 26400 is 0.0041146123534326964
testing: bce: 27.517280, dice: 160.313668, loss: 27.517280
IoU in current test batch is 0.8801837839541089
Epoch 26406: reducing learning rate of group 0 to 7.7792e-04.
training: bce: 0.027058, dice: 0.157685, loss: 0.027058
training IoU in current batch 26500 is 0.8468104985984881
training IoU uptillnow 26500 is 0.004115063053932904
testing: bce: 27.579314, dice: 160.723286, loss: 27.579314
IoU in current test batch is 0.8787927829895005
Epoch 26507: reducing learning rate of group 0 to 7.7715e-04.
training: bce: 0.027010, dice: 0.157476, loss: 0.027010
training IoU in current batch 26600 is 0.8883525974850756
training IoU uptillnow 26600 is 0.004116291203000579
testing: bce: 27.634796, dice: 161.116575, loss: 27.634796
IoU in current test batch is 0.8822289265798433
Epoch 26608: reducing learning rate of group 0 to 7.7637e-04.
training: bce: 0.026958, dice: 0.157253, loss: 0.026958
training IoU in current batch 26700 is 0.8928327645051195
training IoU uptillnow 26700 is 0.004117594047910976
testing: bce: 27.685164, dice: 161.493026, loss: 27.685164
IoU in current test batch is 0.8452640274388155
Epoch 26709: reducing learning rate of group 0 to 7.7559e-04.
training: bce: 0.026912, dice: 0.157038, loss: 0.026912
training IoU in current batch 26800 is 0.8809421019811003
training IoU uptillnow 26800 is 0.004118665338019534
testing: bce: 27.740782, dice: 161.876034, loss: 27.740782
IoU in current test batch is 0.8224571412698093
Epoch 26810: reducing learning rate of group 0 to 7.7482e-04.
training: bce: 0.026863, dice: 0.156817, loss: 0.026863
training IoU in current batch 26900 is 0.9277506273940034
training IoU uptillnow 26900 is 0.004120598678040167
testing: bce: 27.794242, dice: 162.250902, loss: 27.794242
IoU in current test batch is 0.8668004588911604
Epoch 26911: reducing learning rate of group 0 to 7.7404e-04.
training: bce: 0.026810, dice: 0.156591, loss: 0.026810
training IoU in current batch 27000 is 0.8244805911424595
training IoU uptillnow 27000 is 0.004120605360302572
testing: bce: 27.842543, dice: 162.620163, loss: 27.842543
IoU in current test batch is 0.8940760908220565
Epoch 27012: reducing learning rate of group 0 to 7.7327e-04.
training: bce: 0.026756, dice: 0.156343, loss: 0.026756
training IoU in current batch 27100 is 0.8823706377858003
training IoU uptillnow 27100 is 0.004121680035881431
testing: bce: 27.888904, dice: 162.963374, loss: 27.888904
IoU in current test batch is 0.8458066640977694
Epoch 27113: reducing learning rate of group 0 to 7.7250e-04.
training: bce: 0.026699, dice: 0.156109, loss: 0.026699
training IoU in current batch 27200 is 0.8639811015790128
training IoU uptillnow 27200 is 0.004122408779207094
testing: bce: 27.932707, dice: 163.320087, loss: 27.932707
IoU in current test batch is 0.8458826083240829
Epoch 27214: reducing learning rate of group 0 to 7.7172e-04.
training: bce: 0.026644, dice: 0.155865, loss: 0.026644
training IoU in current batch 27300 is 0.8514720476347999
training IoU uptillnow 27300 is 0.004122903088789039
testing: bce: 27.977302, dice: 163.664056, loss: 27.977302
IoU in current test batch is 0.8757702104031154
Epoch 27315: reducing learning rate of group 0 to 7.7095e-04.
training: bce: 0.026627, dice: 0.155731, loss: 0.026627
training IoU in current batch 27400 is 0.9344889851932106
training IoU uptillnow 27400 is 0.004124908642736622
testing: bce: 28.061978, dice: 164.122339, loss: 28.061978
IoU in current test batch is 0.8894108982778898
Epoch 27416: reducing learning rate of group 0 to 7.7018e-04.
training: bce: 0.026574, dice: 0.155482, loss: 0.026574
training IoU in current batch 27500 is 0.8567777115106074
training IoU uptillnow 27500 is 0.004125486730496399
testing: bce: 28.108061, dice: 164.458157, loss: 28.108061
IoU in current test batch is 0.8712130174746804
Epoch 27517: reducing learning rate of group 0 to 7.6941e-04.
training: bce: 0.026544, dice: 0.155365, loss: 0.026544
training IoU in current batch 27600 is 0.9186768816647686
training IoU uptillnow 27600 is 0.004127181950516787
testing: bce: 28.178979, dice: 164.931958, loss: 28.178979
IoU in current test batch is 0.8769122107085541
Epoch 27618: reducing learning rate of group 0 to 7.6864e-04.
training: bce: 0.026496, dice: 0.155172, loss: 0.026496
training IoU in current batch 27700 is 0.9050413443584956
training IoU uptillnow 27700 is 0.004128618811176242
testing: bce: 28.229359, dice: 165.323509, loss: 28.229359
IoU in current test batch is 0.8505754203705151
Epoch 27719: reducing learning rate of group 0 to 7.6787e-04.
training: bce: 0.026497, dice: 0.155128, loss: 0.026497
training IoU in current batch 27800 is 0.8767258957180308
training IoU uptillnow 27800 is 0.004129536082739906
testing: bce: 28.332083, dice: 165.874113, loss: 28.332083
IoU in current test batch is 0.8483485583100313
Epoch 27820: reducing learning rate of group 0 to 7.6710e-04.
training: bce: 0.026460, dice: 0.155006, loss: 0.026460
training IoU in current batch 27900 is 0.6795007141526423
training IoU uptillnow 27900 is 0.004126912404334197
testing: bce: 28.395111, dice: 166.339460, loss: 28.395111
IoU in current test batch is 0.8812004322225189
Epoch 27921: reducing learning rate of group 0 to 7.6634e-04.
training: bce: 0.026417, dice: 0.154836, loss: 0.026417
training IoU in current batch 28000 is 0.9302679658952497
training IoU uptillnow 28000 is 0.004128785292535126
testing: bce: 28.450548, dice: 166.751892, loss: 28.450548
IoU in current test batch is 0.8670560535151215
Epoch 28022: reducing learning rate of group 0 to 7.6557e-04.
training: bce: 0.026377, dice: 0.154663, loss: 0.026377
training IoU in current batch 28100 is 0.9051346902725922
training IoU uptillnow 28100 is 0.004130197655649705
testing: bce: 28.508593, dice: 167.160683, loss: 28.508593
IoU in current test batch is 0.8694374734063116
Epoch 28123: reducing learning rate of group 0 to 7.6481e-04.
training: bce: 0.026331, dice: 0.154464, loss: 0.026331
training IoU in current batch 28200 is 0.922642598087702
training IoU uptillnow 28200 is 0.004131910415249679
testing: bce: 28.560312, dice: 167.540430, loss: 28.560312
IoU in current test batch is 0.857780648972943
Epoch 28224: reducing learning rate of group 0 to 7.6404e-04.
training: bce: 0.026303, dice: 0.154328, loss: 0.026303
training IoU in current batch 28300 is 0.8768424218985003
training IoU uptillnow 28300 is 0.00413280190916948
testing: bce: 28.631081, dice: 167.985841, loss: 28.631081
IoU in current test batch is 0.8731262159140898
Epoch 28325: reducing learning rate of group 0 to 7.6328e-04.
training: bce: 0.026269, dice: 0.154149, loss: 0.026269
training IoU in current batch 28400 is 0.9316758026624902
training IoU uptillnow 28400 is 0.004134652467615108
testing: bce: 28.694311, dice: 168.383998, loss: 28.694311
IoU in current test batch is 0.8686316178963869
Epoch 28426: reducing learning rate of group 0 to 7.6251e-04.
training: bce: 0.026228, dice: 0.153978, loss: 0.026228
training IoU in current batch 28500 is 0.9235836059927673
training IoU uptillnow 28500 is 0.004136348076759871
testing: bce: 28.750434, dice: 168.789417, loss: 28.750434
IoU in current test batch is 0.8805280076850098
Epoch 28527: reducing learning rate of group 0 to 7.6175e-04.
training: bce: 0.026179, dice: 0.153770, loss: 0.026179
training IoU in current batch 28600 is 0.8309543341536779
training IoU uptillnow 28600 is 0.0041364124926684355
testing: bce: 28.798087, dice: 169.152502, loss: 28.798087
IoU in current test batch is 0.8754012055619197
Epoch 28628: reducing learning rate of group 0 to 7.6099e-04.
training: bce: 0.026131, dice: 0.153544, loss: 0.026131
training IoU in current batch 28700 is 0.8947836772744607
training IoU uptillnow 28700 is 0.004137588430418702
testing: bce: 28.845751, dice: 169.494890, loss: 28.845751
IoU in current test batch is 0.8330619859705929
Epoch 28729: reducing learning rate of group 0 to 7.6023e-04.
training: bce: 0.026116, dice: 0.153460, loss: 0.026116
training IoU in current batch 28800 is 0.8818999826959681
training IoU uptillnow 28800 is 0.004138532534731264
testing: bce: 28.929053, dice: 169.992623, loss: 28.929053
IoU in current test batch is 0.8837122273509167
Epoch 28830: reducing learning rate of group 0 to 7.5947e-04.
training: bce: 0.026064, dice: 0.153224, loss: 0.026064
training IoU in current batch 28900 is 0.9116028804919492
training IoU uptillnow 28900 is 0.004139983978860285
testing: bce: 28.972566, dice: 170.320334, loss: 28.972566
IoU in current test batch is 0.8955583941413647
Epoch 28931: reducing learning rate of group 0 to 7.5871e-04.
training: bce: 0.026028, dice: 0.153051, loss: 0.026028
training IoU in current batch 29000 is 0.851910088267877
training IoU uptillnow 29000 is 0.004140396262790078
testing: bce: 29.031757, dice: 170.716983, loss: 29.031757
IoU in current test batch is 0.8671251058479815
Epoch 29032: reducing learning rate of group 0 to 7.5795e-04.
training: bce: 0.026002, dice: 0.152944, loss: 0.026002
training IoU in current batch 29100 is 0.9126021381519743
training IoU uptillnow 29100 is 0.00414184849614278
testing: bce: 29.103187, dice: 171.185165, loss: 29.103187
IoU in current test batch is 0.8834283249048492
Epoch 29133: reducing learning rate of group 0 to 7.5719e-04.
training: bce: 0.025953, dice: 0.152725, loss: 0.025953
training IoU in current batch 29200 is 0.8981293425975414
training IoU uptillnow 29200 is 0.004143042969677402
testing: bce: 29.147703, dice: 171.528007, loss: 29.147703
IoU in current test batch is 0.8855795855744149
Epoch 29234: reducing learning rate of group 0 to 7.5643e-04.
training: bce: 0.025904, dice: 0.152510, loss: 0.025904
training IoU in current batch 29300 is 0.9409855072463769
training IoU uptillnow 29300 is 0.0041449605989957
testing: bce: 29.192727, dice: 171.873087, loss: 29.192727
IoU in current test batch is 0.8519544186128593
Epoch 29335: reducing learning rate of group 0 to 7.5568e-04.
training: bce: 0.025858, dice: 0.152301, loss: 0.025858
training IoU in current batch 29400 is 0.9086600221483943
training IoU uptillnow 29400 is 0.0041463154492108155
testing: bce: 29.239954, dice: 172.222906, loss: 29.239954
IoU in current test batch is 0.887390984704439
Epoch 29436: reducing learning rate of group 0 to 7.5492e-04.
training: bce: 0.025821, dice: 0.152138, loss: 0.025821
training IoU in current batch 29500 is 0.9266589521567375
training IoU uptillnow 29500 is 0.004147966170581525
testing: bce: 29.298366, dice: 172.624349, loss: 29.298366
IoU in current test batch is 0.8788903544870544
Epoch 29537: reducing learning rate of group 0 to 7.5417e-04.
training: bce: 0.025774, dice: 0.151944, loss: 0.025774
training IoU in current batch 29600 is 0.9116230183060349
training IoU uptillnow 29600 is 0.004149351762017452
testing: bce: 29.343798, dice: 172.988482, loss: 29.343798
IoU in current test batch is 0.8540281195968805
Epoch 29638: reducing learning rate of group 0 to 7.5341e-04.
training: bce: 0.025732, dice: 0.151770, loss: 0.025732
training IoU in current batch 29700 is 0.8651098315205801
training IoU uptillnow 29700 is 0.004149944999267327
testing: bce: 29.394634, dice: 173.374287, loss: 29.394634
IoU in current test batch is 0.8950962258724006
Epoch 29739: reducing learning rate of group 0 to 7.5266e-04.
training: bce: 0.025689, dice: 0.151587, loss: 0.025689
training IoU in current batch 29800 is 0.9363406740863525
training IoU uptillnow 29800 is 0.004151729363453645
testing: bce: 29.444820, dice: 173.747398, loss: 29.444820
IoU in current test batch is 0.8577238085265125
Epoch 29840: reducing learning rate of group 0 to 7.5191e-04.
training: bce: 0.025669, dice: 0.151489, loss: 0.025669
training IoU in current batch 29900 is 0.9392406139716825
training IoU uptillnow 29900 is 0.0041535502848489315
testing: bce: 29.519874, dice: 174.218749, loss: 29.519874
IoU in current test batch is 0.8507092938730637
Epoch 29941: reducing learning rate of group 0 to 7.5116e-04.
training: bce: 0.025645, dice: 0.151338, loss: 0.025645
training IoU in current batch 30000 is 0.9272839998228599
training IoU uptillnow 30000 is 0.004155159796912747
testing: bce: 29.591790, dice: 174.626274, loss: 29.591790
IoU in current test batch is 0.878717114045088
Epoch 30042: reducing learning rate of group 0 to 7.5040e-04.
training: bce: 0.025654, dice: 0.151247, loss: 0.025654
training IoU in current batch 30100 is 0.7111389497859902
training IoU uptillnow 30100 is 0.0041531682848434375
testing: bce: 29.700566, dice: 175.103816, loss: 29.700566
IoU in current test batch is 0.854779289024738
Epoch 30143: reducing learning rate of group 0 to 7.4965e-04.
training: bce: 0.025614, dice: 0.151100, loss: 0.025614
training IoU in current batch 30200 is 0.8503778121243345
training IoU uptillnow 30200 is 0.004153495164005645
testing: bce: 29.752640, dice: 175.514344, loss: 29.752640
IoU in current test batch is 0.8741137439305835
Epoch 30244: reducing learning rate of group 0 to 7.4890e-04.
training: bce: 0.025569, dice: 0.150900, loss: 0.025569
training IoU in current batch 30300 is 0.9472586106645688
training IoU uptillnow 30300 is 0.004155418525905639
testing: bce: 29.798598, dice: 175.862172, loss: 29.798598
IoU in current test batch is 0.8585156728107184
Epoch 30345: reducing learning rate of group 0 to 7.4816e-04.
training: bce: 0.025522, dice: 0.150684, loss: 0.025522
training IoU in current batch 30400 is 0.902550057367038
training IoU uptillnow 30400 is 0.0041565939206654485
testing: bce: 29.842650, dice: 176.190551, loss: 29.842650
IoU in current test batch is 0.888920452607895
Epoch 30446: reducing learning rate of group 0 to 7.4741e-04.
training: bce: 0.025494, dice: 0.150547, loss: 0.025494
training IoU in current batch 30500 is 0.9583045115671355
training IoU uptillnow 30500 is 0.004158675585650761
testing: bce: 29.907269, dice: 176.609187, loss: 29.907269
IoU in current test batch is 0.873309815480829
Epoch 30547: reducing learning rate of group 0 to 7.4666e-04.
training: bce: 0.025451, dice: 0.150377, loss: 0.025451
training IoU in current batch 30600 is 0.9447119375726383
training IoU uptillnow 30600 is 0.0041605215518028885
testing: bce: 29.954514, dice: 176.988318, loss: 29.954514
IoU in current test batch is 0.8589483467958465
Epoch 30648: reducing learning rate of group 0 to 7.4591e-04.
training: bce: 0.025408, dice: 0.150188, loss: 0.025408
training IoU in current batch 30700 is 0.9208062418725618
training IoU uptillnow 30700 is 0.00416196616161221
testing: bce: 30.001644, dice: 177.343502, loss: 30.001644
IoU in current test batch is 0.8525177386559288
Epoch 30749: reducing learning rate of group 0 to 7.4517e-04.
training: bce: 0.025361, dice: 0.149961, loss: 0.025361
training IoU in current batch 30800 is 0.9322318568861808
training IoU uptillnow 30800 is 0.00416358686588421
testing: bce: 30.043568, dice: 177.651727, loss: 30.043568
IoU in current test batch is 0.8816776611480678
Epoch 30850: reducing learning rate of group 0 to 7.4442e-04.
training: bce: 0.025329, dice: 0.149829, loss: 0.025329
training IoU in current batch 30900 is 0.890202964416671
training IoU uptillnow 30900 is 0.004164517023342543
testing: bce: 30.103218, dice: 178.071196, loss: 30.103218
IoU in current test batch is 0.8309655096477838
Epoch 30951: reducing learning rate of group 0 to 7.4368e-04.
training: bce: 0.025302, dice: 0.149688, loss: 0.025302
training IoU in current batch 31000 is 0.8669840948232196
training IoU uptillnow 31000 is 0.004165066694162108
testing: bce: 30.168962, dice: 178.479598, loss: 30.168962
IoU in current test batch is 0.8500581292903099
Epoch 31052: reducing learning rate of group 0 to 7.4293e-04.
training: bce: 0.025261, dice: 0.149506, loss: 0.025261
training IoU in current batch 31100 is 0.9320127520865422
training IoU uptillnow 31100 is 0.004166658273424096
testing: bce: 30.216631, dice: 178.838391, loss: 30.216631
IoU in current test batch is 0.8550618121060448
Epoch 31153: reducing learning rate of group 0 to 7.4219e-04.
training: bce: 0.025218, dice: 0.149336, loss: 0.025218
training IoU in current batch 31200 is 0.8528624933119315
training IoU uptillnow 31200 is 0.00416697125760132
testing: bce: 30.262909, dice: 179.209184, loss: 30.262909
IoU in current test batch is 0.8685106124493114
Epoch 31254: reducing learning rate of group 0 to 7.4145e-04.
training: bce: 0.025184, dice: 0.149185, loss: 0.025184
training IoU in current batch 31300 is 0.9401232006638792
training IoU uptillnow 31300 is 0.004168676138422118
testing: bce: 30.319014, dice: 179.601783, loss: 30.319014
IoU in current test batch is 0.8482272515908753
Epoch 31355: reducing learning rate of group 0 to 7.4071e-04.
training: bce: 0.025144, dice: 0.148993, loss: 0.025144
training IoU in current batch 31400 is 0.8293309361756774
training IoU uptillnow 31400 is 0.004168606008625157
testing: bce: 30.367118, dice: 179.943119, loss: 30.367118
IoU in current test batch is 0.8855740706956852
Epoch 31456: reducing learning rate of group 0 to 7.3997e-04.
training: bce: 0.025104, dice: 0.148825, loss: 0.025104
training IoU in current batch 31500 is 0.932713274887633
training IoU uptillnow 31500 is 0.0041701772614927265
testing: bce: 30.415803, dice: 180.313467, loss: 30.415803
IoU in current test batch is 0.8911320565393844
Epoch 31557: reducing learning rate of group 0 to 7.3923e-04.
training: bce: 0.025065, dice: 0.148643, loss: 0.025065
training IoU in current batch 31600 is 0.9183533290594377
training IoU uptillnow 31600 is 0.0041715113628939616
testing: bce: 30.464765, dice: 180.663952, loss: 30.464765
IoU in current test batch is 0.8539122221603775
Epoch 31658: reducing learning rate of group 0 to 7.3849e-04.
training: bce: 0.025023, dice: 0.148457, loss: 0.025023
training IoU in current batch 31700 is 0.9200773174216321
training IoU uptillnow 31700 is 0.004172864238904859
testing: bce: 30.509465, dice: 181.008525, loss: 30.509465
IoU in current test batch is 0.8716265759020249
Epoch 31759: reducing learning rate of group 0 to 7.3775e-04.
training: bce: 0.025000, dice: 0.148367, loss: 0.025000
training IoU in current batch 31800 is 0.8984778177718028
training IoU uptillnow 31800 is 0.0041738690024341624
testing: bce: 30.578087, dice: 181.469624, loss: 30.578087
IoU in current test batch is 0.8717160835810212
Epoch 31860: reducing learning rate of group 0 to 7.3701e-04.
training: bce: 0.024985, dice: 0.148293, loss: 0.024985
training IoU in current batch 31900 is 0.9434223611308424
training IoU uptillnow 31900 is 0.0041755719045476385
testing: bce: 30.655604, dice: 181.949818, loss: 30.655604
IoU in current test batch is 0.8349163626578515
Epoch 31961: reducing learning rate of group 0 to 7.3627e-04.
training: bce: 0.024966, dice: 0.148201, loss: 0.024966
training IoU in current batch 32000 is 0.936416819012797
training IoU uptillnow 32000 is 0.004177154705680467
testing: bce: 30.728855, dice: 182.406840, loss: 30.728855
IoU in current test batch is 0.8549576681633915
Epoch 32062: reducing learning rate of group 0 to 7.3554e-04.
training: bce: 0.024935, dice: 0.148066, loss: 0.024935
training IoU in current batch 32100 is 0.8354318981857568
training IoU uptillnow 32100 is 0.004177154720587317
testing: bce: 30.785618, dice: 182.810449, loss: 30.785618
IoU in current test batch is 0.863667735865116
Epoch 32163: reducing learning rate of group 0 to 7.3480e-04.
training: bce: 0.024895, dice: 0.147882, loss: 0.024895
training IoU in current batch 32200 is 0.86064540232569
training IoU uptillnow 32200 is 0.004177546237282579
testing: bce: 30.833041, dice: 183.152323, loss: 30.833041
IoU in current test batch is 0.858146979923974
Epoch 32264: reducing learning rate of group 0 to 7.3407e-04.
training: bce: 0.024857, dice: 0.147733, loss: 0.024857
training IoU in current batch 32300 is 0.9230373831775701
training IoU uptillnow 32300 is 0.004178901120037309
testing: bce: 30.881108, dice: 183.535106, loss: 30.881108
IoU in current test batch is 0.8775636491735214
Epoch 32365: reducing learning rate of group 0 to 7.3333e-04.
training: bce: 0.024849, dice: 0.147659, loss: 0.024849
training IoU in current batch 32400 is 0.8595796086011114
training IoU uptillnow 32400 is 0.004179268383155633
testing: bce: 30.966416, dice: 184.011311, loss: 30.966416
IoU in current test batch is 0.889896655940867
Epoch 32466: reducing learning rate of group 0 to 7.3260e-04.
training: bce: 0.024810, dice: 0.147500, loss: 0.024810
training IoU in current batch 32500 is 0.9251137594799567
training IoU uptillnow 32500 is 0.004180641572947469
testing: bce: 31.014022, dice: 184.381283, loss: 31.014022
IoU in current test batch is 0.8861306370122358
training: bce: 2.316525, dice: 0.990226, loss: 2.316525
training IoU in current batch 0 is 0.0
training IoU uptillnow 0 is 0.0
testing: bce: 0.089097, dice: 0.038086, loss: 0.089097
IoU in current test batch is 0.0
Epoch 32567: reducing learning rate of group 0 to 7.3187e-04.
training: bce: 0.103842, dice: 0.507706, loss: 0.103842
training IoU in current batch 100 is 0.8776952606465038
training IoU uptillnow 100 is 1.344117460675514e-05
testing: bce: 0.403386, dice: 1.972242, loss: 0.403386
IoU in current test batch is 0.8380671592992248
Epoch 32668: reducing learning rate of group 0 to 7.3114e-04.
training: bce: 0.062759, dice: 0.327993, loss: 0.062759
training IoU in current batch 200 is 0.7425319296745578
training IoU uptillnow 200 is 2.47366706410947e-05
testing: bce: 0.485178, dice: 2.535639, loss: 0.485178
IoU in current test batch is 0.8270149567956964
Epoch 32769: reducing learning rate of group 0 to 7.3040e-04.
training: bce: 0.048675, dice: 0.263674, loss: 0.048675
training IoU in current batch 300 is 0.8971882890875589
training IoU uptillnow 300 is 3.831740938840196e-05
testing: bce: 0.563510, dice: 3.052534, loss: 0.563510
IoU in current test batch is 0.811809341378476
Epoch 32870: reducing learning rate of group 0 to 7.2967e-04.
training: bce: 0.040172, dice: 0.225489, loss: 0.040172
training IoU in current batch 400 is 0.603988399985679
training IoU uptillnow 400 is 4.736648324548627e-05
testing: bce: 0.619568, dice: 3.477739, loss: 0.619568
IoU in current test batch is 0.8688956863307784
Epoch 32971: reducing learning rate of group 0 to 7.2894e-04.
training: bce: 0.034841, dice: 0.201378, loss: 0.034841
training IoU in current batch 500 is 0.9003602423448501
training IoU uptillnow 500 is 6.0844553196555915e-05
testing: bce: 0.671356, dice: 3.880401, loss: 0.671356
IoU in current test batch is 0.8662249154150737
Epoch 33072: reducing learning rate of group 0 to 7.2822e-04.
training: bce: 0.031364, dice: 0.185370, loss: 0.031364
training IoU in current batch 600 is 0.9296438033559022
training IoU uptillnow 600 is 7.468299559714402e-05
testing: bce: 0.724999, dice: 4.284895, loss: 0.724999
IoU in current test batch is 0.8683770197978793
Epoch 33173: reducing learning rate of group 0 to 7.2749e-04.
training: bce: 0.028720, dice: 0.173165, loss: 0.028720
training IoU in current batch 700 is 0.9235666122209658
training IoU uptillnow 700 is 8.834681028761362e-05
testing: bce: 0.774344, dice: 4.668787, loss: 0.774344
IoU in current test batch is 0.8340148326630824
Epoch 33274: reducing learning rate of group 0 to 7.2676e-04.
training: bce: 0.027500, dice: 0.167245, loss: 0.027500
training IoU in current batch 800 is 0.9289145052833814
training IoU uptillnow 800 is 0.00010200886134124048
testing: bce: 0.847207, dice: 5.152435, loss: 0.847207
IoU in current test batch is 0.7963832600194347
Epoch 33375: reducing learning rate of group 0 to 7.2603e-04.
training: bce: 0.025906, dice: 0.160086, loss: 0.025906
training IoU in current batch 900 is 0.8552936311000827
training IoU uptillnow 900 is 0.00011448874682281471
testing: bce: 0.897743, dice: 5.547589, loss: 0.897743
IoU in current test batch is 0.8690631610511426
Epoch 33476: reducing learning rate of group 0 to 7.2531e-04.
training: bce: 0.024610, dice: 0.153534, loss: 0.024610
training IoU in current batch 1000 is 0.9257061076735278
training IoU uptillnow 1000 is 0.00012794361736200258
testing: bce: 0.947468, dice: 5.911051, loss: 0.947468
IoU in current test batch is 0.8816112285507245
Epoch 33577: reducing learning rate of group 0 to 7.2458e-04.
training: bce: 0.023954, dice: 0.150302, loss: 0.023954
training IoU in current batch 1100 is 0.9464444530373922
training IoU uptillnow 1100 is 0.00014162666955542285
testing: bce: 1.014375, dice: 6.364715, loss: 1.014375
IoU in current test batch is 0.8674130515528946
Epoch 33678: reducing learning rate of group 0 to 7.2386e-04.
training: bce: 0.022997, dice: 0.145489, loss: 0.022997
training IoU in current batch 1200 is 0.9522284079246105
training IoU uptillnow 1200 is 0.00015531432528385626
testing: bce: 1.062270, dice: 6.720480, loss: 1.062270
IoU in current test batch is 0.8798040095895576
Epoch 33779: reducing learning rate of group 0 to 7.2313e-04.
training: bce: 0.022768, dice: 0.143291, loss: 0.022768
training IoU in current batch 1300 is 0.9068688742790304
training IoU uptillnow 1300 is 0.0001682510896263467
testing: bce: 1.139271, dice: 7.170056, loss: 1.139271
IoU in current test batch is 0.8546856755890369
Epoch 33880: reducing learning rate of group 0 to 7.2241e-04.
training: bce: 0.022195, dice: 0.141273, loss: 0.022195
training IoU in current batch 1400 is 0.841856846473029
training IoU uptillnow 1400 is 0.00018015416078420997
testing: bce: 1.195989, dice: 7.612466, loss: 1.195989
IoU in current test batch is 0.8681646851433727
Epoch 33981: reducing learning rate of group 0 to 7.2169e-04.
training: bce: 0.021588, dice: 0.138522, loss: 0.021588
training IoU in current batch 1500 is 0.8992017663043478
training IoU uptillnow 1500 is 0.00019282939733904198
testing: bce: 1.246282, dice: 7.996958, loss: 1.246282
IoU in current test batch is 0.8569196125717261
Epoch 34082: reducing learning rate of group 0 to 7.2097e-04.
training: bce: 0.021300, dice: 0.136904, loss: 0.021300
training IoU in current batch 1600 is 0.7500266666666666
training IoU uptillnow 1600 is 0.00020324625244964184
testing: bce: 1.311602, dice: 8.430150, loss: 1.311602
IoU in current test batch is 0.8432353818677897
Epoch 34183: reducing learning rate of group 0 to 7.2024e-04.
training: bce: 0.021013, dice: 0.136016, loss: 0.021013
training IoU in current batch 1700 is 0.872791183735512
training IoU uptillnow 1700 is 0.00021539448721577832
testing: bce: 1.374731, dice: 8.898584, loss: 1.374731
IoU in current test batch is 0.8891634065508547
Epoch 34284: reducing learning rate of group 0 to 7.1952e-04.
training: bce: 0.020773, dice: 0.134908, loss: 0.020773
training IoU in current batch 1800 is 0.9210850311767743
training IoU uptillnow 1800 is 0.00022817496631640015
testing: bce: 1.438944, dice: 9.344968, loss: 1.438944
IoU in current test batch is 0.8665199110014007
Epoch 34385: reducing learning rate of group 0 to 7.1881e-04.
training: bce: 0.020409, dice: 0.133616, loss: 0.020409
training IoU in current batch 1900 is 0.9520356087640536
training IoU uptillnow 1900 is 0.00024133046371840557
testing: bce: 1.492203, dice: 9.769404, loss: 1.492203
IoU in current test batch is 0.8412533771578651
Epoch 34486: reducing learning rate of group 0 to 7.1809e-04.
training: bce: 0.020071, dice: 0.132235, loss: 0.020071
training IoU in current batch 2000 is 0.922323488257762
training IoU uptillnow 2000 is 0.000253979813137559
testing: bce: 1.544665, dice: 10.177000, loss: 1.544665
IoU in current test batch is 0.8954533223297155
Epoch 34587: reducing learning rate of group 0 to 7.1737e-04.
training: bce: 0.020022, dice: 0.131653, loss: 0.020022
training IoU in current batch 2100 is 0.943335370408094
training IoU uptillnow 2100 is 0.0002668593555231718
testing: bce: 1.617939, dice: 10.638552, loss: 1.617939
IoU in current test batch is 0.8889303740188788
Epoch 34688: reducing learning rate of group 0 to 7.1665e-04.
training: bce: 0.019738, dice: 0.130397, loss: 0.019738
training IoU in current batch 2200 is 0.9070407761898518
training IoU uptillnow 2200 is 0.00027914253808817587
testing: bce: 1.670903, dice: 11.038602, loss: 1.670903
IoU in current test batch is 0.8710167044804972
Epoch 34789: reducing learning rate of group 0 to 7.1593e-04.
training: bce: 0.019617, dice: 0.130010, loss: 0.019617
training IoU in current batch 2300 is 0.8954874805099514
training IoU uptillnow 2300 is 0.0002911894680712792
testing: bce: 1.736144, dice: 11.505920, loss: 1.736144
IoU in current test batch is 0.8625604208849029
Epoch 34890: reducing learning rate of group 0 to 7.1522e-04.
training: bce: 0.019404, dice: 0.129275, loss: 0.019404
training IoU in current batch 2400 is 0.9055246812683884
training IoU uptillnow 2400 is 0.00030331105475569717
testing: bce: 1.791863, dice: 11.938007, loss: 1.791863
IoU in current test batch is 0.880058099390687
Epoch 34991: reducing learning rate of group 0 to 7.1450e-04.
training: bce: 0.019188, dice: 0.128171, loss: 0.019188
training IoU in current batch 2500 is 0.928326858497399
training IoU uptillnow 2500 is 0.0003156887583969226
testing: bce: 1.845783, dice: 12.329022, loss: 1.845783
IoU in current test batch is 0.8615375363379472
Epoch 35092: reducing learning rate of group 0 to 7.1379e-04.
training: bce: 0.018936, dice: 0.126884, loss: 0.018936
training IoU in current batch 2600 is 0.8570529533399263
training IoU uptillnow 2600 is 0.0003269821651546367
testing: bce: 1.894363, dice: 12.693304, loss: 1.894363
IoU in current test batch is 0.8641280235275677
Epoch 35193: reducing learning rate of group 0 to 7.1307e-04.
training: bce: 0.018750, dice: 0.125883, loss: 0.018750
training IoU in current batch 2700 is 0.9125511019219021
training IoU uptillnow 2700 is 0.0003389987138842779
testing: bce: 1.947814, dice: 13.077287, loss: 1.947814
IoU in current test batch is 0.859510220741336
Epoch 35294: reducing learning rate of group 0 to 7.1236e-04.
training: bce: 0.018609, dice: 0.125030, loss: 0.018609
training IoU in current batch 2800 is 0.9227344595156861
training IoU uptillnow 2800 is 0.0003510913137334813
testing: bce: 2.004782, dice: 13.469630, loss: 2.004782
IoU in current test batch is 0.8884127086877128
Epoch 35395: reducing learning rate of group 0 to 7.1165e-04.
training: bce: 0.018781, dice: 0.125398, loss: 0.018781
training IoU in current batch 2900 is 0.8565199097272692
training IoU uptillnow 2900 is 0.00036218176136998637
testing: bce: 2.095560, dice: 13.991487, loss: 2.095560
IoU in current test batch is 0.8373786274424726
Epoch 35496: reducing learning rate of group 0 to 7.1094e-04.
training: bce: 0.018606, dice: 0.124641, loss: 0.018606
training IoU in current batch 3000 is 0.924669012904307
training IoU uptillnow 3000 is 0.00037416832462165386
testing: bce: 2.147553, dice: 14.386457, loss: 2.147553
IoU in current test batch is 0.8696714415748036
Epoch 35597: reducing learning rate of group 0 to 7.1023e-04.
training: bce: 0.018733, dice: 0.124459, loss: 0.018733
training IoU in current batch 3100 is 0.917205801513806
training IoU uptillnow 3100 is 0.0003859829662939
testing: bce: 2.234212, dice: 14.844090, loss: 2.234212
IoU in current test batch is 0.8882193677276271
Epoch 35698: reducing learning rate of group 0 to 7.0952e-04.
training: bce: 0.018544, dice: 0.123641, loss: 0.018544
training IoU in current batch 3200 is 0.8844890682180726
training IoU uptillnow 3200 is 0.0003972739280550336
testing: bce: 2.283067, dice: 15.222069, loss: 2.283067
IoU in current test batch is 0.8632056477086716
Epoch 35799: reducing learning rate of group 0 to 7.0881e-04.
training: bce: 0.018438, dice: 0.123251, loss: 0.018438
training IoU in current batch 3300 is 0.7632907754833942
training IoU uptillnow 3300 is 0.0004068115225803741
testing: bce: 2.340977, dice: 15.648101, loss: 2.340977
IoU in current test batch is 0.8317056345430033
Epoch 35900: reducing learning rate of group 0 to 7.0810e-04.
training: bce: 0.018393, dice: 0.122871, loss: 0.018393
training IoU in current batch 3400 is 0.8810991268618388
training IoU uptillnow 3400 is 0.00041793458162633803
testing: bce: 2.405946, dice: 16.072491, loss: 2.405946
IoU in current test batch is 0.8677601554695765
Epoch 36001: reducing learning rate of group 0 to 7.0739e-04.
training: bce: 0.018202, dice: 0.121940, loss: 0.018202
training IoU in current batch 3500 is 0.9423015956884709
training IoU uptillnow 3500 is 0.0004298447978479667
testing: bce: 2.450940, dice: 16.419731, loss: 2.450940
IoU in current test batch is 0.8732294320038798
Epoch 36102: reducing learning rate of group 0 to 7.0668e-04.
training: bce: 0.018120, dice: 0.121558, loss: 0.018120
training IoU in current batch 3600 is 0.9197443738728143
training IoU uptillnow 3600 is 0.0004413771207611912
testing: bce: 2.509551, dice: 16.835802, loss: 2.509551
IoU in current test batch is 0.8855411178895723
Epoch 36203: reducing learning rate of group 0 to 7.0598e-04.
training: bce: 0.018158, dice: 0.121733, loss: 0.018158
training IoU in current batch 3700 is 0.8957145872915347
training IoU uptillnow 3700 is 0.0004525143662837404
testing: bce: 2.584763, dice: 17.328272, loss: 2.584763
IoU in current test batch is 0.8480834486047006
Epoch 36304: reducing learning rate of group 0 to 7.0527e-04.
training: bce: 0.018382, dice: 0.121694, loss: 0.018382
training IoU in current batch 3800 is 0.8474956217162872
training IoU uptillnow 3800 is 0.00046292706451149507
testing: bce: 2.687289, dice: 17.790752, loss: 2.687289
IoU in current test batch is 0.8565881781407867
Epoch 36405: reducing learning rate of group 0 to 7.0456e-04.
training: bce: 0.018347, dice: 0.121651, loss: 0.018347
training IoU in current batch 3900 is 0.9037513620090282
training IoU uptillnow 3900 is 0.0004740543220747912
testing: bce: 2.752730, dice: 18.252287, loss: 2.752730
IoU in current test batch is 0.8635161305226671
Epoch 36506: reducing learning rate of group 0 to 7.0386e-04.
training: bce: 0.018277, dice: 0.121306, loss: 0.018277
training IoU in current batch 4000 is 0.9359411373461922
training IoU uptillnow 4000 is 0.00048556104956670265
testing: bce: 2.812591, dice: 18.667128, loss: 2.812591
IoU in current test batch is 0.8556762055707986
Epoch 36607: reducing learning rate of group 0 to 7.0316e-04.
training: bce: 0.018191, dice: 0.121039, loss: 0.018191
training IoU in current batch 4100 is 0.9436519488921236
training IoU uptillnow 4100 is 0.0004971101803731091
testing: bce: 2.869354, dice: 19.091621, loss: 2.869354
IoU in current test batch is 0.8554382270404456
Epoch 36708: reducing learning rate of group 0 to 7.0245e-04.
training: bce: 0.018073, dice: 0.120489, loss: 0.018073
training IoU in current batch 4200 is 0.9443298282976175
training IoU uptillnow 4200 is 0.0005086056808863541
testing: bce: 2.920200, dice: 19.468205, loss: 2.920200
IoU in current test batch is 0.8759947217425311
Epoch 36809: reducing learning rate of group 0 to 7.0175e-04.
training: bce: 0.018023, dice: 0.120245, loss: 0.018023
training IoU in current batch 4300 is 0.895159523983354
training IoU uptillnow 4300 is 0.0005193716124160367
testing: bce: 2.981461, dice: 19.891334, loss: 2.981461
IoU in current test batch is 0.84570759708183
Epoch 36910: reducing learning rate of group 0 to 7.0105e-04.
training: bce: 0.017951, dice: 0.120053, loss: 0.017951
training IoU in current batch 4400 is 0.8782683065895577
training IoU uptillnow 4400 is 0.0005298506985214827
testing: bce: 3.038500, dice: 20.321304, loss: 3.038500
IoU in current test batch is 0.8665990971179351
Epoch 37011: reducing learning rate of group 0 to 7.0035e-04.
training: bce: 0.017989, dice: 0.120111, loss: 0.017989
training IoU in current batch 4500 is 0.8486495041147921
training IoU uptillnow 4500 is 0.0005398734972692458
testing: bce: 3.114160, dice: 20.793076, loss: 3.114160
IoU in current test batch is 0.8576320184794188
Epoch 37112: reducing learning rate of group 0 to 6.9965e-04.
training: bce: 0.017887, dice: 0.119794, loss: 0.017887
training IoU in current batch 4600 is 0.9413734008210115
training IoU uptillnow 4600 is 0.0005510903198559181
testing: bce: 3.165302, dice: 21.198982, loss: 3.165302
IoU in current test batch is 0.8618665427021864
Epoch 37213: reducing learning rate of group 0 to 6.9895e-04.
training: bce: 0.017806, dice: 0.119358, loss: 0.017806
training IoU in current batch 4700 is 0.744590994737184
training IoU uptillnow 4700 is 0.0005596055070499207
testing: bce: 3.219548, dice: 21.580765, loss: 3.219548
IoU in current test batch is 0.8885569859009459
Epoch 37314: reducing learning rate of group 0 to 6.9825e-04.
training: bce: 0.017691, dice: 0.118804, loss: 0.017691
training IoU in current batch 4800 is 0.8296911803192881
training IoU uptillnow 4800 is 0.000569214338211105
testing: bce: 3.266627, dice: 21.937539, loss: 3.266627
IoU in current test batch is 0.8695061776861739
Epoch 37415: reducing learning rate of group 0 to 6.9755e-04.
training: bce: 0.017573, dice: 0.118269, loss: 0.017573
training IoU in current batch 4900 is 0.9232259731312435
training IoU uptillnow 4900 is 0.0005800206654716695
testing: bce: 3.312461, dice: 22.293644, loss: 3.312461
IoU in current test batch is 0.879147076799635
Epoch 37516: reducing learning rate of group 0 to 6.9685e-04.
training: bce: 0.017601, dice: 0.117995, loss: 0.017601
training IoU in current batch 5000 is 0.9079346298898173
training IoU uptillnow 5000 is 0.0005905658191594081
testing: bce: 3.385566, dice: 22.695956, loss: 3.385566
IoU in current test batch is 0.8608053050596177
Epoch 37617: reducing learning rate of group 0 to 6.9616e-04.
training: bce: 0.017581, dice: 0.117678, loss: 0.017581
training IoU in current batch 5100 is 0.9095450959768604
training IoU uptillnow 5100 is 0.0006010763429664305
testing: bce: 3.449233, dice: 23.087558, loss: 3.449233
IoU in current test batch is 0.8687635461715211
Epoch 37718: reducing learning rate of group 0 to 6.9546e-04.
training: bce: 0.017501, dice: 0.117446, loss: 0.017501
training IoU in current batch 5200 is 0.9069628075957901
training IoU uptillnow 5200 is 0.0006114969781934204
testing: bce: 3.500950, dice: 23.493808, loss: 3.500950
IoU in current test batch is 0.8766212848460012
Epoch 37819: reducing learning rate of group 0 to 6.9476e-04.
training: bce: 0.017415, dice: 0.116979, loss: 0.017415
training IoU in current batch 5300 is 0.9132337471070903
training IoU uptillnow 5300 is 0.000621945390345079
testing: bce: 3.550679, dice: 23.850279, loss: 3.550679
IoU in current test batch is 0.8657388410625331
Epoch 37920: reducing learning rate of group 0 to 6.9407e-04.
training: bce: 0.017297, dice: 0.116491, loss: 0.017297
training IoU in current batch 5400 is 0.8773056913132588
training IoU uptillnow 5400 is 0.0006318653710199791
testing: bce: 3.593190, dice: 24.198837, loss: 3.593190
IoU in current test batch is 0.8454430896908197
Epoch 38021: reducing learning rate of group 0 to 6.9338e-04.
training: bce: 0.017199, dice: 0.116101, loss: 0.017199
training IoU in current batch 5500 is 0.8223029725382737
training IoU uptillnow 5500 is 0.0006410104307229224
testing: bce: 3.638889, dice: 24.564281, loss: 3.638889
IoU in current test batch is 0.8949520074358948
Epoch 38122: reducing learning rate of group 0 to 6.9268e-04.
training: bce: 0.017228, dice: 0.116043, loss: 0.017228
training IoU in current batch 5600 is 0.9344709559005903
training IoU uptillnow 5600 is 0.0006515776579441967
testing: bce: 3.711248, dice: 24.998284, loss: 3.711248
IoU in current test batch is 0.8562532261793712
Epoch 38223: reducing learning rate of group 0 to 6.9199e-04.
training: bce: 0.017215, dice: 0.115938, loss: 0.017215
training IoU in current batch 5700 is 0.8851194488130009
training IoU uptillnow 5700 is 0.0006614445047947982
testing: bce: 3.774790, dice: 25.421594, loss: 3.774790
IoU in current test batch is 0.8675154915392687
Epoch 38324: reducing learning rate of group 0 to 6.9130e-04.
training: bce: 0.017493, dice: 0.116339, loss: 0.017493
training IoU in current batch 5800 is 0.9531002733419652
training IoU uptillnow 5800 is 0.0006721462267518381
testing: bce: 3.902890, dice: 25.956979, loss: 3.902890
IoU in current test batch is 0.8930726236424837
Epoch 38425: reducing learning rate of group 0 to 6.9061e-04.
training: bce: 0.017400, dice: 0.115881, loss: 0.017400
training IoU in current batch 5900 is 0.8887147999153081
training IoU uptillnow 5900 is 0.0006819550091100604
testing: bce: 3.949031, dice: 26.300618, loss: 3.949031
IoU in current test batch is 0.8715439566535571
Epoch 38526: reducing learning rate of group 0 to 6.8992e-04.
training: bce: 0.017407, dice: 0.116142, loss: 0.017407
training IoU in current batch 6000 is 0.9294054248248608
training IoU uptillnow 6000 is 0.0006922406732951063
testing: bce: 4.017745, dice: 26.806529, loss: 4.017745
IoU in current test batch is 0.878054241240376
Epoch 38627: reducing learning rate of group 0 to 6.8923e-04.
training: bce: 0.017320, dice: 0.115680, loss: 0.017320
training IoU in current batch 6100 is 0.7567381738173817
training IoU uptillnow 6100 is 0.0007002393542503368
testing: bce: 4.064151, dice: 27.144799, loss: 4.064151
IoU in current test batch is 0.8649764027766099
Epoch 38728: reducing learning rate of group 0 to 6.8854e-04.
training: bce: 0.017308, dice: 0.115580, loss: 0.017308
training IoU in current batch 6200 is 0.9021341078418992
training IoU uptillnow 6200 is 0.0007100728519340725
testing: bce: 4.127872, dice: 27.565844, loss: 4.127872
IoU in current test batch is 0.9001337312025041
Epoch 38829: reducing learning rate of group 0 to 6.8785e-04.
training: bce: 0.017280, dice: 0.115427, loss: 0.017280
training IoU in current batch 6300 is 0.9001599001599001
training IoU uptillnow 6300 is 0.0007198303176643017
testing: bce: 4.187760, dice: 27.973347, loss: 4.187760
IoU in current test batch is 0.9042613086882423
Epoch 38930: reducing learning rate of group 0 to 6.8716e-04.
training: bce: 0.017377, dice: 0.115590, loss: 0.017377
training IoU in current batch 6400 is 0.9069673456191399
training IoU uptillnow 6400 is 0.0007296250683297311
testing: bce: 4.278192, dice: 28.457356, loss: 4.278192
IoU in current test batch is 0.816242889899094
Epoch 39031: reducing learning rate of group 0 to 6.8647e-04.
training: bce: 0.017460, dice: 0.115944, loss: 0.017460
training IoU in current batch 6500 is 0.8973336287761282
training IoU uptillnow 6500 is 0.0007392463005492241
testing: bce: 4.365602, dice: 28.990425, loss: 4.365602
IoU in current test batch is 0.8545708367115233
Epoch 39132: reducing learning rate of group 0 to 6.8579e-04.
training: bce: 0.017413, dice: 0.115721, loss: 0.017413
training IoU in current batch 6600 is 0.8974772305956314
training IoU uptillnow 6600 is 0.000748820215547957
testing: bce: 4.420879, dice: 29.379712, loss: 4.420879
IoU in current test batch is 0.8210963043205115
Epoch 39233: reducing learning rate of group 0 to 6.8510e-04.
training: bce: 0.017437, dice: 0.115879, loss: 0.017437
training IoU in current batch 6700 is 0.8995215311004785
training IoU uptillnow 6700 is 0.0007583713880213755
testing: bce: 4.494008, dice: 29.865623, loss: 4.494008
IoU in current test batch is 0.8914762121192468
Epoch 39334: reducing learning rate of group 0 to 6.8442e-04.
training: bce: 0.017525, dice: 0.116187, loss: 0.017525
training IoU in current batch 6800 is 0.8452299245024022
training IoU uptillnow 6800 is 0.0007671841511682787
testing: bce: 4.584067, dice: 30.391754, loss: 4.584067
IoU in current test batch is 0.8629459165026877
Epoch 39435: reducing learning rate of group 0 to 6.8373e-04.
training: bce: 0.017486, dice: 0.116245, loss: 0.017486
training IoU in current batch 6900 is 0.9579417831767133
training IoU uptillnow 6900 is 0.0007773807943822999
testing: bce: 4.641271, dice: 30.854227, loss: 4.641271
IoU in current test batch is 0.8823497431187837
Epoch 39536: reducing learning rate of group 0 to 6.8305e-04.
training: bce: 0.017425, dice: 0.115977, loss: 0.017425
training IoU in current batch 7000 is 0.9007299825108357
training IoU uptillnow 7000 is 0.0007868025800386847
testing: bce: 4.691976, dice: 31.228984, loss: 4.691976
IoU in current test batch is 0.8887524297535061
Epoch 39637: reducing learning rate of group 0 to 6.8236e-04.
training: bce: 0.017408, dice: 0.115954, loss: 0.017408
training IoU in current batch 7100 is 0.8854109058732441
training IoU uptillnow 7100 is 0.000795983659117431
testing: bce: 4.754339, dice: 31.668844, loss: 4.754339
IoU in current test batch is 0.8807494414507242
Epoch 39738: reducing learning rate of group 0 to 6.8168e-04.
training: bce: 0.017339, dice: 0.115696, loss: 0.017339
training IoU in current batch 7200 is 0.9015175466329434
training IoU uptillnow 7200 is 0.0008053211453098292
testing: bce: 4.802110, dice: 32.043349, loss: 4.802110
IoU in current test batch is 0.8966305529192902
Epoch 39839: reducing learning rate of group 0 to 6.8100e-04.
training: bce: 0.017281, dice: 0.115516, loss: 0.017281
training IoU in current batch 7300 is 0.8956221992416408
training IoU uptillnow 7300 is 0.0008145377975912841
testing: bce: 4.852591, dice: 32.437663, loss: 4.852591
IoU in current test batch is 0.8621436688350029
Epoch 39940: reducing learning rate of group 0 to 6.8032e-04.
training: bce: 0.017205, dice: 0.115200, loss: 0.017205
training IoU in current batch 7400 is 0.9238013979588489
training IoU uptillnow 7400 is 0.0008240609936067612
testing: bce: 4.897333, dice: 32.792221, loss: 4.897333
IoU in current test batch is 0.8731315592768205
Epoch 40041: reducing learning rate of group 0 to 6.7964e-04.
training: bce: 0.017144, dice: 0.114961, loss: 0.017144
training IoU in current batch 7500 is 0.8950836526778203
training IoU uptillnow 7500 is 0.000833178104356664
testing: bce: 4.946031, dice: 33.166348, loss: 4.946031
IoU in current test batch is 0.8627258062359799
Epoch 40142: reducing learning rate of group 0 to 6.7896e-04.
training: bce: 0.017120, dice: 0.114920, loss: 0.017120
training IoU in current batch 7600 is 0.9210272873194222
training IoU uptillnow 7600 is 0.0008425728871864388
testing: bce: 5.004860, dice: 33.596372, loss: 5.004860
IoU in current test batch is 0.8786379402008662
Epoch 40243: reducing learning rate of group 0 to 6.7828e-04.
training: bce: 0.017074, dice: 0.114675, loss: 0.017074
training IoU in current batch 7700 is 0.9278611892405816
training IoU uptillnow 7700 is 0.0008520058815317512
testing: bce: 5.057211, dice: 33.965835, loss: 5.057211
IoU in current test batch is 0.8998440435232116
Epoch 40344: reducing learning rate of group 0 to 6.7760e-04.
training: bce: 0.017017, dice: 0.114438, loss: 0.017017
training IoU in current batch 7800 is 0.9207282715875476
training IoU uptillnow 7800 is 0.0008613037302694208
testing: bce: 5.105625, dice: 34.335770, loss: 5.105625
IoU in current test batch is 0.8713007327306657
training: bce: 0.016981, dice: 0.114244, loss: 0.016981
training IoU in current batch 7900 is 0.9313361691857484
training IoU uptillnow 7900 is 0.0008706867315813266
testing: bce: 5.160220, dice: 34.716881, loss: 5.160220
IoU in current test batch is 0.870466792237188
Epoch 40513: reducing learning rate of group 0 to 6.7692e-04.
training: bce: 0.016963, dice: 0.114081, loss: 0.016963
training IoU in current batch 8000 is 0.9037780827855234
training IoU uptillnow 8000 is 0.000879683645679765
testing: bce: 5.220041, dice: 35.106299, loss: 5.220041
IoU in current test batch is 0.8855296854358028
Epoch 40614: reducing learning rate of group 0 to 6.7625e-04.
training: bce: 0.016912, dice: 0.113860, loss: 0.016912
training IoU in current batch 8100 is 0.8892527137346371
training IoU uptillnow 8100 is 0.0008884576279501335
testing: bce: 5.269453, dice: 35.476116, loss: 5.269453
IoU in current test batch is 0.8814374352788674
Epoch 40715: reducing learning rate of group 0 to 6.7557e-04.
training: bce: 0.016887, dice: 0.113634, loss: 0.016887
training IoU in current batch 8200 is 0.8883869593010867
training IoU uptillnow 8200 is 0.0008971779243183227
testing: bce: 5.326597, dice: 35.842680, loss: 5.326597
IoU in current test batch is 0.872885709964029
Epoch 40816: reducing learning rate of group 0 to 6.7490e-04.
training: bce: 0.016826, dice: 0.113368, loss: 0.016826
training IoU in current batch 8300 is 0.8706409343994641
training IoU uptillnow 8300 is 0.0009056383136686918
testing: bce: 5.372041, dice: 36.194851, loss: 5.372041
IoU in current test batch is 0.8915545475570809
Epoch 40917: reducing learning rate of group 0 to 6.7422e-04.
training: bce: 0.016913, dice: 0.113640, loss: 0.016913
training IoU in current batch 8400 is 0.9112175482437873
training IoU uptillnow 8400 is 0.000914552828931516
testing: bce: 5.464757, dice: 36.718943, loss: 5.464757
IoU in current test batch is 0.861410200639437
Epoch 41018: reducing learning rate of group 0 to 6.7355e-04.
training: bce: 0.016878, dice: 0.113500, loss: 0.016878
training IoU in current batch 8500 is 0.930126796932208
training IoU uptillnow 8500 is 0.0009236542337128887
testing: bce: 5.518334, dice: 37.110206, loss: 5.518334
IoU in current test batch is 0.8708203789304287
Epoch 41119: reducing learning rate of group 0 to 6.7287e-04.
training: bce: 0.016930, dice: 0.113530, loss: 0.016930
training IoU in current batch 8600 is 0.873542865574738
training IoU uptillnow 8600 is 0.0009320238617622228
testing: bce: 5.600522, dice: 37.556754, loss: 5.600522
IoU in current test batch is 0.8578258410731116
Epoch 41220: reducing learning rate of group 0 to 6.7220e-04.
training: bce: 0.017046, dice: 0.113686, loss: 0.017046
training IoU in current batch 8700 is 0.8525172943889316
training IoU uptillnow 8700 is 0.0009400980508073808
testing: bce: 5.704394, dice: 38.045498, loss: 5.704394
IoU in current test batch is 0.8112743294244796
Epoch 41321: reducing learning rate of group 0 to 6.7153e-04.
training: bce: 0.017026, dice: 0.113640, loss: 0.017026
training IoU in current batch 8800 is 0.9239042080910398
training IoU uptillnow 8800 is 0.0009489964002182511
testing: bce: 5.763405, dice: 38.467016, loss: 5.763405
IoU in current test batch is 0.8905445343609245
Epoch 41422: reducing learning rate of group 0 to 6.7086e-04.
training: bce: 0.016979, dice: 0.113421, loss: 0.016979
training IoU in current batch 8900 is 0.9014002967036944
training IoU uptillnow 8900 is 0.0009575803519747265
testing: bce: 5.812529, dice: 38.829230, loss: 5.812529
IoU in current test batch is 0.8510865799126939
Epoch 41523: reducing learning rate of group 0 to 6.7019e-04.
training: bce: 0.016941, dice: 0.113290, loss: 0.016941
training IoU in current batch 9000 is 0.9295145398699589
training IoU uptillnow 9000 is 0.0009664613068535459
testing: bce: 5.864886, dice: 39.220189, loss: 5.864886
IoU in current test batch is 0.8610375029657757
Epoch 41624: reducing learning rate of group 0 to 6.6952e-04.
training: bce: 0.016893, dice: 0.113069, loss: 0.016893
training IoU in current batch 9100 is 0.9509644022124522
training IoU uptillnow 9100 is 0.0009755571200186708
testing: bce: 5.913063, dice: 39.578636, loss: 5.913063
IoU in current test batch is 0.8668424396117138
Epoch 41725: reducing learning rate of group 0 to 6.6885e-04.
training: bce: 0.016862, dice: 0.112968, loss: 0.016862
training IoU in current batch 9200 is 0.9051652955747882
training IoU uptillnow 9200 is 0.0009840608610403724
testing: bce: 5.967248, dice: 39.977467, loss: 5.967248
IoU in current test batch is 0.8470690199787039
Epoch 41826: reducing learning rate of group 0 to 6.6818e-04.
training: bce: 0.016808, dice: 0.112765, loss: 0.016808
training IoU in current batch 9300 is 0.9190688259109312
training IoU uptillnow 9300 is 0.0009926900758900463
testing: bce: 6.012904, dice: 40.339529, loss: 6.012904
IoU in current test batch is 0.879215900989133
Epoch 41927: reducing learning rate of group 0 to 6.6751e-04.
training: bce: 0.016764, dice: 0.112549, loss: 0.016764
training IoU in current batch 9400 is 0.9143691877211753
training IoU uptillnow 9400 is 0.0010012221343477534
testing: bce: 6.061363, dice: 40.694960, loss: 6.061363
IoU in current test batch is 0.8244057397322029
Epoch 42028: reducing learning rate of group 0 to 6.6684e-04.
training: bce: 0.016728, dice: 0.112409, loss: 0.016728
training IoU in current batch 9500 is 0.9400733487088359
training IoU uptillnow 9500 is 0.0010100192534792446
testing: bce: 6.112889, dice: 41.076791, loss: 6.112889
IoU in current test batch is 0.8613844318265217
Epoch 42129: reducing learning rate of group 0 to 6.6617e-04.
training: bce: 0.016699, dice: 0.112252, loss: 0.016699
training IoU in current batch 9600 is 0.92245859348712
training IoU uptillnow 9600 is 0.0010185656744663414
testing: bce: 6.166523, dice: 41.451162, loss: 6.166523
IoU in current test batch is 0.847349141450731
Epoch 42230: reducing learning rate of group 0 to 6.6551e-04.
training: bce: 0.016664, dice: 0.112099, loss: 0.016664
training IoU in current batch 9700 is 0.8907952173087873
training IoU uptillnow 9700 is 0.00102669691959842
testing: bce: 6.217418, dice: 41.825913, loss: 6.217418
IoU in current test batch is 0.8826597101428694
Epoch 42331: reducing learning rate of group 0 to 6.6484e-04.
training: bce: 0.016609, dice: 0.111822, loss: 0.016609
training IoU in current batch 9800 is 0.9194995444031682
training IoU uptillnow 9800 is 0.0010351286621276528
testing: bce: 6.261077, dice: 42.152677, loss: 6.261077
IoU in current test batch is 0.897506627579001
Epoch 42432: reducing learning rate of group 0 to 6.6418e-04.
training: bce: 0.016560, dice: 0.111628, loss: 0.016560
training IoU in current batch 9900 is 0.9264550613699354
training IoU uptillnow 9900 is 0.0010436026056245657
testing: bce: 6.306179, dice: 42.508975, loss: 6.306179
IoU in current test batch is 0.8777855034616524
Epoch 42533: reducing learning rate of group 0 to 6.6351e-04.
training: bce: 0.016518, dice: 0.111508, loss: 0.016518
training IoU in current batch 10000 is 0.9341628784960321
training IoU uptillnow 10000 is 0.0010521272928402923
testing: bce: 6.353714, dice: 42.891902, loss: 6.353714
IoU in current test batch is 0.870532193187096
Epoch 42634: reducing learning rate of group 0 to 6.6285e-04.
training: bce: 0.016551, dice: 0.111555, loss: 0.016551
training IoU in current batch 10100 is 0.938612831954252
training IoU uptillnow 10100 is 0.0010606641733826925
testing: bce: 6.429963, dice: 43.339004, loss: 6.429963
IoU in current test batch is 0.8547756959528509
Epoch 42735: reducing learning rate of group 0 to 6.6219e-04.
training: bce: 0.016528, dice: 0.111493, loss: 0.016528
training IoU in current batch 10200 is 0.8548170137761492
training IoU uptillnow 10200 is 0.0010681810353237633
testing: bce: 6.484844, dice: 43.743915, loss: 6.484844
IoU in current test batch is 0.8789486991368849
Epoch 42836: reducing learning rate of group 0 to 6.6153e-04.
training: bce: 0.016536, dice: 0.111473, loss: 0.016536
training IoU in current batch 10300 is 0.9198117281285085
training IoU uptillnow 10300 is 0.001076421219235638
testing: bce: 6.551609, dice: 44.164780, loss: 6.551609
IoU in current test batch is 0.8416495442125892
Epoch 42937: reducing learning rate of group 0 to 6.6086e-04.
training: bce: 0.016496, dice: 0.111321, loss: 0.016496
training IoU in current batch 10400 is 0.849407241268824
training IoU uptillnow 10400 is 0.0010838034122462865
testing: bce: 6.599004, dice: 44.532575, loss: 6.599004
IoU in current test batch is 0.889669807031284
Epoch 43038: reducing learning rate of group 0 to 6.6020e-04.
training: bce: 0.016449, dice: 0.111119, loss: 0.016449
training IoU in current batch 10500 is 0.9021119678176333
training IoU uptillnow 10500 is 0.0010917634499397369
testing: bce: 6.643659, dice: 44.879364, loss: 6.643659
IoU in current test batch is 0.8851600751326413
Epoch 43139: reducing learning rate of group 0 to 6.5954e-04.
training: bce: 0.016411, dice: 0.110952, loss: 0.016411
training IoU in current batch 10600 is 0.9447384125721106
training IoU uptillnow 10600 is 0.0011001805315117616
testing: bce: 6.691447, dice: 45.238644, loss: 6.691447
IoU in current test batch is 0.8749368297146098
Epoch 43240: reducing learning rate of group 0 to 6.5888e-04.
training: bce: 0.016431, dice: 0.110854, loss: 0.016431
training IoU in current batch 10700 is 0.8973643843257565
training IoU uptillnow 10700 is 0.0011080110067545204
testing: bce: 6.762642, dice: 45.624976, loss: 6.762642
IoU in current test batch is 0.8686253496475411
Epoch 43341: reducing learning rate of group 0 to 6.5822e-04.
training: bce: 0.016390, dice: 0.110722, loss: 0.016390
training IoU in current batch 10800 is 0.9291327532977828
training IoU uptillnow 10800 is 0.0011161717762206837
testing: bce: 6.808614, dice: 45.996508, loss: 6.808614
IoU in current test batch is 0.8807500850277615
Epoch 43442: reducing learning rate of group 0 to 6.5757e-04.
training: bce: 0.016353, dice: 0.110588, loss: 0.016353
training IoU in current batch 10900 is 0.9260989010989011
training IoU uptillnow 10900 is 0.001124260068903623
testing: bce: 6.856247, dice: 46.366137, loss: 6.856247
IoU in current test batch is 0.8598354152817619
Epoch 43543: reducing learning rate of group 0 to 6.5691e-04.
training: bce: 0.016313, dice: 0.110390, loss: 0.016313
training IoU in current batch 11000 is 0.9008489434345298
training IoU uptillnow 11000 is 0.00113202131678998
testing: bce: 6.902148, dice: 46.707682, loss: 6.902148
IoU in current test batch is 0.8384341127527783
Epoch 43644: reducing learning rate of group 0 to 6.5625e-04.
training: bce: 0.016304, dice: 0.110248, loss: 0.016304
training IoU in current batch 11100 is 0.8708918020679468
training IoU uptillnow 11100 is 0.0011394038473883825
testing: bce: 6.961281, dice: 47.071660, loss: 6.961281
IoU in current test batch is 0.8590334912990405
Epoch 43745: reducing learning rate of group 0 to 6.5560e-04.
training: bce: 0.016303, dice: 0.110238, loss: 0.016303
training IoU in current batch 11200 is 0.9404998894049988
training IoU uptillnow 11200 is 0.0011475481589796844
testing: bce: 7.023354, dice: 47.491221, loss: 7.023354
IoU in current test batch is 0.8964637847354271
Epoch 43846: reducing learning rate of group 0 to 6.5494e-04.
training: bce: 0.016266, dice: 0.110102, loss: 0.016266
training IoU in current batch 11300 is 0.9125952142802185
training IoU uptillnow 11300 is 0.0011553371369895167
testing: bce: 7.070212, dice: 47.856405, loss: 7.070212
IoU in current test batch is 0.8846824374831961
Epoch 43947: reducing learning rate of group 0 to 6.5429e-04.
training: bce: 0.016238, dice: 0.109940, loss: 0.016238
training IoU in current batch 11400 is 0.8905094149293881
training IoU uptillnow 11400 is 0.0011628394064980604
testing: bce: 7.120462, dice: 48.208750, loss: 7.120462
IoU in current test batch is 0.8611853142141945
Epoch 44048: reducing learning rate of group 0 to 6.5363e-04.
training: bce: 0.016202, dice: 0.109803, loss: 0.016202
training IoU in current batch 11500 is 0.8102158064983344
training IoU uptillnow 11500 is 0.0011693962110610942
testing: bce: 7.166771, dice: 48.570764, loss: 7.166771
IoU in current test batch is 0.8350559462520367
Epoch 44149: reducing learning rate of group 0 to 6.5298e-04.
training: bce: 0.016171, dice: 0.109644, loss: 0.016171
training IoU in current batch 11600 is 0.8644816491247205
training IoU uptillnow 11600 is 0.00117653788205298
testing: bce: 7.215290, dice: 48.922219, loss: 7.215290
IoU in current test batch is 0.8739102631651546
training: bce: 0.016228, dice: 0.109725, loss: 0.016228
training IoU in current batch 11700 is 0.9320493010887922
training IoU uptillnow 11700 is 0.001184410758861511
testing: bce: 7.303021, dice: 49.380506, loss: 7.303021
IoU in current test batch is 0.8509670285459278
Epoch 44250: reducing learning rate of group 0 to 6.5232e-04.
training: bce: 0.016198, dice: 0.109649, loss: 0.016198
training IoU in current batch 11800 is 0.8974604512199793
training IoU uptillnow 11800 is 0.0011918581742714671
testing: bce: 7.351891, dice: 49.767881, loss: 7.351891
IoU in current test batch is 0.8808178982159464
Epoch 44351: reducing learning rate of group 0 to 6.5167e-04.
training: bce: 0.016169, dice: 0.109569, loss: 0.016169
training IoU in current batch 11900 is 0.9417149197323255
training IoU uptillnow 11900 is 0.001199769886269105
testing: bce: 7.401019, dice: 50.152977, loss: 7.401019
IoU in current test batch is 0.8808310924134763
Epoch 44452: reducing learning rate of group 0 to 6.5102e-04.
training: bce: 0.016133, dice: 0.109429, loss: 0.016133
training IoU in current batch 12000 is 0.8452456634274816
training IoU uptillnow 12000 is 0.0012065633596658172
testing: bce: 7.446696, dice: 50.510107, loss: 7.446696
IoU in current test batch is 0.8806020561716129
Epoch 44553: reducing learning rate of group 0 to 6.5037e-04.
training: bce: 0.016136, dice: 0.109346, loss: 0.016136
training IoU in current batch 12100 is 0.9297299746284886
training IoU uptillnow 12100 is 0.001214272486337956
testing: bce: 7.509882, dice: 50.892076, loss: 7.509882
IoU in current test batch is 0.8880187199337668
Epoch 44654: reducing learning rate of group 0 to 6.4972e-04.
training: bce: 0.016111, dice: 0.109229, loss: 0.016111
training IoU in current batch 12200 is 0.9351054992523675
training IoU uptillnow 12200 is 0.0012220072208264396
testing: bce: 7.560273, dice: 51.257942, loss: 7.560273
IoU in current test batch is 0.8889099505036338
Epoch 44755: reducing learning rate of group 0 to 6.4907e-04.
training: bce: 0.016076, dice: 0.109078, loss: 0.016076
training IoU in current batch 12300 is 0.9213395257883158
training IoU uptillnow 12300 is 0.0012295539948330954
testing: bce: 7.605666, dice: 51.606408, loss: 7.605666
IoU in current test batch is 0.8599178642380638
Epoch 44856: reducing learning rate of group 0 to 6.4842e-04.
training: bce: 0.016049, dice: 0.109026, loss: 0.016049
training IoU in current batch 12400 is 0.9238921476807808
training IoU uptillnow 12400 is 0.0012370955842691755
testing: bce: 7.654585, dice: 52.001217, loss: 7.654585
IoU in current test batch is 0.8793742795034837
Epoch 44957: reducing learning rate of group 0 to 6.4777e-04.
training: bce: 0.016016, dice: 0.108899, loss: 0.016016
training IoU in current batch 12500 is 0.9419733942707909
training IoU uptillnow 12500 is 0.0012448043743491647
testing: bce: 7.700513, dice: 52.359498, loss: 7.700513
IoU in current test batch is 0.8765026902201735
Epoch 45058: reducing learning rate of group 0 to 6.4712e-04.
training: bce: 0.015987, dice: 0.108731, loss: 0.015987
training IoU in current batch 12600 is 0.8846280855279451
training IoU uptillnow 12600 is 0.001251843956300882
testing: bce: 7.748252, dice: 52.696752, loss: 7.748252
IoU in current test batch is 0.8842677991783441
Epoch 45159: reducing learning rate of group 0 to 6.4648e-04.
training: bce: 0.015952, dice: 0.108587, loss: 0.015952
training IoU in current batch 12700 is 0.9267145041759793
training IoU uptillnow 12700 is 0.001259317472173055
testing: bce: 7.792656, dice: 53.044678, loss: 7.792656
IoU in current test batch is 0.8796297779330812
Epoch 45260: reducing learning rate of group 0 to 6.4583e-04.
training: bce: 0.015920, dice: 0.108427, loss: 0.015920
training IoU in current batch 12800 is 0.8945978240362241
training IoU uptillnow 12800 is 0.0012664039265948416
testing: bce: 7.838167, dice: 53.383633, loss: 7.838167
IoU in current test batch is 0.8616701040937242
Epoch 45361: reducing learning rate of group 0 to 6.4518e-04.
training: bce: 0.015927, dice: 0.108414, loss: 0.015927
training IoU in current batch 12900 is 0.9099819331526648
training IoU uptillnow 12900 is 0.0012736284411421271
testing: bce: 7.903077, dice: 53.794026, loss: 7.903077
IoU in current test batch is 0.8479332260303557
Epoch 45462: reducing learning rate of group 0 to 6.4454e-04.
training: bce: 0.015898, dice: 0.108311, loss: 0.015898
training IoU in current batch 13000 is 0.9304495335029687
training IoU uptillnow 13000 is 0.0012810459083511473
testing: bce: 7.949585, dice: 54.159480, loss: 7.949585
IoU in current test batch is 0.8747353543188015
Epoch 45563: reducing learning rate of group 0 to 6.4389e-04.
training: bce: 0.015870, dice: 0.108198, loss: 0.015870
training IoU in current batch 13100 is 0.8837739510065487
training IoU uptillnow 13100 is 0.0012879196393814577
testing: bce: 7.996544, dice: 54.519191, loss: 7.996544
IoU in current test batch is 0.8634042406220653
Epoch 45664: reducing learning rate of group 0 to 6.4325e-04.
training: bce: 0.015881, dice: 0.108139, loss: 0.015881
training IoU in current batch 13200 is 0.8694642630979018
training IoU uptillnow 13200 is 0.0012946069292449711
testing: bce: 8.063524, dice: 54.905380, loss: 8.063524
IoU in current test batch is 0.8052937669334691
Epoch 45765: reducing learning rate of group 0 to 6.4261e-04.
training: bce: 0.015905, dice: 0.108283, loss: 0.015905
training IoU in current batch 13300 is 0.7414239041655323
training IoU uptillnow 13300 is 0.0012998687370980179
testing: bce: 8.136728, dice: 55.394991, loss: 8.136728
IoU in current test batch is 0.8854447375857238
Epoch 45866: reducing learning rate of group 0 to 6.4196e-04.
training: bce: 0.015917, dice: 0.108380, loss: 0.015917
training IoU in current batch 13400 is 0.8879833609236385
training IoU uptillnow 13400 is 0.001306702430756317
testing: bce: 8.204078, dice: 55.861661, loss: 8.204078
IoU in current test batch is 0.8800708650523243
Epoch 45967: reducing learning rate of group 0 to 6.4132e-04.
training: bce: 0.015900, dice: 0.108374, loss: 0.015900
training IoU in current batch 13500 is 0.9453086215455899
training IoU uptillnow 13500 is 0.0013141288755102703
testing: bce: 8.256143, dice: 56.275431, loss: 8.256143
IoU in current test batch is 0.8806691808219175
training: bce: 0.015930, dice: 0.108515, loss: 0.015930
training IoU in current batch 13600 is 0.8642674664000889
training IoU uptillnow 13600 is 0.001320645107444506
testing: bce: 8.333127, dice: 56.766000, loss: 8.333127
IoU in current test batch is 0.8490281042155802
Epoch 46162: reducing learning rate of group 0 to 6.4068e-04.
training: bce: 0.015908, dice: 0.108395, loss: 0.015908
training IoU in current batch 13700 is 0.8926933523309141
training IoU uptillnow 13700 is 0.0013274404709710523
testing: bce: 8.383149, dice: 57.120117, loss: 8.383149
IoU in current test batch is 0.8763605897403953
Epoch 46263: reducing learning rate of group 0 to 6.4004e-04.
training: bce: 0.015973, dice: 0.108506, loss: 0.015973
training IoU in current batch 13800 is 0.808783203045062
training IoU uptillnow 13800 is 0.001333301322855656
testing: bce: 8.478398, dice: 57.595588, loss: 8.478398
IoU in current test batch is 0.8265699390452342
Epoch 46364: reducing learning rate of group 0 to 6.3940e-04.
training: bce: 0.015944, dice: 0.108404, loss: 0.015944
training IoU in current batch 13900 is 0.9244781171919078
training IoU uptillnow 13900 is 0.0013403823232175627
testing: bce: 8.524599, dice: 57.958609, loss: 8.524599
IoU in current test batch is 0.8871216894234407
Epoch 46465: reducing learning rate of group 0 to 6.3876e-04.
training: bce: 0.015928, dice: 0.108348, loss: 0.015928
training IoU in current batch 14000 is 0.9217391304347826
training IoU uptillnow 14000 is 0.0013474034798979918
testing: bce: 8.577007, dice: 58.345569, loss: 8.577007
IoU in current test batch is 0.8808940060181798
Epoch 46566: reducing learning rate of group 0 to 6.3812e-04.
training: bce: 0.015905, dice: 0.108254, loss: 0.015905
training IoU in current batch 14100 is 0.8983891445802253
training IoU uptillnow 14100 is 0.0013541442643501362
testing: bce: 8.625939, dice: 58.711404, loss: 8.625939
IoU in current test batch is 0.8811151935471776
Epoch 46667: reducing learning rate of group 0 to 6.3748e-04.
training: bce: 0.015875, dice: 0.108141, loss: 0.015875
training IoU in current batch 14200 is 0.8687509919060467
training IoU uptillnow 14200 is 0.0013605392219329555
testing: bce: 8.670869, dice: 59.065993, loss: 8.670869
IoU in current test batch is 0.8249190512274626
Epoch 46768: reducing learning rate of group 0 to 6.3685e-04.
training: bce: 0.015869, dice: 0.108089, loss: 0.015869
training IoU in current batch 14300 is 0.9385184282173189
training IoU uptillnow 14300 is 0.001367651470557068
testing: bce: 8.728409, dice: 59.452991, loss: 8.728409
IoU in current test batch is 0.8916643579016938
Epoch 46869: reducing learning rate of group 0 to 6.3621e-04.
training: bce: 0.015861, dice: 0.108062, loss: 0.015861
training IoU in current batch 14400 is 0.8691900158392047
training IoU uptillnow 14400 is 0.0013739950921262836
testing: bce: 8.785031, dice: 59.854151, loss: 8.785031
IoU in current test batch is 0.8592123647235312
Epoch 46970: reducing learning rate of group 0 to 6.3557e-04.
training: bce: 0.015847, dice: 0.108050, loss: 0.015847
training IoU in current batch 14500 is 0.928145751745275
training IoU uptillnow 14500 is 0.0013809382767862694
testing: bce: 8.838419, dice: 60.263073, loss: 8.838419
IoU in current test batch is 0.8679960911902437
Epoch 47071: reducing learning rate of group 0 to 6.3494e-04.
training: bce: 0.015820, dice: 0.107932, loss: 0.015820
training IoU in current batch 14600 is 0.9059696710807348
training IoU uptillnow 14600 is 0.0013876168419430948
testing: bce: 8.884028, dice: 60.611916, loss: 8.884028
IoU in current test batch is 0.8851627912110623
Epoch 47172: reducing learning rate of group 0 to 6.3430e-04.
training: bce: 0.015798, dice: 0.107842, loss: 0.015798
training IoU in current batch 14700 is 0.9034762065474181
training IoU uptillnow 14700 is 0.0013942407515946129
testing: bce: 8.932697, dice: 60.976094, loss: 8.932697
IoU in current test batch is 0.8752690212950303
Epoch 47273: reducing learning rate of group 0 to 6.3367e-04.
training: bce: 0.015788, dice: 0.107812, loss: 0.015788
training IoU in current batch 14800 is 0.8125320803885182
training IoU uptillnow 14800 is 0.001399876333069281
testing: bce: 8.987812, dice: 61.374085, loss: 8.987812
IoU in current test batch is 0.9047449536583068
Epoch 47374: reducing learning rate of group 0 to 6.3304e-04.
training: bce: 0.015761, dice: 0.107683, loss: 0.015761
training IoU in current batch 14900 is 0.892112676056338
training IoU uptillnow 14900 is 0.001406326742551388
testing: bce: 9.032736, dice: 61.714786, loss: 9.032736
IoU in current test batch is 0.8550523198288297
Epoch 47475: reducing learning rate of group 0 to 6.3240e-04.
training: bce: 0.015746, dice: 0.107608, loss: 0.015746
training IoU in current batch 15000 is 0.9149025441507419
training IoU uptillnow 15000 is 0.001412989664302831
testing: bce: 9.084630, dice: 62.085955, loss: 9.084630
IoU in current test batch is 0.8364586573765938
Epoch 47576: reducing learning rate of group 0 to 6.3177e-04.
training: bce: 0.015739, dice: 0.107548, loss: 0.015739
training IoU in current batch 15100 is 0.9346829036849597
training IoU uptillnow 15100 is 0.0014198321807072465
testing: bce: 9.141065, dice: 62.464679, loss: 9.141065
IoU in current test batch is 0.8724104646233749
Epoch 47677: reducing learning rate of group 0 to 6.3114e-04.
training: bce: 0.015733, dice: 0.107607, loss: 0.015733
training IoU in current batch 15200 is 0.9622591184903647
training IoU uptillnow 15200 is 0.0014269347962566124
testing: bce: 9.198634, dice: 62.912719, loss: 9.198634
IoU in current test batch is 0.8833855839161026
Epoch 47778: reducing learning rate of group 0 to 6.3051e-04.
training: bce: 0.015708, dice: 0.107489, loss: 0.015708
training IoU in current batch 15300 is 0.8919343027513338
training IoU uptillnow 15300 is 0.0014332728702542512
testing: bce: 9.244170, dice: 63.257351, loss: 9.244170
IoU in current test batch is 0.859168619023957
Epoch 47879: reducing learning rate of group 0 to 6.2988e-04.
training: bce: 0.015693, dice: 0.107424, loss: 0.015693
training IoU in current batch 15400 is 0.9323778930895313
training IoU uptillnow 15400 is 0.0014400062388924921
testing: bce: 9.295853, dice: 63.632482, loss: 9.295853
IoU in current test batch is 0.8789331891211251
Epoch 47980: reducing learning rate of group 0 to 6.2925e-04.
training: bce: 0.015711, dice: 0.107479, loss: 0.015711
training IoU in current batch 15500 is 0.935766029593095
training IoU uptillnow 15500 is 0.0014467468374607872
testing: bce: 9.366531, dice: 64.078353, loss: 9.366531
IoU in current test batch is 0.8982404502172988
Epoch 48081: reducing learning rate of group 0 to 6.2862e-04.
training: bce: 0.015688, dice: 0.107418, loss: 0.015688
training IoU in current batch 15600 is 0.8658584719481252
training IoU uptillnow 15600 is 0.0014527334946893768
testing: bce: 9.413565, dice: 64.454774, loss: 9.413565
IoU in current test batch is 0.8941010288119825
Epoch 48182: reducing learning rate of group 0 to 6.2799e-04.
training: bce: 0.015689, dice: 0.107413, loss: 0.015689
training IoU in current batch 15700 is 0.9349717268377555
training IoU uptillnow 15700 is 0.001459411543455684
testing: bce: 9.474225, dice: 64.864867, loss: 9.474225
IoU in current test batch is 0.9063250822587603
Epoch 48283: reducing learning rate of group 0 to 6.2736e-04.
training: bce: 0.015668, dice: 0.107341, loss: 0.015668
training IoU in current batch 15800 is 0.93269043823152
training IoU uptillnow 15800 is 0.001466038376510218
testing: bce: 9.521671, dice: 65.234290, loss: 9.521671
IoU in current test batch is 0.8952600624213376
Epoch 48384: reducing learning rate of group 0 to 6.2673e-04.
training: bce: 0.015651, dice: 0.107283, loss: 0.015651
training IoU in current batch 15900 is 0.8764117318675656
training IoU uptillnow 15900 is 0.0014720570563373115
testing: bce: 9.571664, dice: 65.611627, loss: 9.571664
IoU in current test batch is 0.8859438106040608
Epoch 48485: reducing learning rate of group 0 to 6.2611e-04.
training: bce: 0.015630, dice: 0.107219, loss: 0.015630
training IoU in current batch 16000 is 0.8932526544126042
training IoU uptillnow 16000 is 0.0014782243829127154
testing: bce: 9.619117, dice: 65.985139, loss: 9.619117
IoU in current test batch is 0.8639613858994478
Epoch 48586: reducing learning rate of group 0 to 6.2548e-04.
training: bce: 0.015635, dice: 0.107222, loss: 0.015635
training IoU in current batch 16100 is 0.9213489057239057
training IoU uptillnow 16100 is 0.001484655117341038
testing: bce: 9.682502, dice: 66.399575, loss: 9.682502
IoU in current test batch is 0.8635641288396374
Epoch 48687: reducing learning rate of group 0 to 6.2486e-04.
training: bce: 0.015607, dice: 0.107082, loss: 0.015607
training IoU in current batch 16200 is 0.9307773758051707
training IoU uptillnow 16200 is 0.0014911561722476213
testing: bce: 9.725065, dice: 66.724296, loss: 9.725065
IoU in current test batch is 0.8728855023150106
Epoch 48788: reducing learning rate of group 0 to 6.2423e-04.
training: bce: 0.015633, dice: 0.107142, loss: 0.015633
training IoU in current batch 16300 is 0.8656130790190736
training IoU uptillnow 16300 is 0.0014969636200676559
testing: bce: 9.801055, dice: 67.173777, loss: 9.801055
IoU in current test batch is 0.8746246581527151
Epoch 48889: reducing learning rate of group 0 to 6.2361e-04.
training: bce: 0.015661, dice: 0.107230, loss: 0.015661
training IoU in current batch 16400 is 0.9094385537980184
training IoU uptillnow 16400 is 0.001503194999650537
testing: bce: 9.879233, dice: 67.641598, loss: 9.879233
IoU in current test batch is 0.865915201759879
Epoch 48990: reducing learning rate of group 0 to 6.2298e-04.
training: bce: 0.015650, dice: 0.107257, loss: 0.015650
training IoU in current batch 16500 is 0.9137700226192125
training IoU uptillnow 16500 is 0.001509445124755677
testing: bce: 9.932268, dice: 68.071011, loss: 9.932268
IoU in current test batch is 0.8820805791655932
Epoch 49091: reducing learning rate of group 0 to 6.2236e-04.
training: bce: 0.015626, dice: 0.107169, loss: 0.015626
training IoU in current batch 16600 is 0.9068198850861854
training IoU uptillnow 16600 is 0.0015155991126918212
testing: bce: 9.977009, dice: 68.427468, loss: 9.977009
IoU in current test batch is 0.8847434925608065
Epoch 49192: reducing learning rate of group 0 to 6.2174e-04.
training: bce: 0.015625, dice: 0.107202, loss: 0.015625
training IoU in current batch 16700 is 0.9275759951223761
training IoU uptillnow 16700 is 0.0015219388336289272
testing: bce: 10.036625, dice: 68.860640, loss: 10.036625
IoU in current test batch is 0.8512468683213125
Epoch 49293: reducing learning rate of group 0 to 6.2112e-04.
training: bce: 0.015628, dice: 0.107191, loss: 0.015628
training IoU in current batch 16800 is 0.8749261374827654
training IoU uptillnow 16800 is 0.0015277194228016338
testing: bce: 10.098392, dice: 69.265779, loss: 10.098392
IoU in current test batch is 0.8757063290330276
Epoch 49394: reducing learning rate of group 0 to 6.2049e-04.
training: bce: 0.015610, dice: 0.107131, loss: 0.015610
training IoU in current batch 16900 is 0.8845734890883822
training IoU uptillnow 16900 is 0.0015335741797205921
testing: bce: 10.146786, dice: 69.639156, loss: 10.146786
IoU in current test batch is 0.8360655545094731
Epoch 49495: reducing learning rate of group 0 to 6.1987e-04.
training: bce: 0.015591, dice: 0.107063, loss: 0.015591
training IoU in current batch 17000 is 0.7850186658010063
training IoU uptillnow 17000 is 0.001538400705012037
testing: bce: 10.194620, dice: 70.006966, loss: 10.194620
IoU in current test batch is 0.8676147494818145
Epoch 49596: reducing learning rate of group 0 to 6.1925e-04.
training: bce: 0.015571, dice: 0.106969, loss: 0.015571
training IoU in current batch 17100 is 0.9210550670985654
training IoU uptillnow 17100 is 0.0015445777553961914
testing: bce: 10.241301, dice: 70.356982, loss: 10.241301
IoU in current test batch is 0.893440530778379
Epoch 49697: reducing learning rate of group 0 to 6.1863e-04.
training: bce: 0.015547, dice: 0.106860, loss: 0.015547
training IoU in current batch 17200 is 0.9031283500633467
training IoU uptillnow 17200 is 0.0015505498033462624
testing: bce: 10.285634, dice: 70.696142, loss: 10.285634
IoU in current test batch is 0.8768590435087045
Epoch 49798: reducing learning rate of group 0 to 6.1802e-04.
training: bce: 0.015526, dice: 0.106771, loss: 0.015526
training IoU in current batch 17300 is 0.8270522284509074
training IoU uptillnow 17300 is 0.001555734832963226
testing: bce: 10.331220, dice: 71.047813, loss: 10.331220
IoU in current test batch is 0.8576086799073417
Epoch 49899: reducing learning rate of group 0 to 6.1740e-04.
training: bce: 0.015519, dice: 0.106704, loss: 0.015519
training IoU in current batch 17400 is 0.8860530286699719
training IoU uptillnow 17400 is 0.0015614897060057722
testing: bce: 10.386114, dice: 71.413654, loss: 10.386114
IoU in current test batch is 0.8740644132211819
Epoch 50000: reducing learning rate of group 0 to 6.1678e-04.
Maximum training samples requirement meet, I have been training for more than  100001  samples.
Making network now
ResNetUNet(
  (base_model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer0_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer1_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2_1x1): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3_1x1): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4_1x1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (upsample): Upsample(scale_factor=2.0, mode=bilinear)
  (conv_up3): Sequential(
    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up2): Sequential(
    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up1): Sequential(
    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up0): Sequential(
    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size2): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 512, 512]           1,792
              ReLU-2         [-1, 64, 512, 512]               0
            Conv2d-3         [-1, 64, 512, 512]          36,928
              ReLU-4         [-1, 64, 512, 512]               0
            Conv2d-5         [-1, 64, 256, 256]           9,408
            Conv2d-6         [-1, 64, 256, 256]           9,408
       BatchNorm2d-7         [-1, 64, 256, 256]             128
       BatchNorm2d-8         [-1, 64, 256, 256]             128
              ReLU-9         [-1, 64, 256, 256]               0
             ReLU-10         [-1, 64, 256, 256]               0
        MaxPool2d-11         [-1, 64, 128, 128]               0
        MaxPool2d-12         [-1, 64, 128, 128]               0
           Conv2d-13         [-1, 64, 128, 128]          36,864
           Conv2d-14         [-1, 64, 128, 128]          36,864
      BatchNorm2d-15         [-1, 64, 128, 128]             128
      BatchNorm2d-16         [-1, 64, 128, 128]             128
             ReLU-17         [-1, 64, 128, 128]               0
             ReLU-18         [-1, 64, 128, 128]               0
           Conv2d-19         [-1, 64, 128, 128]          36,864
           Conv2d-20         [-1, 64, 128, 128]          36,864
      BatchNorm2d-21         [-1, 64, 128, 128]             128
      BatchNorm2d-22         [-1, 64, 128, 128]             128
             ReLU-23         [-1, 64, 128, 128]               0
             ReLU-24         [-1, 64, 128, 128]               0
       BasicBlock-25         [-1, 64, 128, 128]               0
       BasicBlock-26         [-1, 64, 128, 128]               0
           Conv2d-27         [-1, 64, 128, 128]          36,864
           Conv2d-28         [-1, 64, 128, 128]          36,864
      BatchNorm2d-29         [-1, 64, 128, 128]             128
      BatchNorm2d-30         [-1, 64, 128, 128]             128
             ReLU-31         [-1, 64, 128, 128]               0
             ReLU-32         [-1, 64, 128, 128]               0
           Conv2d-33         [-1, 64, 128, 128]          36,864
           Conv2d-34         [-1, 64, 128, 128]          36,864
      BatchNorm2d-35         [-1, 64, 128, 128]             128
      BatchNorm2d-36         [-1, 64, 128, 128]             128
             ReLU-37         [-1, 64, 128, 128]               0
             ReLU-38         [-1, 64, 128, 128]               0
       BasicBlock-39         [-1, 64, 128, 128]               0
       BasicBlock-40         [-1, 64, 128, 128]               0
           Conv2d-41         [-1, 64, 128, 128]          36,864
           Conv2d-42         [-1, 64, 128, 128]          36,864
      BatchNorm2d-43         [-1, 64, 128, 128]             128
      BatchNorm2d-44         [-1, 64, 128, 128]             128
             ReLU-45         [-1, 64, 128, 128]               0
             ReLU-46         [-1, 64, 128, 128]               0
           Conv2d-47         [-1, 64, 128, 128]          36,864
           Conv2d-48         [-1, 64, 128, 128]          36,864
      BatchNorm2d-49         [-1, 64, 128, 128]             128
      BatchNorm2d-50         [-1, 64, 128, 128]             128
             ReLU-51         [-1, 64, 128, 128]               0
             ReLU-52         [-1, 64, 128, 128]               0
       BasicBlock-53         [-1, 64, 128, 128]               0
       BasicBlock-54         [-1, 64, 128, 128]               0
           Conv2d-55          [-1, 128, 64, 64]          73,728
           Conv2d-56          [-1, 128, 64, 64]          73,728
      BatchNorm2d-57          [-1, 128, 64, 64]             256
      BatchNorm2d-58          [-1, 128, 64, 64]             256
             ReLU-59          [-1, 128, 64, 64]               0
             ReLU-60          [-1, 128, 64, 64]               0
           Conv2d-61          [-1, 128, 64, 64]         147,456
           Conv2d-62          [-1, 128, 64, 64]         147,456
      BatchNorm2d-63          [-1, 128, 64, 64]             256
      BatchNorm2d-64          [-1, 128, 64, 64]             256
           Conv2d-65          [-1, 128, 64, 64]           8,192
           Conv2d-66          [-1, 128, 64, 64]           8,192
      BatchNorm2d-67          [-1, 128, 64, 64]             256
      BatchNorm2d-68          [-1, 128, 64, 64]             256
             ReLU-69          [-1, 128, 64, 64]               0
             ReLU-70          [-1, 128, 64, 64]               0
       BasicBlock-71          [-1, 128, 64, 64]               0
       BasicBlock-72          [-1, 128, 64, 64]               0
           Conv2d-73          [-1, 128, 64, 64]         147,456
           Conv2d-74          [-1, 128, 64, 64]         147,456
      BatchNorm2d-75          [-1, 128, 64, 64]             256
      BatchNorm2d-76          [-1, 128, 64, 64]             256
             ReLU-77          [-1, 128, 64, 64]               0
             ReLU-78          [-1, 128, 64, 64]               0
           Conv2d-79          [-1, 128, 64, 64]         147,456
           Conv2d-80          [-1, 128, 64, 64]         147,456
      BatchNorm2d-81          [-1, 128, 64, 64]             256
      BatchNorm2d-82          [-1, 128, 64, 64]             256
             ReLU-83          [-1, 128, 64, 64]               0
             ReLU-84          [-1, 128, 64, 64]               0
       BasicBlock-85          [-1, 128, 64, 64]               0
       BasicBlock-86          [-1, 128, 64, 64]               0
           Conv2d-87          [-1, 128, 64, 64]         147,456
           Conv2d-88          [-1, 128, 64, 64]         147,456
      BatchNorm2d-89          [-1, 128, 64, 64]             256
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
             ReLU-92          [-1, 128, 64, 64]               0
           Conv2d-93          [-1, 128, 64, 64]         147,456
           Conv2d-94          [-1, 128, 64, 64]         147,456
      BatchNorm2d-95          [-1, 128, 64, 64]             256
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
             ReLU-98          [-1, 128, 64, 64]               0
       BasicBlock-99          [-1, 128, 64, 64]               0
      BasicBlock-100          [-1, 128, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]         147,456
          Conv2d-102          [-1, 128, 64, 64]         147,456
     BatchNorm2d-103          [-1, 128, 64, 64]             256
     BatchNorm2d-104          [-1, 128, 64, 64]             256
            ReLU-105          [-1, 128, 64, 64]               0
            ReLU-106          [-1, 128, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]         147,456
          Conv2d-108          [-1, 128, 64, 64]         147,456
     BatchNorm2d-109          [-1, 128, 64, 64]             256
     BatchNorm2d-110          [-1, 128, 64, 64]             256
            ReLU-111          [-1, 128, 64, 64]               0
            ReLU-112          [-1, 128, 64, 64]               0
      BasicBlock-113          [-1, 128, 64, 64]               0
      BasicBlock-114          [-1, 128, 64, 64]               0
          Conv2d-115          [-1, 256, 32, 32]         294,912
          Conv2d-116          [-1, 256, 32, 32]         294,912
     BatchNorm2d-117          [-1, 256, 32, 32]             512
     BatchNorm2d-118          [-1, 256, 32, 32]             512
            ReLU-119          [-1, 256, 32, 32]               0
            ReLU-120          [-1, 256, 32, 32]               0
          Conv2d-121          [-1, 256, 32, 32]         589,824
          Conv2d-122          [-1, 256, 32, 32]         589,824
     BatchNorm2d-123          [-1, 256, 32, 32]             512
     BatchNorm2d-124          [-1, 256, 32, 32]             512
          Conv2d-125          [-1, 256, 32, 32]          32,768
          Conv2d-126          [-1, 256, 32, 32]          32,768
     BatchNorm2d-127          [-1, 256, 32, 32]             512
     BatchNorm2d-128          [-1, 256, 32, 32]             512
            ReLU-129          [-1, 256, 32, 32]               0
            ReLU-130          [-1, 256, 32, 32]               0
      BasicBlock-131          [-1, 256, 32, 32]               0
      BasicBlock-132          [-1, 256, 32, 32]               0
          Conv2d-133          [-1, 256, 32, 32]         589,824
          Conv2d-134          [-1, 256, 32, 32]         589,824
     BatchNorm2d-135          [-1, 256, 32, 32]             512
     BatchNorm2d-136          [-1, 256, 32, 32]             512
            ReLU-137          [-1, 256, 32, 32]               0
            ReLU-138          [-1, 256, 32, 32]               0
          Conv2d-139          [-1, 256, 32, 32]         589,824
          Conv2d-140          [-1, 256, 32, 32]         589,824
     BatchNorm2d-141          [-1, 256, 32, 32]             512
     BatchNorm2d-142          [-1, 256, 32, 32]             512
            ReLU-143          [-1, 256, 32, 32]               0
            ReLU-144          [-1, 256, 32, 32]               0
      BasicBlock-145          [-1, 256, 32, 32]               0
      BasicBlock-146          [-1, 256, 32, 32]               0
          Conv2d-147          [-1, 256, 32, 32]         589,824
          Conv2d-148          [-1, 256, 32, 32]         589,824
     BatchNorm2d-149          [-1, 256, 32, 32]             512
     BatchNorm2d-150          [-1, 256, 32, 32]             512
            ReLU-151          [-1, 256, 32, 32]               0
            ReLU-152          [-1, 256, 32, 32]               0
          Conv2d-153          [-1, 256, 32, 32]         589,824
          Conv2d-154          [-1, 256, 32, 32]         589,824
     BatchNorm2d-155          [-1, 256, 32, 32]             512
     BatchNorm2d-156          [-1, 256, 32, 32]             512
            ReLU-157          [-1, 256, 32, 32]               0
            ReLU-158          [-1, 256, 32, 32]               0
      BasicBlock-159          [-1, 256, 32, 32]               0
      BasicBlock-160          [-1, 256, 32, 32]               0
          Conv2d-161          [-1, 256, 32, 32]         589,824
          Conv2d-162          [-1, 256, 32, 32]         589,824
     BatchNorm2d-163          [-1, 256, 32, 32]             512
     BatchNorm2d-164          [-1, 256, 32, 32]             512
            ReLU-165          [-1, 256, 32, 32]               0
            ReLU-166          [-1, 256, 32, 32]               0
          Conv2d-167          [-1, 256, 32, 32]         589,824
          Conv2d-168          [-1, 256, 32, 32]         589,824
     BatchNorm2d-169          [-1, 256, 32, 32]             512
     BatchNorm2d-170          [-1, 256, 32, 32]             512
            ReLU-171          [-1, 256, 32, 32]               0
            ReLU-172          [-1, 256, 32, 32]               0
      BasicBlock-173          [-1, 256, 32, 32]               0
      BasicBlock-174          [-1, 256, 32, 32]               0
          Conv2d-175          [-1, 256, 32, 32]         589,824
          Conv2d-176          [-1, 256, 32, 32]         589,824
     BatchNorm2d-177          [-1, 256, 32, 32]             512
     BatchNorm2d-178          [-1, 256, 32, 32]             512
            ReLU-179          [-1, 256, 32, 32]               0
            ReLU-180          [-1, 256, 32, 32]               0
          Conv2d-181          [-1, 256, 32, 32]         589,824
          Conv2d-182          [-1, 256, 32, 32]         589,824
     BatchNorm2d-183          [-1, 256, 32, 32]             512
     BatchNorm2d-184          [-1, 256, 32, 32]             512
            ReLU-185          [-1, 256, 32, 32]               0
            ReLU-186          [-1, 256, 32, 32]               0
      BasicBlock-187          [-1, 256, 32, 32]               0
      BasicBlock-188          [-1, 256, 32, 32]               0
          Conv2d-189          [-1, 256, 32, 32]         589,824
          Conv2d-190          [-1, 256, 32, 32]         589,824
     BatchNorm2d-191          [-1, 256, 32, 32]             512
     BatchNorm2d-192          [-1, 256, 32, 32]             512
            ReLU-193          [-1, 256, 32, 32]               0
            ReLU-194          [-1, 256, 32, 32]               0
          Conv2d-195          [-1, 256, 32, 32]         589,824
          Conv2d-196          [-1, 256, 32, 32]         589,824
     BatchNorm2d-197          [-1, 256, 32, 32]             512
     BatchNorm2d-198          [-1, 256, 32, 32]             512
            ReLU-199          [-1, 256, 32, 32]               0
            ReLU-200          [-1, 256, 32, 32]               0
      BasicBlock-201          [-1, 256, 32, 32]               0
      BasicBlock-202          [-1, 256, 32, 32]               0
          Conv2d-203          [-1, 512, 16, 16]       1,179,648
          Conv2d-204          [-1, 512, 16, 16]       1,179,648
     BatchNorm2d-205          [-1, 512, 16, 16]           1,024
     BatchNorm2d-206          [-1, 512, 16, 16]           1,024
            ReLU-207          [-1, 512, 16, 16]               0
            ReLU-208          [-1, 512, 16, 16]               0
          Conv2d-209          [-1, 512, 16, 16]       2,359,296
          Conv2d-210          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-211          [-1, 512, 16, 16]           1,024
     BatchNorm2d-212          [-1, 512, 16, 16]           1,024
          Conv2d-213          [-1, 512, 16, 16]         131,072
          Conv2d-214          [-1, 512, 16, 16]         131,072
     BatchNorm2d-215          [-1, 512, 16, 16]           1,024
     BatchNorm2d-216          [-1, 512, 16, 16]           1,024
            ReLU-217          [-1, 512, 16, 16]               0
            ReLU-218          [-1, 512, 16, 16]               0
      BasicBlock-219          [-1, 512, 16, 16]               0
      BasicBlock-220          [-1, 512, 16, 16]               0
          Conv2d-221          [-1, 512, 16, 16]       2,359,296
          Conv2d-222          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-223          [-1, 512, 16, 16]           1,024
     BatchNorm2d-224          [-1, 512, 16, 16]           1,024
            ReLU-225          [-1, 512, 16, 16]               0
            ReLU-226          [-1, 512, 16, 16]               0
          Conv2d-227          [-1, 512, 16, 16]       2,359,296
          Conv2d-228          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-229          [-1, 512, 16, 16]           1,024
     BatchNorm2d-230          [-1, 512, 16, 16]           1,024
            ReLU-231          [-1, 512, 16, 16]               0
            ReLU-232          [-1, 512, 16, 16]               0
      BasicBlock-233          [-1, 512, 16, 16]               0
      BasicBlock-234          [-1, 512, 16, 16]               0
          Conv2d-235          [-1, 512, 16, 16]       2,359,296
          Conv2d-236          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-237          [-1, 512, 16, 16]           1,024
     BatchNorm2d-238          [-1, 512, 16, 16]           1,024
            ReLU-239          [-1, 512, 16, 16]               0
            ReLU-240          [-1, 512, 16, 16]               0
          Conv2d-241          [-1, 512, 16, 16]       2,359,296
          Conv2d-242          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-243          [-1, 512, 16, 16]           1,024
     BatchNorm2d-244          [-1, 512, 16, 16]           1,024
            ReLU-245          [-1, 512, 16, 16]               0
            ReLU-246          [-1, 512, 16, 16]               0
      BasicBlock-247          [-1, 512, 16, 16]               0
      BasicBlock-248          [-1, 512, 16, 16]               0
          Conv2d-249          [-1, 512, 16, 16]         262,656
            ReLU-250          [-1, 512, 16, 16]               0
        Upsample-251          [-1, 512, 32, 32]               0
          Conv2d-252          [-1, 256, 32, 32]          65,792
            ReLU-253          [-1, 256, 32, 32]               0
          Conv2d-254          [-1, 512, 32, 32]       3,539,456
            ReLU-255          [-1, 512, 32, 32]               0
        Upsample-256          [-1, 512, 64, 64]               0
          Conv2d-257          [-1, 128, 64, 64]          16,512
            ReLU-258          [-1, 128, 64, 64]               0
          Conv2d-259          [-1, 256, 64, 64]       1,474,816
            ReLU-260          [-1, 256, 64, 64]               0
        Upsample-261        [-1, 256, 128, 128]               0
          Conv2d-262         [-1, 64, 128, 128]           4,160
            ReLU-263         [-1, 64, 128, 128]               0
          Conv2d-264        [-1, 256, 128, 128]         737,536
            ReLU-265        [-1, 256, 128, 128]               0
        Upsample-266        [-1, 256, 256, 256]               0
          Conv2d-267         [-1, 64, 256, 256]           4,160
            ReLU-268         [-1, 64, 256, 256]               0
          Conv2d-269        [-1, 128, 256, 256]         368,768
            ReLU-270        [-1, 128, 256, 256]               0
        Upsample-271        [-1, 128, 512, 512]               0
          Conv2d-272         [-1, 64, 512, 512]         110,656
            ReLU-273         [-1, 64, 512, 512]               0
          Conv2d-274          [-1, 1, 512, 512]              65
================================================================
Total params: 49,192,641
Trainable params: 49,192,641
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 2522.00
Params size (MB): 187.66
Estimated Total Size (MB): 2712.66
----------------------------------------------------------------
Start training now...
training: bce: 0.677309, dice: 0.997968, loss: 0.837639
training IoU in current batch 0 is 0.00012342110305986898
training IoU uptillnow 0 is 4.114036768662299e-05
testing: bce: 0.039842, dice: 0.058704, loss: 0.049273
IoU in current test batch is 0.0
training: bce: 0.412623, dice: 0.523395, loss: 0.468009
training IoU in current batch 100 is 0.5269021865280163
training IoU uptillnow 100 is 0.001739358441026654
testing: bce: 2.451464, dice: 3.109581, loss: 2.780522
IoU in current test batch is 0.4230960196991508
training: bce: 0.237095, dice: 0.351643, loss: 0.294369
training IoU in current batch 200 is 0.5742120804290621
training IoU uptillnow 200 is 0.0018262648226536292
testing: bce: 2.803299, dice: 4.157659, loss: 3.480479
IoU in current test batch is 0.7498499016956273
Epoch   264: reducing learning rate of group 0 to 9.9900e-04.
training: bce: 0.257226, dice: 0.330475, loss: 0.293850
training IoU in current batch 300 is 0.4026591734630937
training IoU uptillnow 300 is 0.0016654450293723502
testing: bce: 4.554406, dice: 5.851358, loss: 5.202882
IoU in current test batch is 0.00027086909298600954
Epoch   365: reducing learning rate of group 0 to 9.9800e-04.
training: bce: 0.415889, dice: 0.403656, loss: 0.409772
training IoU in current batch 400 is 0.6121659690280276
training IoU uptillnow 400 is 0.0017589882215721193
testing: bce: 9.810087, dice: 9.521527, loss: 9.665807
IoU in current test batch is 0.595560238667351
Epoch   466: reducing learning rate of group 0 to 9.9700e-04.
training: bce: 0.348607, dice: 0.362792, loss: 0.355700
training IoU in current batch 500 is 0.7850370415536112
training IoU uptillnow 500 is 0.001930206169065117
testing: bce: 10.273668, dice: 10.691690, loss: 10.482679
IoU in current test batch is 0.8154414641737204
Epoch   567: reducing learning rate of group 0 to 9.9601e-04.
training: bce: 0.298138, dice: 0.326324, loss: 0.312231
training IoU in current batch 600 is 0.8160467879009412
training IoU uptillnow 600 is 0.002061645402110822
testing: bce: 10.540039, dice: 11.536505, loss: 11.038272
IoU in current test batch is 0.8037808031288105
Epoch   679: reducing learning rate of group 0 to 9.9501e-04.
training: bce: 0.267824, dice: 0.306910, loss: 0.287367
training IoU in current batch 700 is 0.7732547233016214
training IoU uptillnow 700 is 0.0021352360358095263
testing: bce: 11.043791, dice: 12.655515, loss: 11.849653
IoU in current test batch is 0.7771681107479674
training: bce: 0.240763, dice: 0.285065, loss: 0.262914
training IoU in current batch 800 is 0.8133890427679452
training IoU uptillnow 800 is 0.0022071537353622055
testing: bce: 11.344207, dice: 13.431610, loss: 12.387908
IoU in current test batch is 0.811805818408638
Epoch   856: reducing learning rate of group 0 to 9.9401e-04.
training: bce: 0.234833, dice: 0.291550, loss: 0.263192
training IoU in current batch 900 is 0.718082131829123
training IoU uptillnow 900 is 0.0022278477831685173
testing: bce: 12.446151, dice: 15.452173, loss: 13.949162
IoU in current test batch is 0.723680719092196
Epoch   957: reducing learning rate of group 0 to 9.9302e-04.
training: bce: 0.215835, dice: 0.276516, loss: 0.246176
training IoU in current batch 1000 is 0.7722725248626985
training IoU uptillnow 1000 is 0.0022624525750140527
testing: bce: 12.708899, dice: 16.281911, loss: 14.495405
IoU in current test batch is 0.8048539003762105
training: bce: 0.199471, dice: 0.262062, loss: 0.230767
training IoU in current batch 1100 is 0.7103297752617399
training IoU uptillnow 1100 is 0.0022720178195667394
testing: bce: 12.918687, dice: 16.972392, loss: 14.945539
IoU in current test batch is 0.8361500142874416
training: bce: 0.187163, dice: 0.250837, loss: 0.219000
training IoU in current batch 1200 is 0.8785597591056156
training IoU uptillnow 1200 is 0.0023266818254606037
testing: bce: 13.222501, dice: 17.720908, loss: 15.471705
IoU in current test batch is 0.7951737809924113
Epoch  1207: reducing learning rate of group 0 to 9.9203e-04.
training: bce: 0.174741, dice: 0.238773, loss: 0.206757
training IoU in current batch 1300 is 0.7992365380717519
training IoU uptillnow 1300 is 0.0023526187945698968
testing: bce: 13.372826, dice: 18.273136, loss: 15.822981
IoU in current test batch is 0.8304253454221602
Epoch  1308: reducing learning rate of group 0 to 9.9104e-04.
training: bce: 0.164064, dice: 0.228396, loss: 0.196230
training IoU in current batch 1400 is 0.9219334374852057
training IoU uptillnow 1400 is 0.002404045822672261
testing: bce: 13.520803, dice: 18.822533, loss: 16.171668
IoU in current test batch is 0.8580281277811416
Epoch  1488: reducing learning rate of group 0 to 9.9004e-04.
training: bce: 0.154983, dice: 0.218959, loss: 0.186971
training IoU in current batch 1500 is 0.8174745379102225
training IoU uptillnow 1500 is 0.0024254228582282333
testing: bce: 13.684104, dice: 19.332810, loss: 16.508457
IoU in current test batch is 0.8322523903420027
Epoch  1589: reducing learning rate of group 0 to 9.8905e-04.
training: bce: 0.147395, dice: 0.212009, loss: 0.179702
training IoU in current batch 1600 is 0.8721650595270161
training IoU uptillnow 1600 is 0.0024555161753339063
testing: bce: 13.881098, dice: 19.966225, loss: 16.923662
IoU in current test batch is 0.8441758292097452
Epoch  1690: reducing learning rate of group 0 to 9.8807e-04.
training: bce: 0.139992, dice: 0.204723, loss: 0.172358
training IoU in current batch 1700 is 0.8883557852002214
training IoU uptillnow 1700 is 0.002485243969298251
testing: bce: 14.007467, dice: 20.484335, loss: 17.245901
IoU in current test batch is 0.8201332779684458
Epoch  1791: reducing learning rate of group 0 to 9.8708e-04.
training: bce: 0.133810, dice: 0.198377, loss: 0.166094
training IoU in current batch 1800 is 0.9167533064145651
training IoU uptillnow 1800 is 0.0025169263893658226
testing: bce: 14.176029, dice: 21.016340, loss: 17.596184
IoU in current test batch is 0.8828428995110889
Epoch  1892: reducing learning rate of group 0 to 9.8609e-04.
training: bce: 0.128469, dice: 0.192750, loss: 0.160609
training IoU in current batch 1900 is 0.8795879706369879
training IoU uptillnow 1900 is 0.0025387587677328646
testing: bce: 14.365824, dice: 21.554002, loss: 17.959913
IoU in current test batch is 0.8368591320717952
Epoch  1993: reducing learning rate of group 0 to 9.8510e-04.
training: bce: 0.123308, dice: 0.187440, loss: 0.155374
training IoU in current batch 2000 is 0.8663401730178022
training IoU uptillnow 2000 is 0.0025562021364981392
testing: bce: 14.514032, dice: 22.062754, loss: 18.288393
IoU in current test batch is 0.8379810664568204
Epoch  2094: reducing learning rate of group 0 to 9.8412e-04.
training: bce: 0.118843, dice: 0.183113, loss: 0.150978
training IoU in current batch 2100 is 0.9189070678834718
training IoU uptillnow 2100 is 0.002580325002900492
testing: bce: 14.687603, dice: 22.630668, loss: 18.659135
IoU in current test batch is 0.8712727689707435
training: bce: 0.114526, dice: 0.179127, loss: 0.146826
training IoU in current batch 2200 is 0.8637840787874278
training IoU uptillnow 2200 is 0.002593907704387283
testing: bce: 14.827690, dice: 23.191718, loss: 19.009704
IoU in current test batch is 0.8858454113687512
Epoch  2271: reducing learning rate of group 0 to 9.8314e-04.
training: bce: 0.110692, dice: 0.175235, loss: 0.142963
training IoU in current batch 2300 is 0.8489856566698002
training IoU uptillnow 2300 is 0.0026041660479123614
testing: bce: 14.982456, dice: 23.718535, loss: 19.350495
IoU in current test batch is 0.8648214038222107
Epoch  2372: reducing learning rate of group 0 to 9.8215e-04.
training: bce: 0.107085, dice: 0.171480, loss: 0.139283
training IoU in current batch 2400 is 0.8766357615894039
training IoU uptillnow 2400 is 0.0026174085784157206
testing: bce: 15.124162, dice: 24.219069, loss: 19.671615
IoU in current test batch is 0.8714006757144575
Epoch  2473: reducing learning rate of group 0 to 9.8117e-04.
training: bce: 0.105940, dice: 0.170611, loss: 0.138276
training IoU in current batch 2500 is 0.8844988561182218
training IoU uptillnow 2500 is 0.00263064012347683
testing: bce: 15.585659, dice: 25.099890, loss: 20.342774
IoU in current test batch is 0.8075886830363506
Epoch  2574: reducing learning rate of group 0 to 9.8019e-04.
training: bce: 0.103266, dice: 0.168211, loss: 0.135739
training IoU in current batch 2600 is 0.8738417282106602
training IoU uptillnow 2600 is 0.002641488475542396
testing: bce: 15.799768, dice: 25.736329, loss: 20.768048
IoU in current test batch is 0.8350298385487135
Epoch  2675: reducing learning rate of group 0 to 9.7921e-04.
training: bce: 0.100269, dice: 0.164946, loss: 0.132608
training IoU in current batch 2700 is 0.8327843703469747
training IoU uptillnow 2700 is 0.0026464666105151537
testing: bce: 15.930955, dice: 26.207066, loss: 21.069010
IoU in current test batch is 0.8486091295339248
Epoch  2776: reducing learning rate of group 0 to 9.7823e-04.
training: bce: 0.097688, dice: 0.162040, loss: 0.129864
training IoU in current batch 2800 is 0.913238383113889
training IoU uptillnow 2800 is 0.0026606637305864786
testing: bce: 16.095596, dice: 26.698468, loss: 21.397032
IoU in current test batch is 0.8427453707318661
Epoch  2877: reducing learning rate of group 0 to 9.7725e-04.
training: bce: 0.094991, dice: 0.159161, loss: 0.127076
training IoU in current batch 2900 is 0.8478171390939275
training IoU uptillnow 2900 is 0.0026663649853167996
testing: bce: 16.209956, dice: 27.160369, loss: 21.685163
IoU in current test batch is 0.864518599896599
training: bce: 0.092506, dice: 0.156465, loss: 0.124486
training IoU in current batch 3000 is 0.8623125148797715
training IoU uptillnow 3000 is 0.0026732963436734286
testing: bce: 16.330088, dice: 27.620628, loss: 21.975358
IoU in current test batch is 0.8322997581658759
Epoch  3079: reducing learning rate of group 0 to 9.7627e-04.
training: bce: 0.090580, dice: 0.154480, loss: 0.122530
training IoU in current batch 3100 is 0.8632565528196982
training IoU uptillnow 3100 is 0.0026798821385479498
testing: bce: 16.522922, dice: 28.179031, loss: 22.350977
IoU in current test batch is 0.8635951925042678
Epoch  3180: reducing learning rate of group 0 to 9.7530e-04.
training: bce: 0.088549, dice: 0.152278, loss: 0.120414
training IoU in current batch 3200 is 0.9042898745519713
training IoU uptillnow 3200 is 0.002690329418875721
testing: bce: 16.673337, dice: 28.673101, loss: 22.673219
IoU in current test batch is 0.850477627112479
Epoch  3281: reducing learning rate of group 0 to 9.7432e-04.
training: bce: 0.086514, dice: 0.150054, loss: 0.118284
training IoU in current batch 3300 is 0.9276027432578765
training IoU uptillnow 3300 is 0.0027024978443624583
testing: bce: 16.798990, dice: 29.136963, loss: 22.967977
IoU in current test batch is 0.8373238434961499
Epoch  3382: reducing learning rate of group 0 to 9.7335e-04.
training: bce: 0.084643, dice: 0.147902, loss: 0.116273
training IoU in current batch 3400 is 0.9239800410918697
training IoU uptillnow 3400 is 0.0027135956281302844
testing: bce: 16.933586, dice: 29.589162, loss: 23.261374
IoU in current test batch is 0.8795231373924292
training: bce: 0.082773, dice: 0.145887, loss: 0.114330
training IoU in current batch 3500 is 0.8740194068607353
training IoU uptillnow 3500 is 0.0027193026374058866
testing: bce: 17.046311, dice: 30.044052, loss: 23.545182
IoU in current test batch is 0.8547688953193405
training: bce: 0.080987, dice: 0.143927, loss: 0.112457
training IoU in current batch 3600 is 0.9211234023676842
training IoU uptillnow 3600 is 0.0027290529485366763
testing: bce: 17.154969, dice: 30.487148, loss: 23.821058
IoU in current test batch is 0.8297157604745528
Epoch  3602: reducing learning rate of group 0 to 9.7237e-04.
training: bce: 0.079283, dice: 0.141971, loss: 0.110627
training IoU in current batch 3700 is 0.938758743720952
training IoU uptillnow 3700 is 0.0027398646984385
testing: bce: 17.260441, dice: 30.907984, loss: 24.084213
IoU in current test batch is 0.8426500472591993
Epoch  3703: reducing learning rate of group 0 to 9.7140e-04.
training: bce: 0.077735, dice: 0.140168, loss: 0.108952
training IoU in current batch 3800 is 0.8622862571247625
training IoU uptillnow 3800 is 0.002743401210548753
testing: bce: 17.380703, dice: 31.339894, loss: 24.360299
IoU in current test batch is 0.8431719147580703
Epoch  3804: reducing learning rate of group 0 to 9.7043e-04.
training: bce: 0.076350, dice: 0.138491, loss: 0.107421
training IoU in current batch 3900 is 0.8852624212736179
training IoU uptillnow 3900 is 0.0027487196808648248
testing: bce: 17.520170, dice: 31.779566, loss: 24.649868
IoU in current test batch is 0.8703353510485641
Epoch  3957: reducing learning rate of group 0 to 9.6946e-04.
training: bce: 0.074839, dice: 0.136597, loss: 0.105718
training IoU in current batch 4000 is 0.9230705895268416
training IoU uptillnow 4000 is 0.002756922187343821
testing: bce: 17.613523, dice: 32.148534, loss: 24.881028
IoU in current test batch is 0.893467536593641
Epoch  4058: reducing learning rate of group 0 to 9.6849e-04.
training: bce: 0.073399, dice: 0.134788, loss: 0.104094
training IoU in current batch 4100 is 0.940568208256857
training IoU uptillnow 4100 is 0.002766146892867166
testing: bce: 17.706515, dice: 32.515590, loss: 25.111053
IoU in current test batch is 0.8754968970938384
training: bce: 0.072038, dice: 0.133133, loss: 0.102586
training IoU in current batch 4200 is 0.9063138076423325
training IoU uptillnow 4200 is 0.0027722144751715526
testing: bce: 17.801905, dice: 32.899515, loss: 25.350710
IoU in current test batch is 0.898811972626162
Epoch  4246: reducing learning rate of group 0 to 9.6752e-04.
training: bce: 0.070817, dice: 0.131693, loss: 0.101255
training IoU in current batch 4300 is 0.9325289853175953
training IoU uptillnow 4300 is 0.0027800316217859934
testing: bce: 17.916797, dice: 33.318345, loss: 25.617571
IoU in current test batch is 0.9028153020140641
Epoch  4347: reducing learning rate of group 0 to 9.6656e-04.
training: bce: 0.069716, dice: 0.130282, loss: 0.099999
training IoU in current batch 4400 is 0.8935396102252594
training IoU uptillnow 4400 is 0.0027845404549064557
testing: bce: 18.048342, dice: 33.727705, loss: 25.888023
IoU in current test batch is 0.8912523805245001
Epoch  4448: reducing learning rate of group 0 to 9.6559e-04.
training: bce: 0.068835, dice: 0.129165, loss: 0.099000
training IoU in current batch 4500 is 0.8806298276886512
training IoU uptillnow 4500 is 0.002787892872237176
testing: bce: 18.225185, dice: 34.198260, loss: 26.211722
IoU in current test batch is 0.8539883570474935
Epoch  4549: reducing learning rate of group 0 to 9.6462e-04.
training: bce: 0.067746, dice: 0.127890, loss: 0.097818
training IoU in current batch 4600 is 0.8325887087841081
training IoU uptillnow 4600 is 0.0027876190800987244
testing: bce: 18.335196, dice: 34.613053, loss: 26.474124
IoU in current test batch is 0.9078982745424714
Epoch  4650: reducing learning rate of group 0 to 9.6366e-04.
training: bce: 0.066773, dice: 0.126594, loss: 0.096683
training IoU in current batch 4700 is 0.895877234301014
training IoU uptillnow 4700 is 0.002791844529313175
testing: bce: 18.464604, dice: 35.006861, loss: 26.735733
IoU in current test batch is 0.8882485770628762
Epoch  4751: reducing learning rate of group 0 to 9.6269e-04.
training: bce: 0.065880, dice: 0.125497, loss: 0.095689
training IoU in current batch 4800 is 0.9141732508057845
training IoU uptillnow 4800 is 0.002797164246872838
testing: bce: 18.605347, dice: 35.441843, loss: 27.023595
IoU in current test batch is 0.818072465130313
Epoch  4852: reducing learning rate of group 0 to 9.6173e-04.
training: bce: 0.064987, dice: 0.124549, loss: 0.094768
training IoU in current batch 4900 is 0.8954643126778559
training IoU uptillnow 4900 is 0.0028009944202127012
testing: bce: 18.735399, dice: 35.906698, loss: 27.321048
IoU in current test batch is 0.8712619450834838
training: bce: 0.064007, dice: 0.123313, loss: 0.093660
training IoU in current batch 5000 is 0.9122399050456604
training IoU uptillnow 5000 is 0.0028057895664489107
testing: bce: 18.829442, dice: 36.275850, loss: 27.552646
IoU in current test batch is 0.8968519818803049
Epoch  5020: reducing learning rate of group 0 to 9.6077e-04.
training: bce: 0.063509, dice: 0.122588, loss: 0.093048
training IoU in current batch 5100 is 0.9104477611940298
training IoU uptillnow 5100 is 0.002810279593976804
testing: bce: 19.056475, dice: 36.783488, loss: 27.919982
IoU in current test batch is 0.8890205412323373
Epoch  5121: reducing learning rate of group 0 to 9.5981e-04.
training: bce: 0.062638, dice: 0.121534, loss: 0.092086
training IoU in current batch 5200 is 0.9146092548581015
training IoU uptillnow 5200 is 0.0028148636724658806
testing: bce: 19.163642, dice: 37.182112, loss: 28.172877
IoU in current test batch is 0.8757990607877573
Epoch  5300: reducing learning rate of group 0 to 9.5885e-04.
training: bce: 0.061846, dice: 0.120512, loss: 0.091179
training IoU in current batch 5300 is 0.9292193169022895
training IoU uptillnow 5300 is 0.002820193497980722
testing: bce: 19.285085, dice: 37.578505, loss: 28.431795
IoU in current test batch is 0.8766215450842405
training: bce: 0.061154, dice: 0.119517, loss: 0.090336
training IoU in current batch 5400 is 0.8847025043866645
training IoU uptillnow 5400 is 0.002822578516495346
testing: bce: 19.429060, dice: 37.971353, loss: 28.700206
IoU in current test batch is 0.8645992305575491
Epoch  5401: reducing learning rate of group 0 to 9.5789e-04.
training: bce: 0.060356, dice: 0.118515, loss: 0.089435
training IoU in current batch 5500 is 0.8920950258715111
training IoU uptillnow 5500 is 0.002825324772989493
testing: bce: 19.530390, dice: 38.350095, loss: 28.940243
IoU in current test batch is 0.8702547629180453
Epoch  5502: reducing learning rate of group 0 to 9.5693e-04.
training: bce: 0.059836, dice: 0.117789, loss: 0.088812
training IoU in current batch 5600 is 0.8835971969893589
training IoU uptillnow 5600 is 0.0028274672335675153
testing: bce: 19.714047, dice: 38.807852, loss: 29.260949
IoU in current test batch is 0.8605272585982137
Epoch  5603: reducing learning rate of group 0 to 9.5598e-04.
training: bce: 0.059083, dice: 0.116873, loss: 0.087978
training IoU in current batch 5700 is 0.8786430960344004
training IoU uptillnow 5700 is 0.0028292448705881635
testing: bce: 19.813688, dice: 39.193688, loss: 29.503688
IoU in current test batch is 0.8803407108216161
Epoch  5704: reducing learning rate of group 0 to 9.5502e-04.
training: bce: 0.058581, dice: 0.116086, loss: 0.087334
training IoU in current batch 5800 is 0.850033553491329
training IoU uptillnow 5800 is 0.0028293172772028206
testing: bce: 19.989915, dice: 39.612740, loss: 29.801328
IoU in current test batch is 0.8710443988909882
Epoch  5805: reducing learning rate of group 0 to 9.5406e-04.
training: bce: 0.058119, dice: 0.115482, loss: 0.086800
training IoU in current batch 5900 is 0.9275030525030525
training IoU uptillnow 5900 is 0.002833763295919547
testing: bce: 20.174062, dice: 40.085796, loss: 30.129929
IoU in current test batch is 0.8862433423664812
Epoch  5906: reducing learning rate of group 0 to 9.5311e-04.
training: bce: 0.057421, dice: 0.114612, loss: 0.086016
training IoU in current batch 6000 is 0.9038078921189344
training IoU uptillnow 6000 is 0.0028367449602723256
testing: bce: 20.269532, dice: 40.458052, loss: 30.363792
IoU in current test batch is 0.8706638929988062
Epoch  6007: reducing learning rate of group 0 to 9.5216e-04.
training: bce: 0.056922, dice: 0.113895, loss: 0.085408
training IoU in current batch 6100 is 0.9528741584671155
training IoU uptillnow 6100 is 0.002842309658430301
testing: bce: 20.428153, dice: 40.874764, loss: 30.651458
IoU in current test batch is 0.8834273832090476
Epoch  6108: reducing learning rate of group 0 to 9.5121e-04.
training: bce: 0.056324, dice: 0.113111, loss: 0.084718
training IoU in current batch 6200 is 0.8737217544414095
training IoU uptillnow 6200 is 0.002843440059812461
testing: bce: 20.545167, dice: 41.258787, loss: 30.901977
IoU in current test batch is 0.8564034113077411
Epoch  6209: reducing learning rate of group 0 to 9.5025e-04.
training: bce: 0.055698, dice: 0.112361, loss: 0.084029
training IoU in current batch 6300 is 0.9013525653823118
training IoU uptillnow 6300 is 0.0028459962967821786
testing: bce: 20.644129, dice: 41.646422, loss: 31.145276
IoU in current test batch is 0.8869340010610987
Epoch  6310: reducing learning rate of group 0 to 9.4930e-04.
training: bce: 0.055092, dice: 0.111610, loss: 0.083351
training IoU in current batch 6400 is 0.8965880826213907
training IoU uptillnow 6400 is 0.002848224552449873
testing: bce: 20.743702, dice: 42.024500, loss: 31.384101
IoU in current test batch is 0.8986875034731131
Epoch  6411: reducing learning rate of group 0 to 9.4835e-04.
training: bce: 0.054474, dice: 0.110770, loss: 0.082622
training IoU in current batch 6500 is 0.90173304510314
training IoU uptillnow 6500 is 0.002850648060595706
testing: bce: 20.831465, dice: 42.359856, loss: 31.595660
IoU in current test batch is 0.8648661946544174
Epoch  6512: reducing learning rate of group 0 to 9.4741e-04.
training: bce: 0.053893, dice: 0.109999, loss: 0.081946
training IoU in current batch 6600 is 0.9311311857844841
training IoU uptillnow 6600 is 0.00285448266987742
testing: bce: 20.926484, dice: 42.712065, loss: 31.819275
IoU in current test batch is 0.8721080103685362
Epoch  6673: reducing learning rate of group 0 to 9.4646e-04.
training: bce: 0.053389, dice: 0.109315, loss: 0.081352
training IoU in current batch 6700 is 0.9123677248677249
training IoU uptillnow 6700 is 0.0028572694640824885
testing: bce: 21.044660, dice: 43.089591, loss: 32.067125
IoU in current test batch is 0.8787841402654938
Epoch  6774: reducing learning rate of group 0 to 9.4551e-04.
training: bce: 0.052913, dice: 0.108711, loss: 0.080812
training IoU in current batch 6800 is 0.9092543234364346
training IoU uptillnow 6800 is 0.002859821710527212
testing: bce: 21.168512, dice: 43.490683, loss: 32.329598
IoU in current test batch is 0.8525987351317024
Epoch  6875: reducing learning rate of group 0 to 9.4457e-04.
training: bce: 0.052413, dice: 0.108056, loss: 0.080234
training IoU in current batch 6900 is 0.9349354020239727
training IoU uptillnow 6900 is 0.0028635404415742007
testing: bce: 21.276639, dice: 43.864219, loss: 32.570429
IoU in current test batch is 0.8849310056377561
Epoch  6976: reducing learning rate of group 0 to 9.4362e-04.
training: bce: 0.052072, dice: 0.107512, loss: 0.079792
training IoU in current batch 7000 is 0.9451327910359317
training IoU uptillnow 7000 is 0.002867638458931896
testing: bce: 21.444270, dice: 44.275934, loss: 32.860102
IoU in current test batch is 0.891078213354319
Epoch  7077: reducing learning rate of group 0 to 9.4268e-04.
training: bce: 0.051623, dice: 0.106992, loss: 0.079308
training IoU in current batch 7100 is 0.9167938035441849
training IoU uptillnow 7100 is 0.002870290773904652
testing: bce: 21.563400, dice: 44.691114, loss: 33.127257
IoU in current test batch is 0.9059635401599108
Epoch  7178: reducing learning rate of group 0 to 9.4174e-04.
training: bce: 0.051203, dice: 0.106428, loss: 0.078815
training IoU in current batch 7200 is 0.9435837474422684
training IoU uptillnow 7200 is 0.0028741095266367205
testing: bce: 21.688797, dice: 45.081571, loss: 33.385184
IoU in current test batch is 0.8870040857349092
Epoch  7279: reducing learning rate of group 0 to 9.4079e-04.
training: bce: 0.051461, dice: 0.106178, loss: 0.078820
training IoU in current batch 7300 is 0.703103448275862
training IoU uptillnow 7300 is 0.002866844338775918
testing: bce: 22.100792, dice: 45.600533, loss: 33.850663
IoU in current test batch is 0.8964344241419284
Epoch  7380: reducing learning rate of group 0 to 9.3985e-04.
training: bce: 0.050997, dice: 0.105597, loss: 0.078297
training IoU in current batch 7400 is 0.9177819137936128
training IoU uptillnow 7400 is 0.0028694443753547964
testing: bce: 22.201527, dice: 45.971975, loss: 34.086751
IoU in current test batch is 0.8791183348009634
Epoch  7481: reducing learning rate of group 0 to 9.3891e-04.
training: bce: 0.050534, dice: 0.105055, loss: 0.077794
training IoU in current batch 7500 is 0.911559156410415
training IoU uptillnow 7500 is 0.002871698556744121
testing: bce: 22.297458, dice: 46.353769, loss: 34.325614
IoU in current test batch is 0.8973972391191698
Epoch  7582: reducing learning rate of group 0 to 9.3797e-04.
training: bce: 0.050203, dice: 0.104681, loss: 0.077442
training IoU in current batch 7600 is 0.8876932355648668
training IoU uptillnow 7600 is 0.002872846812172864
testing: bce: 22.446596, dice: 46.804605, loss: 34.625600
IoU in current test batch is 0.868956248985491
Epoch  7683: reducing learning rate of group 0 to 9.3704e-04.
training: bce: 0.049775, dice: 0.104192, loss: 0.076983
training IoU in current batch 7700 is 0.9224501336623483
training IoU uptillnow 7700 is 0.002875469678900583
testing: bce: 22.548274, dice: 47.198761, loss: 34.873517
IoU in current test batch is 0.8544905582762234
Epoch  7784: reducing learning rate of group 0 to 9.3610e-04.
training: bce: 0.049394, dice: 0.103672, loss: 0.076533
training IoU in current batch 7800 is 0.8382674396502566
training IoU uptillnow 7800 is 0.0028744282113955656
testing: bce: 22.666235, dice: 47.573354, loss: 35.119795
IoU in current test batch is 0.8960196619797187
Epoch  7885: reducing learning rate of group 0 to 9.3516e-04.
training: bce: 0.049070, dice: 0.103221, loss: 0.076146
training IoU in current batch 7900 is 0.9244073816693692
training IoU uptillnow 7900 is 0.002877047243511783
testing: bce: 22.806178, dice: 47.973378, loss: 35.389778
IoU in current test batch is 0.8942094354651735
Epoch  7986: reducing learning rate of group 0 to 9.3423e-04.
training: bce: 0.048758, dice: 0.102801, loss: 0.075780
training IoU in current batch 8000 is 0.935880898634849
training IoU uptillnow 8000 is 0.00288007881146501
testing: bce: 22.947904, dice: 48.383089, loss: 35.665496
IoU in current test batch is 0.8606123759720233
Epoch  8087: reducing learning rate of group 0 to 9.3329e-04.
training: bce: 0.048405, dice: 0.102304, loss: 0.075354
training IoU in current batch 8100 is 0.9099595289535344
training IoU uptillnow 8100 is 0.0028819689437743564
testing: bce: 23.066234, dice: 48.750983, loss: 35.908609
IoU in current test batch is 0.879648977693001
Epoch  8188: reducing learning rate of group 0 to 9.3236e-04.
training: bce: 0.048010, dice: 0.101844, loss: 0.074927
training IoU in current batch 8200 is 0.9000028668902841
training IoU uptillnow 8200 is 0.002883408287909542
testing: bce: 23.160463, dice: 49.130619, loss: 36.145541
IoU in current test batch is 0.8641520118731183
Epoch  8289: reducing learning rate of group 0 to 9.3143e-04.
training: bce: 0.047748, dice: 0.101460, loss: 0.074604
training IoU in current batch 8300 is 0.8802418460173064
training IoU uptillnow 8300 is 0.002884019433540368
testing: bce: 23.315099, dice: 49.542114, loss: 36.428606
IoU in current test batch is 0.8979714312269452
Epoch  8390: reducing learning rate of group 0 to 9.3050e-04.
training: bce: 0.047354, dice: 0.100925, loss: 0.074139
training IoU in current batch 8400 is 0.9006768300696544
training IoU uptillnow 8400 is 0.002885426845356721
testing: bce: 23.401037, dice: 49.874850, loss: 36.637943
IoU in current test batch is 0.8780909046973694
Epoch  8491: reducing learning rate of group 0 to 9.2957e-04.
training: bce: 0.046972, dice: 0.100447, loss: 0.073710
training IoU in current batch 8500 is 0.888703552542854
training IoU uptillnow 8500 is 0.0028863316604347835
testing: bce: 23.489009, dice: 50.229258, loss: 36.859134
IoU in current test batch is 0.8679163449554621
Epoch  8592: reducing learning rate of group 0 to 9.2864e-04.
training: bce: 0.046712, dice: 0.100168, loss: 0.073440
training IoU in current batch 8600 is 0.9158368644067797
training IoU uptillnow 8600 is 0.0028882669922286196
testing: bce: 23.633730, dice: 50.679271, loss: 37.156501
IoU in current test batch is 0.8606757913200827
Epoch  8693: reducing learning rate of group 0 to 9.2771e-04.
training: bce: 0.046375, dice: 0.099724, loss: 0.073049
training IoU in current batch 8700 is 0.9118957931966529
training IoU uptillnow 8700 is 0.0028900068572069006
testing: bce: 23.735595, dice: 51.041042, loss: 37.388318
IoU in current test batch is 0.895332897191849
Epoch  8794: reducing learning rate of group 0 to 9.2678e-04.
training: bce: 0.046057, dice: 0.099294, loss: 0.072676
training IoU in current batch 8800 is 0.9110187633799269
training IoU uptillnow 8800 is 0.0028916739672405277
testing: bce: 23.843910, dice: 51.405190, loss: 37.624550
IoU in current test batch is 0.8997280510383191
Epoch  8895: reducing learning rate of group 0 to 9.2585e-04.
training: bce: 0.045763, dice: 0.099032, loss: 0.072397
training IoU in current batch 8900 is 0.7698594561564314
training IoU uptillnow 8900 is 0.0028880173468602056
testing: bce: 23.960988, dice: 51.851837, loss: 37.906412
IoU in current test batch is 0.8738363582970512
Epoch  8996: reducing learning rate of group 0 to 9.2493e-04.
training: bce: 0.045442, dice: 0.098639, loss: 0.072041
training IoU in current batch 9000 is 0.862778532412358
training IoU uptillnow 9000 is 0.0028878830406110596
testing: bce: 24.060332, dice: 52.226634, loss: 38.143483
IoU in current test batch is 0.91964542506684
Epoch  9097: reducing learning rate of group 0 to 9.2400e-04.
training: bce: 0.045121, dice: 0.098199, loss: 0.071660
training IoU in current batch 9100 is 0.9246929620277989
training IoU uptillnow 9100 is 0.00289001936445256
testing: bce: 24.155726, dice: 52.571225, loss: 38.363475
IoU in current test batch is 0.8846419904790748
Epoch  9198: reducing learning rate of group 0 to 9.2308e-04.
training: bce: 0.044802, dice: 0.097756, loss: 0.071279
training IoU in current batch 9200 is 0.9246774344715549
training IoU uptillnow 9200 is 0.002892108688987422
testing: bce: 24.248370, dice: 52.909157, loss: 38.578763
IoU in current test batch is 0.884032860577797
Epoch  9299: reducing learning rate of group 0 to 9.2216e-04.
training: bce: 0.044528, dice: 0.097380, loss: 0.070954
training IoU in current batch 9300 is 0.8315721196724413
training IoU uptillnow 9300 is 0.002890816337375632
testing: bce: 24.362327, dice: 53.278116, loss: 38.820221
IoU in current test batch is 0.8806448732500691
Epoch  9400: reducing learning rate of group 0 to 9.2123e-04.
training: bce: 0.044394, dice: 0.097102, loss: 0.070748
training IoU in current batch 9400 is 0.7498438730348367
training IoU uptillnow 9400 is 0.0028866536231899826
testing: bce: 24.550093, dice: 53.697632, loss: 39.123863
IoU in current test batch is 0.9024607243029846
training: bce: 0.044111, dice: 0.096775, loss: 0.070443
training IoU in current batch 9500 is 0.945987860637985
training IoU uptillnow 9500 is 0.002889460056677019
testing: bce: 24.652702, dice: 54.086000, loss: 39.369351
IoU in current test batch is 0.8741815910626869
Epoch  9501: reducing learning rate of group 0 to 9.2031e-04.
training: bce: 0.043881, dice: 0.096477, loss: 0.070179
training IoU in current batch 9600 is 0.9536951579136933
training IoU uptillnow 9600 is 0.002892475615504592
testing: bce: 24.782567, dice: 54.486564, loss: 39.634566
IoU in current test batch is 0.8462010029609676
Epoch  9602: reducing learning rate of group 0 to 9.1939e-04.
training: bce: 0.043696, dice: 0.096115, loss: 0.069906
training IoU in current batch 9700 is 0.9097086804940022
training IoU uptillnow 9700 is 0.002893917597288004
testing: bce: 24.935148, dice: 54.847780, loss: 39.891464
IoU in current test batch is 0.9122363649453646
Epoch  9703: reducing learning rate of group 0 to 9.1847e-04.
training: bce: 0.043515, dice: 0.095961, loss: 0.069738
training IoU in current batch 9800 is 0.9464620493866838
training IoU uptillnow 9800 is 0.0028965801409128137
testing: bce: 25.087818, dice: 55.324272, loss: 40.206045
IoU in current test batch is 0.8771439436273072
Epoch  9804: reducing learning rate of group 0 to 9.1755e-04.
training: bce: 0.043280, dice: 0.095624, loss: 0.069452
training IoU in current batch 9900 is 0.9182897584371528
training IoU uptillnow 9900 is 0.0028982404350300175
testing: bce: 25.206740, dice: 55.692415, loss: 40.449577
IoU in current test batch is 0.8924889118923358
Epoch  9905: reducing learning rate of group 0 to 9.1664e-04.
training: bce: 0.043015, dice: 0.095285, loss: 0.069150
training IoU in current batch 10000 is 0.9491830985915493
training IoU uptillnow 10000 is 0.002900897201622776
testing: bce: 25.305396, dice: 56.055637, loss: 40.680517
IoU in current test batch is 0.9034835818283922
Epoch 10006: reducing learning rate of group 0 to 9.1572e-04.
training: bce: 0.042747, dice: 0.094947, loss: 0.068847
training IoU in current batch 10100 is 0.927249910362137
training IoU uptillnow 10100 is 0.0029027775682490277
testing: bce: 25.399146, dice: 56.415454, loss: 40.907300
IoU in current test batch is 0.9109427765490685
Epoch 10107: reducing learning rate of group 0 to 9.1480e-04.
training: bce: 0.042502, dice: 0.094581, loss: 0.068542
training IoU in current batch 10200 is 0.9017475837266802
training IoU uptillnow 10200 is 0.0029037877408873954
testing: bce: 25.503906, dice: 56.753985, loss: 41.128945
IoU in current test batch is 0.8957820651373636
Epoch 10208: reducing learning rate of group 0 to 9.1389e-04.
training: bce: 0.042241, dice: 0.094236, loss: 0.068238
training IoU in current batch 10300 is 0.9447329006490265
training IoU uptillnow 10300 is 0.002906169275961104
testing: bce: 25.595316, dice: 57.101680, loss: 41.348498
IoU in current test batch is 0.8866009777212306
Epoch 10309: reducing learning rate of group 0 to 9.1298e-04.
training: bce: 0.041959, dice: 0.093821, loss: 0.067890
training IoU in current batch 10400 is 0.9160070491950177
training IoU uptillnow 10400 is 0.002907584404839952
testing: bce: 25.671271, dice: 57.401863, loss: 41.536567
IoU in current test batch is 0.8900175682596568
training: bce: 0.041695, dice: 0.093479, loss: 0.067587
training IoU in current batch 10500 is 0.9214389672846928
training IoU uptillnow 10500 is 0.002909145006872542
testing: bce: 25.755073, dice: 57.742490, loss: 41.748781
IoU in current test batch is 0.9142101178229917
Epoch 10503: reducing learning rate of group 0 to 9.1206e-04.
training: bce: 0.041445, dice: 0.093132, loss: 0.067288
training IoU in current batch 10600 is 0.9228764597418562
training IoU uptillnow 10600 is 0.00291072136626254
testing: bce: 25.844360, dice: 58.076007, loss: 41.960184
IoU in current test batch is 0.898255550802043
Epoch 10604: reducing learning rate of group 0 to 9.1115e-04.
training: bce: 0.041265, dice: 0.092927, loss: 0.067096
training IoU in current batch 10700 is 0.6645632486123284
training IoU uptillnow 10700 is 0.00290422187520979
testing: bce: 25.974898, dice: 58.495115, loss: 42.235006
IoU in current test batch is 0.9058106363459578
Epoch 10705: reducing learning rate of group 0 to 9.1024e-04.
training: bce: 0.041016, dice: 0.092611, loss: 0.066814
training IoU in current batch 10800 is 0.9483437950055705
training IoU uptillnow 10800 is 0.0029066005818864133
testing: bce: 26.059709, dice: 58.840978, loss: 42.450344
IoU in current test batch is 0.8564691611438847
Epoch 10806: reducing learning rate of group 0 to 9.0933e-04.
training: bce: 0.040836, dice: 0.092360, loss: 0.066598
training IoU in current batch 10900 is 0.906699358925242
training IoU uptillnow 10900 is 0.002907662233244372
testing: bce: 26.185401, dice: 59.224351, loss: 42.704876
IoU in current test batch is 0.9102018104441958
Epoch 10907: reducing learning rate of group 0 to 9.0842e-04.
training: bce: 0.040634, dice: 0.092197, loss: 0.066416
training IoU in current batch 11000 is 0.9051089517078916
training IoU uptillnow 11000 is 0.0029086563938277906
testing: bce: 26.295042, dice: 59.662271, loss: 42.978656
IoU in current test batch is 0.8845650376091021
Epoch 11008: reducing learning rate of group 0 to 9.0751e-04.
training: bce: 0.040435, dice: 0.091971, loss: 0.066203
training IoU in current batch 11100 is 0.8548909310919972
training IoU uptillnow 11100 is 0.0029081247304023833
testing: bce: 26.404139, dice: 60.057387, loss: 43.230763
IoU in current test batch is 0.8976672969837048
Epoch 11109: reducing learning rate of group 0 to 9.0660e-04.
training: bce: 0.040214, dice: 0.091656, loss: 0.065935
training IoU in current batch 11200 is 0.9352132575869072
training IoU uptillnow 11200 is 0.002909992892128009
testing: bce: 26.496189, dice: 60.390764, loss: 43.443477
IoU in current test batch is 0.8448401182082863
Epoch 11210: reducing learning rate of group 0 to 9.0570e-04.
training: bce: 0.040009, dice: 0.091391, loss: 0.065700
training IoU in current batch 11300 is 0.9271787785021179
training IoU uptillnow 11300 is 0.002911591007659488
testing: bce: 26.596902, dice: 60.753300, loss: 43.675101
IoU in current test batch is 0.9115644913780371
Epoch 11311: reducing learning rate of group 0 to 9.0479e-04.
training: bce: 0.039792, dice: 0.091100, loss: 0.065446
training IoU in current batch 11400 is 0.9107652613087396
training IoU uptillnow 11400 is 0.0029126812032274465
testing: bce: 26.686083, dice: 61.096185, loss: 43.891134
IoU in current test batch is 0.9059671035171942
Epoch 11412: reducing learning rate of group 0 to 9.0389e-04.
training: bce: 0.039580, dice: 0.090788, loss: 0.065184
training IoU in current batch 11500 is 0.8513287368632438
training IoU uptillnow 11500 is 0.002912029792506495
testing: bce: 26.777035, dice: 61.420835, loss: 44.098935
IoU in current test batch is 0.8977746648056842
Epoch 11513: reducing learning rate of group 0 to 9.0298e-04.
training: bce: 0.039355, dice: 0.090471, loss: 0.064913
training IoU in current batch 11600 is 0.9208562725316491
training IoU uptillnow 11600 is 0.0029133873575089286
testing: bce: 26.856486, dice: 61.738425, loss: 44.297456
IoU in current test batch is 0.8597453571250276
Epoch 11614: reducing learning rate of group 0 to 9.0208e-04.
training: bce: 0.039238, dice: 0.090274, loss: 0.064756
training IoU in current batch 11700 is 0.941333060305109
training IoU uptillnow 11700 is 0.0029153050526646825
testing: bce: 27.007159, dice: 62.135372, loss: 44.571265
IoU in current test batch is 0.9000329922077595
Epoch 11715: reducing learning rate of group 0 to 9.0118e-04.
training: bce: 0.039080, dice: 0.090047, loss: 0.064564
training IoU in current batch 11800 is 0.9006938533364286
training IoU uptillnow 11800 is 0.002916042344350049
testing: bce: 27.128346, dice: 62.508644, loss: 44.818495
IoU in current test batch is 0.8927438390065007
Epoch 11816: reducing learning rate of group 0 to 9.0028e-04.
training: bce: 0.038867, dice: 0.089734, loss: 0.064300
training IoU in current batch 11900 is 0.9424509495331025
training IoU uptillnow 11900 is 0.0029179368138968122
testing: bce: 27.209306, dice: 62.818736, loss: 45.014021
IoU in current test batch is 0.8847371845210842
Epoch 11917: reducing learning rate of group 0 to 8.9938e-04.
training: bce: 0.038681, dice: 0.089505, loss: 0.064093
training IoU in current batch 12000 is 0.9398563965308696
training IoU uptillnow 12000 is 0.0029197276466707985
testing: bce: 27.306459, dice: 63.185525, loss: 45.245992
IoU in current test batch is 0.8824193321170617
Epoch 12018: reducing learning rate of group 0 to 8.9848e-04.
training: bce: 0.038522, dice: 0.089299, loss: 0.063910
training IoU in current batch 12100 is 0.9613959072875339
training IoU uptillnow 12100 is 0.0029220822072659643
testing: bce: 27.420834, dice: 63.565143, loss: 45.492989
IoU in current test batch is 0.8745687701442365
Epoch 12119: reducing learning rate of group 0 to 8.9758e-04.
training: bce: 0.038330, dice: 0.088991, loss: 0.063660
training IoU in current batch 12200 is 0.9334743490436751
training IoU uptillnow 12200 is 0.002923635350092068
testing: bce: 27.509464, dice: 63.869258, loss: 45.689361
IoU in current test batch is 0.895905160756138
Epoch 12220: reducing learning rate of group 0 to 8.9668e-04.
training: bce: 0.038138, dice: 0.088741, loss: 0.063439
training IoU in current batch 12300 is 0.9202768612824559
training IoU uptillnow 12300 is 0.0029248056141967438
testing: bce: 27.595947, dice: 64.211663, loss: 45.903805
IoU in current test batch is 0.9075959551593398
Epoch 12321: reducing learning rate of group 0 to 8.9578e-04.
training: bce: 0.038025, dice: 0.088571, loss: 0.063298
training IoU in current batch 12400 is 0.9531824798200345
training IoU uptillnow 12400 is 0.0029268414929044017
testing: bce: 27.737891, dice: 64.610249, loss: 46.174070
IoU in current test batch is 0.8659394441036972
Epoch 12422: reducing learning rate of group 0 to 8.9489e-04.
training: bce: 0.037866, dice: 0.088398, loss: 0.063132
training IoU in current batch 12500 is 0.9251580739299611
training IoU uptillnow 12500 is 0.0029280975424486686
testing: bce: 27.844814, dice: 65.003908, loss: 46.424361
IoU in current test batch is 0.8554015359484627
Epoch 12523: reducing learning rate of group 0 to 8.9399e-04.
training: bce: 0.037672, dice: 0.088114, loss: 0.062893
training IoU in current batch 12600 is 0.9064298685709009
training IoU uptillnow 12600 is 0.0029288382404312704
testing: bce: 27.923640, dice: 65.312939, loss: 46.618289
IoU in current test batch is 0.9017944315840544
Epoch 12624: reducing learning rate of group 0 to 8.9310e-04.
training: bce: 0.037499, dice: 0.087879, loss: 0.062689
training IoU in current batch 12700 is 0.9383083925472346
training IoU uptillnow 12700 is 0.0029304039155859263
testing: bce: 28.016174, dice: 65.656022, loss: 46.836098
IoU in current test batch is 0.8743110468178633
Epoch 12725: reducing learning rate of group 0 to 8.9221e-04.
training: bce: 0.037339, dice: 0.087663, loss: 0.062501
training IoU in current batch 12800 is 0.915917648487793
training IoU uptillnow 12800 is 0.002931362082234678
testing: bce: 28.116607, dice: 66.010613, loss: 47.063610
IoU in current test batch is 0.9153492944616046
Epoch 12826: reducing learning rate of group 0 to 8.9131e-04.
training: bce: 0.037160, dice: 0.087423, loss: 0.062292
training IoU in current batch 12900 is 0.9186787778695293
training IoU uptillnow 12900 is 0.0029323767362201348
testing: bce: 28.200313, dice: 66.343788, loss: 47.272051
IoU in current test batch is 0.9010554154879964
Epoch 12927: reducing learning rate of group 0 to 8.9042e-04.
training: bce: 0.037046, dice: 0.087215, loss: 0.062131
training IoU in current batch 13000 is 0.8052177167143354
training IoU uptillnow 13000 is 0.002930466747138482
testing: bce: 28.331660, dice: 66.699336, loss: 47.515498
IoU in current test batch is 0.8956269053751241
Epoch 13028: reducing learning rate of group 0 to 8.8953e-04.
training: bce: 0.036873, dice: 0.086967, loss: 0.061920
training IoU in current batch 13100 is 0.9054950718288991
training IoU uptillnow 13100 is 0.0029311373078510826
testing: bce: 28.415738, dice: 67.021228, loss: 47.718483
IoU in current test batch is 0.9059082044471957
Epoch 13129: reducing learning rate of group 0 to 8.8864e-04.
training: bce: 0.036699, dice: 0.086729, loss: 0.061714
training IoU in current batch 13200 is 0.92767925528781
training IoU uptillnow 13200 is 0.002932357873538846
testing: bce: 28.497690, dice: 67.347895, loss: 47.922792
IoU in current test batch is 0.9178196321059131
Epoch 13230: reducing learning rate of group 0 to 8.8775e-04.
training: bce: 0.036548, dice: 0.086580, loss: 0.061564
training IoU in current batch 13300 is 0.8574644395145504
training IoU uptillnow 13300 is 0.0029318004487199826
testing: bce: 28.595566, dice: 67.741095, loss: 48.168331
IoU in current test batch is 0.8824822986208831
Epoch 13331: reducing learning rate of group 0 to 8.8687e-04.
training: bce: 0.036400, dice: 0.086380, loss: 0.061390
training IoU in current batch 13400 is 0.8798325967982674
training IoU uptillnow 13400 is 0.0029318077233557627
testing: bce: 28.693936, dice: 68.092527, loss: 48.393231
IoU in current test batch is 0.8999187758337449
Epoch 13432: reducing learning rate of group 0 to 8.8598e-04.
training: bce: 0.036318, dice: 0.086281, loss: 0.061299
training IoU in current batch 13500 is 0.9287124473728028
training IoU uptillnow 13500 is 0.002933021710723762
testing: bce: 28.842547, dice: 68.522221, loss: 48.682384
IoU in current test batch is 0.9022782753142987
Epoch 13533: reducing learning rate of group 0 to 8.8509e-04.
training: bce: 0.036205, dice: 0.086108, loss: 0.061156
training IoU in current batch 13600 is 0.886915594603075
training IoU uptillnow 13600 is 0.002933193489303424
testing: bce: 28.966000, dice: 68.891231, loss: 48.928615
IoU in current test batch is 0.8732563945567807
Epoch 13634: reducing learning rate of group 0 to 8.8421e-04.
training: bce: 0.036072, dice: 0.085908, loss: 0.060990
training IoU in current batch 13700 is 0.944147990039132
training IoU uptillnow 13700 is 0.002934755174417603
testing: bce: 29.072290, dice: 69.237065, loss: 49.154678
IoU in current test batch is 0.895463471196245
Epoch 13735: reducing learning rate of group 0 to 8.8333e-04.
training: bce: 0.035947, dice: 0.085731, loss: 0.060839
training IoU in current batch 13800 is 0.9320631302494983
training IoU uptillnow 13800 is 0.002936002344379302
testing: bce: 29.182245, dice: 69.598068, loss: 49.390156
IoU in current test batch is 0.8384163478546472
Epoch 13836: reducing learning rate of group 0 to 8.8244e-04.
training: bce: 0.035825, dice: 0.085524, loss: 0.060675
training IoU in current batch 13900 is 0.9128341309676666
training IoU uptillnow 13900 is 0.0029367704768314966
testing: bce: 29.294233, dice: 69.933601, loss: 49.613917
IoU in current test batch is 0.874705760054783
Epoch 13937: reducing learning rate of group 0 to 8.8156e-04.
training: bce: 0.035746, dice: 0.085361, loss: 0.060553
training IoU in current batch 14000 is 0.9236100782342267
training IoU uptillnow 14000 is 0.00293778418859458
testing: bce: 29.439637, dice: 70.301918, loss: 49.870777
IoU in current test batch is 0.8936407896995997
Epoch 14038: reducing learning rate of group 0 to 8.8068e-04.
training: bce: 0.035646, dice: 0.085222, loss: 0.060434
training IoU in current batch 14100 is 0.9238103133808655
training IoU uptillnow 14100 is 0.0029387882558428247
testing: bce: 29.567490, dice: 70.689427, loss: 50.128458
IoU in current test batch is 0.8743114906070831
Epoch 14139: reducing learning rate of group 0 to 8.7980e-04.
training: bce: 0.035532, dice: 0.085137, loss: 0.060334
training IoU in current batch 14200 is 0.7927291810786956
training IoU uptillnow 14200 is 0.002936701377086067
testing: bce: 29.681678, dice: 71.119183, loss: 50.400430
IoU in current test batch is 0.9107817014699108
Epoch 14240: reducing learning rate of group 0 to 8.7892e-04.
training: bce: 0.035402, dice: 0.084908, loss: 0.060155
training IoU in current batch 14300 is 0.9095552312670516
training IoU uptillnow 14300 is 0.0029373667109354766
testing: bce: 29.781081, dice: 71.427926, loss: 50.604503
IoU in current test batch is 0.8727958957565889
Epoch 14341: reducing learning rate of group 0 to 8.7804e-04.
training: bce: 0.035296, dice: 0.084708, loss: 0.060002
training IoU in current batch 14400 is 0.8919068736141907
training IoU uptillnow 14400 is 0.002937614306249079
testing: bce: 29.899856, dice: 71.757788, loss: 50.828822
IoU in current test batch is 0.9086107084841082
Epoch 14442: reducing learning rate of group 0 to 8.7716e-04.
training: bce: 0.035163, dice: 0.084544, loss: 0.059854
training IoU in current batch 14500 is 0.8871656356190643
training IoU uptillnow 14500 is 0.0029377495002298236
testing: bce: 29.994289, dice: 72.115816, loss: 51.055052
IoU in current test batch is 0.8898498722763618
Epoch 14543: reducing learning rate of group 0 to 8.7628e-04.
training: bce: 0.035031, dice: 0.084376, loss: 0.059704
training IoU in current batch 14600 is 0.9317021092420001
training IoU uptillnow 14600 is 0.002938899587191289
testing: bce: 30.087793, dice: 72.469114, loss: 51.278454
IoU in current test batch is 0.8947259688065253
Epoch 14644: reducing learning rate of group 0 to 8.7541e-04.
training: bce: 0.034900, dice: 0.084207, loss: 0.059554
training IoU in current batch 14700 is 0.8625256302014233
training IoU uptillnow 14700 is 0.002938465506834942
testing: bce: 30.179940, dice: 72.819634, loss: 51.499787
IoU in current test batch is 0.8309876729325788
Epoch 14745: reducing learning rate of group 0 to 8.7453e-04.
training: bce: 0.034775, dice: 0.084035, loss: 0.059405
training IoU in current batch 14800 is 0.9269416014449127
training IoU uptillnow 14800 is 0.0029394880041750866
testing: bce: 30.276870, dice: 73.165022, loss: 51.720946
IoU in current test batch is 0.8872405512911906
Epoch 14846: reducing learning rate of group 0 to 8.7366e-04.
training: bce: 0.034638, dice: 0.083830, loss: 0.059234
training IoU in current batch 14900 is 0.9279026217228464
training IoU uptillnow 14900 is 0.0029405182755320495
testing: bce: 30.360966, dice: 73.479473, loss: 51.920220
IoU in current test batch is 0.8858611714006176
Epoch 14947: reducing learning rate of group 0 to 8.7278e-04.
training: bce: 0.034513, dice: 0.083676, loss: 0.059094
training IoU in current batch 15000 is 0.890655185737153
training IoU uptillnow 15000 is 0.002940707145231348
testing: bce: 30.454417, dice: 73.836254, loss: 52.145336
IoU in current test batch is 0.9026082641856663
Epoch 15048: reducing learning rate of group 0 to 8.7191e-04.
training: bce: 0.034426, dice: 0.083519, loss: 0.058972
training IoU in current batch 15100 is 0.8918027239658235
training IoU uptillnow 15100 is 0.0029409188438031075
testing: bce: 30.580394, dice: 74.189342, loss: 52.384868
IoU in current test batch is 0.8712757905145342
Epoch 15149: reducing learning rate of group 0 to 8.7104e-04.
training: bce: 0.034334, dice: 0.083373, loss: 0.058853
training IoU in current batch 15200 is 0.9283326241565862
training IoU uptillnow 15200 is 0.0029419287986529124
testing: bce: 30.700406, dice: 74.550072, loss: 52.625239
IoU in current test batch is 0.8551530418214203
Epoch 15250: reducing learning rate of group 0 to 8.7017e-04.
training: bce: 0.034216, dice: 0.083232, loss: 0.058724
training IoU in current batch 15300 is 0.9026246639886436
training IoU uptillnow 15300 is 0.0029423655026677427
testing: bce: 30.796509, dice: 74.913310, loss: 52.854910
IoU in current test batch is 0.8783851098301722
Epoch 15351: reducing learning rate of group 0 to 8.6930e-04.
training: bce: 0.034145, dice: 0.083084, loss: 0.058614
training IoU in current batch 15400 is 0.9244179507367001
training IoU uptillnow 15400 is 0.0029432682211045624
testing: bce: 30.933236, dice: 75.268852, loss: 53.101044
IoU in current test batch is 0.8562732535150503
Epoch 15452: reducing learning rate of group 0 to 8.6843e-04.
training: bce: 0.034067, dice: 0.082970, loss: 0.058519
training IoU in current batch 15500 is 0.758450184501845
training IoU uptillnow 15500 is 0.002940590323295184
testing: bce: 31.063170, dice: 75.654277, loss: 53.358724
IoU in current test batch is 0.8889537231302728
Epoch 15553: reducing learning rate of group 0 to 8.6756e-04.
training: bce: 0.033963, dice: 0.082820, loss: 0.058391
training IoU in current batch 15600 is 0.895879370856623
training IoU uptillnow 15600 is 0.002940883088157865
testing: bce: 31.168091, dice: 76.004201, loss: 53.586146
IoU in current test batch is 0.8906224539485639
Epoch 15654: reducing learning rate of group 0 to 8.6669e-04.
training: bce: 0.033845, dice: 0.082659, loss: 0.058252
training IoU in current batch 15700 is 0.9516287146738369
training IoU uptillnow 15700 is 0.0029423556862562127
testing: bce: 31.259069, dice: 76.343214, loss: 53.801142
IoU in current test batch is 0.9044491190631284
Epoch 15755: reducing learning rate of group 0 to 8.6583e-04.
training: bce: 0.033719, dice: 0.082470, loss: 0.058094
training IoU in current batch 15800 is 0.9510510971305816
training IoU uptillnow 15800 is 0.0029437974597991048
testing: bce: 31.340457, dice: 76.653358, loss: 53.996907
IoU in current test batch is 0.9041526848976007
Epoch 15856: reducing learning rate of group 0 to 8.6496e-04.
training: bce: 0.033619, dice: 0.082313, loss: 0.057966
training IoU in current batch 15900 is 0.911705143782908
training IoU uptillnow 15900 is 0.0029443962880875392
testing: bce: 31.445382, dice: 76.991813, loss: 54.218598
IoU in current test batch is 0.8714627270594643
Epoch 15957: reducing learning rate of group 0 to 8.6409e-04.
training: bce: 0.033504, dice: 0.082137, loss: 0.057820
training IoU in current batch 16000 is 0.883133951137321
training IoU uptillnow 16000 is 0.002944392435509806
testing: bce: 31.534864, dice: 77.309852, loss: 54.422358
IoU in current test batch is 0.8686287283812055
Epoch 16058: reducing learning rate of group 0 to 8.6323e-04.
training: bce: 0.033420, dice: 0.082013, loss: 0.057716
training IoU in current batch 16100 is 0.9119807702695403
training IoU uptillnow 16100 is 0.0029449858363258335
testing: bce: 31.652205, dice: 77.675980, loss: 54.664093
IoU in current test batch is 0.8759031341257525
Epoch 16159: reducing learning rate of group 0 to 8.6237e-04.
training: bce: 0.033319, dice: 0.081864, loss: 0.057591
training IoU in current batch 16200 is 0.9348834529057539
training IoU uptillnow 16200 is 0.00294604313118434
testing: bce: 31.752929, dice: 78.016262, loss: 54.884595
IoU in current test batch is 0.915413343094691
Epoch 16260: reducing learning rate of group 0 to 8.6150e-04.
training: bce: 0.033193, dice: 0.081665, loss: 0.057429
training IoU in current batch 16300 is 0.9307353150334234
training IoU uptillnow 16300 is 0.0029470026301042043
testing: bce: 31.828316, dice: 78.307554, loss: 55.067935
IoU in current test batch is 0.8958962659426085
Epoch 16361: reducing learning rate of group 0 to 8.6064e-04.
training: bce: 0.033084, dice: 0.081503, loss: 0.057294
training IoU in current batch 16400 is 0.8954469946096213
training IoU uptillnow 16400 is 0.0029472332299777556
testing: bce: 31.918426, dice: 78.631448, loss: 55.274937
IoU in current test batch is 0.9053709702689905
Epoch 16462: reducing learning rate of group 0 to 8.5978e-04.
training: bce: 0.033001, dice: 0.081369, loss: 0.057185
training IoU in current batch 16500 is 0.8246435411789742
training IoU uptillnow 16500 is 0.002946030748758146
testing: bce: 32.031910, dice: 78.980607, loss: 55.506259
IoU in current test batch is 0.9015051554986385
Epoch 16563: reducing learning rate of group 0 to 8.5892e-04.
training: bce: 0.032887, dice: 0.081215, loss: 0.057051
training IoU in current batch 16600 is 0.9517214568603154
training IoU uptillnow 16600 is 0.0029473943660549527
testing: bce: 32.115185, dice: 79.309055, loss: 55.712120
IoU in current test batch is 0.9004399338016004
Epoch 16664: reducing learning rate of group 0 to 8.5806e-04.
training: bce: 0.032786, dice: 0.081069, loss: 0.056927
training IoU in current batch 16700 is 0.9251738587731811
training IoU uptillnow 16700 is 0.002948211793134303
testing: bce: 32.208983, dice: 79.643083, loss: 55.926033
IoU in current test batch is 0.9044033055025881
Epoch 16765: reducing learning rate of group 0 to 8.5721e-04.
training: bce: 0.032687, dice: 0.080935, loss: 0.056811
training IoU in current batch 16800 is 0.9223459771010203
training IoU uptillnow 16800 is 0.0029489633840943798
testing: bce: 32.304810, dice: 79.987209, loss: 56.146010
IoU in current test batch is 0.8657456860419463
Epoch 16866: reducing learning rate of group 0 to 8.5635e-04.
training: bce: 0.032576, dice: 0.080778, loss: 0.056677
training IoU in current batch 16900 is 0.826075367340819
training IoU uptillnow 16900 is 0.0029478073647683537
testing: bce: 32.386373, dice: 80.307407, loss: 56.346890
IoU in current test batch is 0.901345333217607
Epoch 16967: reducing learning rate of group 0 to 8.5549e-04.
training: bce: 0.032499, dice: 0.080670, loss: 0.056585
training IoU in current batch 17000 is 0.9533291548141363
training IoU uptillnow 17000 is 0.0029491599704069165
testing: bce: 32.501347, dice: 80.675219, loss: 56.588283
IoU in current test batch is 0.9027214400673431
Epoch 17068: reducing learning rate of group 0 to 8.5464e-04.
training: bce: 0.032546, dice: 0.080556, loss: 0.056551
training IoU in current batch 17100 is 0.9405754235009411
training IoU uptillnow 17100 is 0.002950248160812524
testing: bce: 32.739684, dice: 81.034958, loss: 56.887321
IoU in current test batch is 0.9199659517238228
Epoch 17169: reducing learning rate of group 0 to 8.5378e-04.
training: bce: 0.032452, dice: 0.080443, loss: 0.056447
training IoU in current batch 17200 is 0.9248489067493593
training IoU uptillnow 17200 is 0.002951018938839104
testing: bce: 32.835312, dice: 81.394137, loss: 57.114725
IoU in current test batch is 0.9091535403990085
Epoch 17270: reducing learning rate of group 0 to 8.5293e-04.
training: bce: 0.032401, dice: 0.080351, loss: 0.056376
training IoU in current batch 17300 is 0.9202100244813812
training IoU uptillnow 17300 is 0.002951691430657104
testing: bce: 32.974268, dice: 81.774067, loss: 57.374168
IoU in current test batch is 0.895842417911158
Epoch 17371: reducing learning rate of group 0 to 8.5208e-04.
training: bce: 0.032339, dice: 0.080256, loss: 0.056298
training IoU in current batch 17400 is 0.9217144881331917
training IoU uptillnow 17400 is 0.0029523850126147704
testing: bce: 33.102218, dice: 82.149565, loss: 57.625891
IoU in current test batch is 0.8832999881507634
Epoch 17472: reducing learning rate of group 0 to 8.5122e-04.
training: bce: 0.032244, dice: 0.080124, loss: 0.056184
training IoU in current batch 17500 is 0.9256426407510723
training IoU uptillnow 17500 is 0.0029531454860537484
testing: bce: 33.194200, dice: 82.485491, loss: 57.839846
IoU in current test batch is 0.8955900639024929
Epoch 17573: reducing learning rate of group 0 to 8.5037e-04.
training: bce: 0.032151, dice: 0.080006, loss: 0.056078
training IoU in current batch 17600 is 0.9355004691425285
training IoU uptillnow 17600 is 0.0029540840089279487
testing: bce: 33.287456, dice: 82.834136, loss: 58.060796
IoU in current test batch is 0.9099481971176419
Epoch 17674: reducing learning rate of group 0 to 8.4952e-04.
training: bce: 0.032082, dice: 0.079899, loss: 0.055990
training IoU in current batch 17700 is 0.9521663140465458
training IoU uptillnow 17700 is 0.002955325767611416
testing: bce: 33.404690, dice: 83.193202, loss: 58.298946
IoU in current test batch is 0.9009404992177723
Epoch 17775: reducing learning rate of group 0 to 8.4867e-04.
training: bce: 0.032000, dice: 0.079816, loss: 0.055908
training IoU in current batch 17800 is 0.832320332305061
training IoU uptillnow 17800 is 0.002954309394037303
testing: bce: 33.508085, dice: 83.576543, loss: 58.542314
IoU in current test batch is 0.8615462411162137
Epoch 17876: reducing learning rate of group 0 to 8.4782e-04.
training: bce: 0.031901, dice: 0.079669, loss: 0.055785
training IoU in current batch 17900 is 0.9529603083780039
training IoU uptillnow 17900 is 0.002955550804948552
testing: bce: 33.591318, dice: 83.891937, loss: 58.741627
IoU in current test batch is 0.883239102433501
Epoch 17978: reducing learning rate of group 0 to 8.4698e-04.
training: bce: 0.031984, dice: 0.079569, loss: 0.055776
training IoU in current batch 18000 is 0.9380113808545743
training IoU uptillnow 18000 is 0.002956501606559018
testing: bce: 33.867146, dice: 84.254104, loss: 59.060625
IoU in current test batch is 0.8906633775552791
Epoch 18079: reducing learning rate of group 0 to 8.4613e-04.
training: bce: 0.031897, dice: 0.079466, loss: 0.055682
training IoU in current batch 18100 is 0.9322131570167723
training IoU uptillnow 18100 is 0.002957335127267802
testing: bce: 33.962729, dice: 84.612756, loss: 59.287742
IoU in current test batch is 0.8790257975435533
Epoch 18180: reducing learning rate of group 0 to 8.4528e-04.
training: bce: 0.031848, dice: 0.079405, loss: 0.055627
training IoU in current batch 18200 is 0.9242671009771987
training IoU uptillnow 18200 is 0.0029580139647455385
testing: bce: 34.097869, dice: 85.015089, loss: 59.556479
IoU in current test batch is 0.8748210248764969
Epoch 18281: reducing learning rate of group 0 to 8.4444e-04.
training: bce: 0.031759, dice: 0.079276, loss: 0.055517
training IoU in current batch 18300 is 0.8613358530679738
training IoU uptillnow 18300 is 0.0029575391576064807
testing: bce: 34.189229, dice: 85.342579, loss: 59.765904
IoU in current test batch is 0.8518290596337239
Epoch 18382: reducing learning rate of group 0 to 8.4359e-04.
training: bce: 0.031668, dice: 0.079145, loss: 0.055407
training IoU in current batch 18400 is 0.9318220172132531
training IoU uptillnow 18400 is 0.002958346365003385
testing: bce: 34.278041, dice: 85.667423, loss: 59.972732
IoU in current test batch is 0.8977991993846085
Epoch 18483: reducing learning rate of group 0 to 8.4275e-04.
training: bce: 0.031577, dice: 0.079022, loss: 0.055300
training IoU in current batch 18500 is 0.9190450828660046
training IoU uptillnow 18500 is 0.0029589146437156164
testing: bce: 34.365072, dice: 85.999700, loss: 60.182386
IoU in current test batch is 0.8790634743039941
Epoch 18584: reducing learning rate of group 0 to 8.4191e-04.
training: bce: 0.031488, dice: 0.078883, loss: 0.055186
training IoU in current batch 18600 is 0.903395801877017
training IoU uptillnow 18600 is 0.0029591963742455585
testing: bce: 34.453529, dice: 86.312148, loss: 60.382839
IoU in current test batch is 0.9134702381206989
Epoch 18685: reducing learning rate of group 0 to 8.4106e-04.
training: bce: 0.031407, dice: 0.078796, loss: 0.055102
training IoU in current batch 18700 is 0.9149029083816123
training IoU uptillnow 18700 is 0.0029596801985706024
testing: bce: 34.549699, dice: 86.680396, loss: 60.615047
IoU in current test batch is 0.9019260026672403
Epoch 18786: reducing learning rate of group 0 to 8.4022e-04.
training: bce: 0.031316, dice: 0.078654, loss: 0.054985
training IoU in current batch 18800 is 0.9559021515482511
training IoU uptillnow 18800 is 0.002960885774372901
testing: bce: 34.633920, dice: 86.987008, loss: 60.810464
IoU in current test batch is 0.9009738869849611
Epoch 18887: reducing learning rate of group 0 to 8.3938e-04.
training: bce: 0.031263, dice: 0.078568, loss: 0.054916
training IoU in current batch 18900 is 0.8918758594655809
training IoU uptillnow 18900 is 0.002960949441677166
testing: bce: 34.758874, dice: 87.353911, loss: 61.056392
IoU in current test batch is 0.8464990133649681
Epoch 18988: reducing learning rate of group 0 to 8.3854e-04.
training: bce: 0.031177, dice: 0.078435, loss: 0.054806
training IoU in current batch 19000 is 0.8042792716421959
training IoU uptillnow 19000 is 0.0029594757374710547
testing: bce: 34.846947, dice: 87.666823, loss: 61.256885
IoU in current test batch is 0.8909327254171218
Epoch 19089: reducing learning rate of group 0 to 8.3771e-04.
training: bce: 0.031105, dice: 0.078317, loss: 0.054711
training IoU in current batch 19100 is 0.9312422521405714
training IoU uptillnow 19100 is 0.0029602331067344312
testing: bce: 34.949661, dice: 87.996027, loss: 61.472844
IoU in current test batch is 0.9066517671796732
Epoch 19190: reducing learning rate of group 0 to 8.3687e-04.
training: bce: 0.031021, dice: 0.078214, loss: 0.054618
training IoU in current batch 19200 is 0.9357754769538025
training IoU uptillnow 19200 is 0.0029610612848663597
testing: bce: 35.037326, dice: 88.340863, loss: 61.689094
IoU in current test batch is 0.9209982825539531
Epoch 19291: reducing learning rate of group 0 to 8.3603e-04.
training: bce: 0.030928, dice: 0.078073, loss: 0.054501
training IoU in current batch 19300 is 0.8582632254384714
training IoU uptillnow 19300 is 0.0029605422243682606
testing: bce: 35.113936, dice: 88.640911, loss: 61.877424
IoU in current test batch is 0.8563738583593706
training: bce: 0.030855, dice: 0.077967, loss: 0.054411
training IoU in current batch 19400 is 0.9198749632637794
training IoU uptillnow 19400 is 0.002961087081093056
testing: bce: 35.212493, dice: 88.979256, loss: 62.095875
IoU in current test batch is 0.9074064416816213
Epoch 19402: reducing learning rate of group 0 to 8.3519e-04.
training: bce: 0.030791, dice: 0.077863, loss: 0.054327
training IoU in current batch 19500 is 0.9090267077555213
training IoU uptillnow 19500 is 0.0029614409190744863
testing: bce: 35.320804, dice: 89.317961, loss: 62.319383
IoU in current test batch is 0.9152548309639085
Epoch 19503: reducing learning rate of group 0 to 8.3436e-04.
training: bce: 0.030727, dice: 0.077766, loss: 0.054247
training IoU in current batch 19600 is 0.9098231384476738
training IoU uptillnow 19600 is 0.0029618046906971134
testing: bce: 35.428465, dice: 89.664723, loss: 62.546594
IoU in current test batch is 0.8766905472995541
Epoch 19604: reducing learning rate of group 0 to 8.3353e-04.
training: bce: 0.030664, dice: 0.077646, loss: 0.054155
training IoU in current batch 19700 is 0.9322564301254731
training IoU uptillnow 19700 is 0.0029625443320506206
testing: bce: 35.536502, dice: 89.983051, loss: 62.759776
IoU in current test batch is 0.8979756159060837
Epoch 19705: reducing learning rate of group 0 to 8.3269e-04.
training: bce: 0.030579, dice: 0.077507, loss: 0.054043
training IoU in current batch 19800 is 0.9104926534140018
training IoU uptillnow 19800 is 0.002962910127613114
testing: bce: 35.617424, dice: 90.277324, loss: 62.947374
IoU in current test batch is 0.911866829789699
Epoch 19806: reducing learning rate of group 0 to 8.3186e-04.
training: bce: 0.030488, dice: 0.077364, loss: 0.053926
training IoU in current batch 19900 is 0.9491105666808692
training IoU uptillnow 19900 is 0.0029639190807376967
testing: bce: 35.690931, dice: 90.565400, loss: 63.128166
IoU in current test batch is 0.8898711069567127
Epoch 19907: reducing learning rate of group 0 to 8.3103e-04.
training: bce: 0.030446, dice: 0.077294, loss: 0.053870
training IoU in current batch 20000 is 0.9457419866465426
training IoU uptillnow 20000 is 0.0029648618046419217
testing: bce: 35.821191, dice: 90.938280, loss: 63.379736
IoU in current test batch is 0.893103201895197
Epoch 20008: reducing learning rate of group 0 to 8.3020e-04.
training: bce: 0.030383, dice: 0.077194, loss: 0.053788
training IoU in current batch 20100 is 0.9603014947416669
training IoU uptillnow 20100 is 0.0029660365878757425
testing: bce: 35.924777, dice: 91.275460, loss: 63.600118
IoU in current test batch is 0.8324767910218395
Epoch 20109: reducing learning rate of group 0 to 8.2937e-04.
training: bce: 0.030365, dice: 0.077134, loss: 0.053749
training IoU in current batch 20200 is 0.9333104159504985
training IoU uptillnow 20200 is 0.0029667543648766795
testing: bce: 36.082152, dice: 91.657387, loss: 63.869769
IoU in current test batch is 0.89224928720221
Epoch 20210: reducing learning rate of group 0 to 8.2854e-04.
training: bce: 0.030391, dice: 0.077063, loss: 0.053727
training IoU in current batch 20300 is 0.897808629372533
training IoU uptillnow 20300 is 0.0029668821470862504
testing: bce: 36.292813, dice: 92.026746, loss: 64.159779
IoU in current test batch is 0.8598220195364334
Epoch 20311: reducing learning rate of group 0 to 8.2771e-04.
training: bce: 0.030365, dice: 0.077017, loss: 0.053691
training IoU in current batch 20400 is 0.8980273830380897
training IoU uptillnow 20400 is 0.002967012250821561
testing: bce: 36.440099, dice: 92.424980, loss: 64.432540
IoU in current test batch is 0.8869267926135621
Epoch 20412: reducing learning rate of group 0 to 8.2688e-04.
training: bce: 0.030294, dice: 0.076934, loss: 0.053614
training IoU in current batch 20500 is 0.9171632371503214
training IoU uptillnow 20500 is 0.0029674522222360262
testing: bce: 36.532576, dice: 92.777634, loss: 64.655105
IoU in current test batch is 0.8680821893981749
Epoch 20513: reducing learning rate of group 0 to 8.2605e-04.
training: bce: 0.030241, dice: 0.076861, loss: 0.053551
training IoU in current batch 20600 is 0.9104461069548352
training IoU uptillnow 20600 is 0.0029677792361396237
testing: bce: 36.646209, dice: 93.142029, loss: 64.894119
IoU in current test batch is 0.903718750642358
Epoch 20614: reducing learning rate of group 0 to 8.2523e-04.
training: bce: 0.030170, dice: 0.076775, loss: 0.053472
training IoU in current batch 20700 is 0.9360199869650229
training IoU uptillnow 20700 is 0.00296851488846114
testing: bce: 36.738237, dice: 93.488902, loss: 65.113569
IoU in current test batch is 0.8885018153419071
Epoch 20715: reducing learning rate of group 0 to 8.2440e-04.
training: bce: 0.030093, dice: 0.076660, loss: 0.053377
training IoU in current batch 20800 is 0.8956415602909017
training IoU uptillnow 20800 is 0.0029685964084802505
testing: bce: 36.821151, dice: 93.800768, loss: 65.310959
IoU in current test batch is 0.9060491637348167
Epoch 20816: reducing learning rate of group 0 to 8.2358e-04.
training: bce: 0.030037, dice: 0.076569, loss: 0.053303
training IoU in current batch 20900 is 0.9105093316477594
training IoU uptillnow 20900 is 0.002968914262635613
testing: bce: 36.929362, dice: 94.138908, loss: 65.534135
IoU in current test batch is 0.8839787053703377
Epoch 20917: reducing learning rate of group 0 to 8.2275e-04.
training: bce: 0.029988, dice: 0.076506, loss: 0.053247
training IoU in current batch 21000 is 0.9124970145689038
training IoU uptillnow 21000 is 0.0029692606387729118
testing: bce: 37.045984, dice: 94.512089, loss: 65.779036
IoU in current test batch is 0.8962207701921402
Epoch 21018: reducing learning rate of group 0 to 8.2193e-04.
training: bce: 0.029928, dice: 0.076414, loss: 0.053171
training IoU in current batch 21100 is 0.9019045365824806
training IoU uptillnow 21100 is 0.002969436402085087
testing: bce: 37.147428, dice: 94.848100, loss: 65.997764
IoU in current test batch is 0.885951130343116
Epoch 21119: reducing learning rate of group 0 to 8.2111e-04.
training: bce: 0.029863, dice: 0.076338, loss: 0.053101
training IoU in current batch 21200 is 0.9520113657291694
training IoU uptillnow 21200 is 0.002970398313395931
testing: bce: 37.242885, dice: 95.202390, loss: 66.222638
IoU in current test batch is 0.8858134368018584
Epoch 21220: reducing learning rate of group 0 to 8.2029e-04.
training: bce: 0.029838, dice: 0.076291, loss: 0.053064
training IoU in current batch 21300 is 0.948813427444006
training IoU uptillnow 21300 is 0.00297130114946662
testing: bce: 37.387183, dice: 95.592205, loss: 66.489694
IoU in current test batch is 0.9152445597894703
Epoch 21321: reducing learning rate of group 0 to 8.1947e-04.
training: bce: 0.029781, dice: 0.076198, loss: 0.052990
training IoU in current batch 21400 is 0.873154636369821
training IoU uptillnow 21400 is 0.0029710171174358716
testing: bce: 37.491328, dice: 95.924159, loss: 66.707743
IoU in current test batch is 0.9004355558209638
Epoch 21422: reducing learning rate of group 0 to 8.1865e-04.
training: bce: 0.029715, dice: 0.076134, loss: 0.052924
training IoU in current batch 21500 is 0.933722452542003
training IoU uptillnow 21500 is 0.002971674719676252
testing: bce: 37.581972, dice: 96.291703, loss: 66.936838
IoU in current test batch is 0.9049769188471628
Epoch 21523: reducing learning rate of group 0 to 8.1783e-04.
training: bce: 0.029649, dice: 0.076032, loss: 0.052841
training IoU in current batch 21600 is 0.9399674006133105
training IoU uptillnow 21600 is 0.0029724226014828108
testing: bce: 37.673875, dice: 96.609937, loss: 67.141906
IoU in current test batch is 0.9059512469641477
Epoch 21624: reducing learning rate of group 0 to 8.1701e-04.
training: bce: 0.500329, dice: 0.932159, loss: 0.716244
training IoU in current batch 0 is 0.0
training IoU uptillnow 0 is 0.0
testing: bce: 0.029431, dice: 0.054833, loss: 0.042132
IoU in current test batch is 0.0
Epoch 21725: reducing learning rate of group 0 to 8.1620e-04.
training: bce: 0.057713, dice: 0.178631, loss: 0.118172
training IoU in current batch 100 is 0.9363850391980777
training IoU uptillnow 100 is 1.431781405501648e-05
testing: bce: 0.342884, dice: 1.061275, loss: 0.702080
IoU in current test batch is 0.8889417475455167
Epoch 21826: reducing learning rate of group 0 to 8.1538e-04.
training: bce: 0.040107, dice: 0.126990, loss: 0.083548
training IoU in current batch 200 is 0.9322951100663581
training IoU uptillnow 200 is 2.84426202323354e-05
testing: bce: 0.474201, dice: 1.501465, loss: 0.987833
IoU in current test batch is 0.8880542593717721
Epoch 21927: reducing learning rate of group 0 to 8.1456e-04.
training: bce: 0.034022, dice: 0.109171, loss: 0.071597
training IoU in current batch 300 is 0.9023296794423281
training IoU uptillnow 300 is 4.198499740464794e-05
testing: bce: 0.602394, dice: 1.932967, loss: 1.267681
IoU in current test batch is 0.8588083234382307
Epoch 22028: reducing learning rate of group 0 to 8.1375e-04.
training: bce: 0.030936, dice: 0.100156, loss: 0.065546
training IoU in current batch 400 is 0.8869176729580109
training IoU uptillnow 400 is 5.517236050776433e-05
testing: bce: 0.729728, dice: 2.362506, loss: 1.546117
IoU in current test batch is 0.883217528434705
Epoch 22129: reducing learning rate of group 0 to 8.1294e-04.
training: bce: 0.028516, dice: 0.093298, loss: 0.060907
training IoU in current batch 500 is 0.9288132546364263
training IoU uptillnow 500 is 6.886998132584386e-05
testing: bce: 0.840373, dice: 2.749548, loss: 1.794961
IoU in current test batch is 0.8622891288240442
Epoch 22230: reducing learning rate of group 0 to 8.1212e-04.
training: bce: 0.026280, dice: 0.088047, loss: 0.057164
training IoU in current batch 600 is 0.9167628390075014
training IoU uptillnow 600 is 8.226462773256655e-05
testing: bce: 0.929065, dice: 3.112736, loss: 2.020901
IoU in current test batch is 0.8815111008446268
Epoch 22331: reducing learning rate of group 0 to 8.1131e-04.
training: bce: 0.026643, dice: 0.085904, loss: 0.056273
training IoU in current batch 700 is 0.934435741224253
training IoU uptillnow 700 is 9.580266869840707e-05
testing: bce: 1.098622, dice: 3.542284, loss: 2.320453
IoU in current test batch is 0.8953638093720314
Epoch 22432: reducing learning rate of group 0 to 8.1050e-04.
training: bce: 0.025278, dice: 0.083044, loss: 0.054161
training IoU in current batch 800 is 0.910653685674548
training IoU uptillnow 800 is 0.0001088680447734445
testing: bce: 1.191049, dice: 3.912841, loss: 2.551945
IoU in current test batch is 0.8662712709218282
Epoch 22533: reducing learning rate of group 0 to 8.0969e-04.
training: bce: 0.024320, dice: 0.081027, loss: 0.052674
training IoU in current batch 900 is 0.9448364888123925
training IoU uptillnow 900 is 0.00012232196918908402
testing: bce: 1.288978, dice: 4.294421, loss: 2.791699
IoU in current test batch is 0.8963308334175883
Epoch 22634: reducing learning rate of group 0 to 8.0888e-04.
training: bce: 0.023359, dice: 0.078878, loss: 0.051119
training IoU in current batch 1000 is 0.9043194499800717
training IoU uptillnow 1000 is 0.00013506239296622567
testing: bce: 1.375417, dice: 4.644544, loss: 3.009981
IoU in current test batch is 0.9120577188121312
Epoch 22735: reducing learning rate of group 0 to 8.0807e-04.
training: bce: 0.022549, dice: 0.077219, loss: 0.049884
training IoU in current batch 1100 is 0.9035730039700044
training IoU uptillnow 1100 is 0.0001476801456866955
testing: bce: 1.460401, dice: 5.001056, loss: 3.230728
IoU in current test batch is 0.9008796871399705
Epoch 22836: reducing learning rate of group 0 to 8.0726e-04.
training: bce: 0.022543, dice: 0.076672, loss: 0.049607
training IoU in current batch 1200 is 0.8965432837444587
training IoU uptillnow 1200 is 0.0001600853747993367
testing: bce: 1.592619, dice: 5.416628, loss: 3.504623
IoU in current test batch is 0.8940291802856328
Epoch 22937: reducing learning rate of group 0 to 8.0645e-04.
training: bce: 0.022656, dice: 0.076410, loss: 0.049533
training IoU in current batch 1300 is 0.9348860985973992
training IoU uptillnow 1300 is 0.00017293842532335987
testing: bce: 1.733865, dice: 5.847635, loss: 3.790750
IoU in current test batch is 0.9039783508992569
Epoch 23038: reducing learning rate of group 0 to 8.0565e-04.
training: bce: 0.022412, dice: 0.075514, loss: 0.048963
training IoU in current batch 1400 is 0.9294629898403484
training IoU uptillnow 1400 is 0.00018560193848704443
testing: bce: 1.847023, dice: 6.223259, loss: 4.035141
IoU in current test batch is 0.8698962797199841
Epoch 23139: reducing learning rate of group 0 to 8.0484e-04.
training: bce: 0.022265, dice: 0.074863, loss: 0.048564
training IoU in current batch 1500 is 0.9204635893354556
training IoU uptillnow 1500 is 0.0001980269817024085
testing: bce: 1.965898, dice: 6.609990, loss: 4.287944
IoU in current test batch is 0.8884429734391759
Epoch 23240: reducing learning rate of group 0 to 8.0404e-04.
training: bce: 0.022314, dice: 0.074341, loss: 0.048327
training IoU in current batch 1600 is 0.9102171177855921
training IoU uptillnow 1600 is 0.00021019878461048963
testing: bce: 2.101447, dice: 7.001148, loss: 4.551297
IoU in current test batch is 0.85913976221323
Epoch 23341: reducing learning rate of group 0 to 8.0323e-04.
training: bce: 0.021781, dice: 0.073090, loss: 0.047435
training IoU in current batch 1700 is 0.7567890896040196
training IoU uptillnow 1700 is 0.00022008097056805192
testing: bce: 2.179368, dice: 7.313312, loss: 4.746340
IoU in current test batch is 0.8547434084978056
Epoch 23442: reducing learning rate of group 0 to 8.0243e-04.
training: bce: 0.022174, dice: 0.072697, loss: 0.047435
training IoU in current batch 1800 is 0.9033031088082901
training IoU uptillnow 1800 is 0.00023195726585369556
testing: bce: 2.349125, dice: 7.701604, loss: 5.025365
IoU in current test batch is 0.9048511903849374
Epoch 23543: reducing learning rate of group 0 to 8.0163e-04.
training: bce: 0.021801, dice: 0.071982, loss: 0.046891
training IoU in current batch 1900 is 0.9252341284213794
training IoU uptillnow 1900 is 0.00024404267473314854
testing: bce: 2.437852, dice: 8.049253, loss: 5.243553
IoU in current test batch is 0.8874709809771587
Epoch 23644: reducing learning rate of group 0 to 8.0083e-04.
training: bce: 0.021776, dice: 0.071878, loss: 0.046827
training IoU in current batch 2000 is 0.8848914009907278
training IoU uptillnow 2000 is 0.00025545868877774466
testing: bce: 2.563189, dice: 8.460430, loss: 5.511810
IoU in current test batch is 0.9037118731758403
Epoch 23745: reducing learning rate of group 0 to 8.0003e-04.
training: bce: 0.021423, dice: 0.071112, loss: 0.046268
training IoU in current batch 2100 is 0.9370398640829292
training IoU uptillnow 2100 is 0.00026750914056275314
testing: bce: 2.647691, dice: 8.788546, loss: 5.718119
IoU in current test batch is 0.901840832606472
Epoch 23846: reducing learning rate of group 0 to 7.9923e-04.
training: bce: 0.021103, dice: 0.070484, loss: 0.045794
training IoU in current batch 2200 is 0.9293453903139912
training IoU uptillnow 2200 is 0.0002793514369106634
testing: bce: 2.732236, dice: 9.125601, loss: 5.928918
IoU in current test batch is 0.9059977656083812
Epoch 23947: reducing learning rate of group 0 to 7.9843e-04.
training: bce: 0.020948, dice: 0.070103, loss: 0.045525
training IoU in current batch 2300 is 0.9272498927893157
training IoU uptillnow 2300 is 0.00029106594332338725
testing: bce: 2.835388, dice: 9.488580, loss: 6.161984
IoU in current test batch is 0.8861509910196858
Epoch 24048: reducing learning rate of group 0 to 7.9763e-04.
training: bce: 0.020634, dice: 0.069519, loss: 0.045076
training IoU in current batch 2400 is 0.8966012766099656
training IoU uptillnow 2400 is 0.0003022593249777849
testing: bce: 2.914208, dice: 9.818537, loss: 6.366373
IoU in current test batch is 0.8936683911188062
Epoch 24149: reducing learning rate of group 0 to 7.9683e-04.
training: bce: 0.020910, dice: 0.069117, loss: 0.045013
training IoU in current batch 2500 is 0.9089367974940115
training IoU uptillnow 2500 is 0.00031353011010176117
testing: bce: 3.076198, dice: 10.168302, loss: 6.622250
IoU in current test batch is 0.8478006319475115
Epoch 24250: reducing learning rate of group 0 to 7.9603e-04.
training: bce: 0.020736, dice: 0.068949, loss: 0.044843
training IoU in current batch 2600 is 0.9156379804598374
training IoU uptillnow 2600 is 0.00032480005451094234
testing: bce: 3.172644, dice: 10.549199, loss: 6.860921
IoU in current test batch is 0.8949216801590164
Epoch 24351: reducing learning rate of group 0 to 7.9524e-04.
training: bce: 0.020565, dice: 0.068570, loss: 0.044567
training IoU in current batch 2700 is 0.8418532130526785
training IoU uptillnow 2700 is 0.00033496963370082484
testing: bce: 3.267340, dice: 10.894558, loss: 7.080949
IoU in current test batch is 0.8438295790249525
Epoch 24452: reducing learning rate of group 0 to 7.9444e-04.
training: bce: 0.020424, dice: 0.068358, loss: 0.044391
training IoU in current batch 2800 is 0.9187618078767621
training IoU uptillnow 2800 is 0.0003461025713575121
testing: bce: 3.365075, dice: 11.263029, loss: 7.314052
IoU in current test batch is 0.8605677642257258
Epoch 24553: reducing learning rate of group 0 to 7.9365e-04.
training: bce: 0.020325, dice: 0.067985, loss: 0.044155
training IoU in current batch 2900 is 0.9336758095587561
training IoU uptillnow 2900 is 0.0003573470840695921
testing: bce: 3.468415, dice: 11.601385, loss: 7.534900
IoU in current test batch is 0.9021859548310494
Epoch 24654: reducing learning rate of group 0 to 7.9285e-04.
training: bce: 0.020370, dice: 0.068233, loss: 0.044301
training IoU in current batch 3000 is 0.8508113824900023
training IoU uptillnow 3000 is 0.0003673822697277449
testing: bce: 3.595902, dice: 12.045120, loss: 7.820511
IoU in current test batch is 0.8829931221556987
Epoch 24755: reducing learning rate of group 0 to 7.9206e-04.
training: bce: 0.020406, dice: 0.068062, loss: 0.044234
training IoU in current batch 3100 is 0.9128788798019754
training IoU uptillnow 3100 is 0.0003781707670245682
testing: bce: 3.722324, dice: 12.415297, loss: 8.068811
IoU in current test batch is 0.9017429696952947
Epoch 24856: reducing learning rate of group 0 to 7.9127e-04.
training: bce: 0.021273, dice: 0.068377, loss: 0.044825
training IoU in current batch 3200 is 0.9502050380785003
training IoU uptillnow 3200 is 0.00038937229055831826
testing: bce: 4.005586, dice: 12.874928, loss: 8.440257
IoU in current test batch is 0.9043567844432875
Epoch 24957: reducing learning rate of group 0 to 7.9048e-04.
training: bce: 0.021064, dice: 0.068004, loss: 0.044534
training IoU in current batch 3300 is 0.882984788022443
training IoU uptillnow 3300 is 0.0003995879319030509
testing: bce: 4.090082, dice: 13.204824, loss: 8.647453
IoU in current test batch is 0.9019532515050726
Epoch 25058: reducing learning rate of group 0 to 7.8969e-04.
training: bce: 0.021222, dice: 0.067761, loss: 0.044492
training IoU in current batch 3400 is 0.9422272172935747
training IoU uptillnow 3400 is 0.0004105089257639096
testing: bce: 4.245586, dice: 13.556254, loss: 8.900920
IoU in current test batch is 0.8987077696866477
Epoch 25159: reducing learning rate of group 0 to 7.8890e-04.
training: bce: 0.021095, dice: 0.067525, loss: 0.044310
training IoU in current batch 3500 is 0.8391727493917275
training IoU uptillnow 3500 is 0.000419980090732991
testing: bce: 4.344290, dice: 13.906225, loss: 9.125257
IoU in current test batch is 0.8789154498057391
Epoch 25260: reducing learning rate of group 0 to 7.8811e-04.
training: bce: 0.020912, dice: 0.067242, loss: 0.044077
training IoU in current batch 3600 is 0.9205962337728976
training IoU uptillnow 3600 is 0.00043044915801300417
testing: bce: 4.429567, dice: 14.243532, loss: 9.336549
IoU in current test batch is 0.893904914283179
Epoch 25361: reducing learning rate of group 0 to 7.8732e-04.
training: bce: 0.020756, dice: 0.066832, loss: 0.043794
training IoU in current batch 3700 is 0.9060367817684342
training IoU uptillnow 3700 is 0.00044064472276844423
testing: bce: 4.518774, dice: 14.549695, loss: 9.534234
IoU in current test batch is 0.8672452899769861
Epoch 25462: reducing learning rate of group 0 to 7.8653e-04.
training: bce: 0.020617, dice: 0.066583, loss: 0.043600
training IoU in current batch 3800 is 0.8968358251358358
training IoU uptillnow 3800 is 0.0004506400483672064
testing: bce: 4.609760, dice: 14.887090, loss: 9.748425
IoU in current test batch is 0.8685851348214918
Epoch 25563: reducing learning rate of group 0 to 7.8575e-04.
training: bce: 0.020533, dice: 0.066366, loss: 0.043450
training IoU in current batch 3900 is 0.938536877873276
training IoU uptillnow 3900 is 0.00046110026794224694
testing: bce: 4.711656, dice: 15.229153, loss: 9.970405
IoU in current test batch is 0.9133954625712928
Epoch 25664: reducing learning rate of group 0 to 7.8496e-04.
training: bce: 0.020350, dice: 0.066130, loss: 0.043240
training IoU in current batch 4000 is 0.8876286494433943
training IoU uptillnow 4000 is 0.0004708187967238386
testing: bce: 4.789410, dice: 15.563853, loss: 10.176631
IoU in current test batch is 0.9079124221910311
Epoch 25765: reducing learning rate of group 0 to 7.8418e-04.
training: bce: 0.020247, dice: 0.065896, loss: 0.043072
training IoU in current batch 4100 is 0.9283694115087566
training IoU uptillnow 4100 is 0.00048098835450796786
testing: bce: 4.884380, dice: 15.896420, loss: 10.390400
IoU in current test batch is 0.9064172007314649
Epoch 25866: reducing learning rate of group 0 to 7.8339e-04.
training: bce: 0.020083, dice: 0.065628, loss: 0.042856
training IoU in current batch 4200 is 0.9285882322189404
training IoU uptillnow 4200 is 0.000491082199113715
testing: bce: 4.962974, dice: 16.217950, loss: 10.590462
IoU in current test batch is 0.9048798723667527
Epoch 25967: reducing learning rate of group 0 to 7.8261e-04.
training: bce: 0.020473, dice: 0.065646, loss: 0.043060
training IoU in current batch 4300 is 0.9056933286762138
training IoU uptillnow 4300 is 0.0005008048743565624
testing: bce: 5.179623, dice: 16.608553, loss: 10.894088
IoU in current test batch is 0.8977548161967226
Epoch 26068: reducing learning rate of group 0 to 7.8183e-04.
training: bce: 0.020314, dice: 0.065349, loss: 0.042832
training IoU in current batch 4400 is 0.9135067732245084
training IoU uptillnow 4400 is 0.0005105528349046792
testing: bce: 5.258906, dice: 16.917793, loss: 11.088349
IoU in current test batch is 0.9148733919502721
Epoch 26169: reducing learning rate of group 0 to 7.8104e-04.
training: bce: 0.020392, dice: 0.065224, loss: 0.042808
training IoU in current batch 4500 is 0.9011821862348178
training IoU uptillnow 4500 is 0.0005200695821790229
testing: bce: 5.399002, dice: 17.269074, loss: 11.334038
IoU in current test batch is 0.8906568586789705
Epoch 26270: reducing learning rate of group 0 to 7.8026e-04.
training: bce: 0.020236, dice: 0.064925, loss: 0.042581
training IoU in current batch 4600 is 0.9340744295158883
training IoU uptillnow 4600 is 0.0005299308439643483
testing: bce: 5.476866, dice: 17.571728, loss: 11.524297
IoU in current test batch is 0.9013144103628395
Epoch 26371: reducing learning rate of group 0 to 7.7948e-04.
training: bce: 0.020114, dice: 0.064761, loss: 0.042438
training IoU in current batch 4700 is 0.8880836934752819
training IoU uptillnow 4700 is 0.0005391367081093734
testing: bce: 5.562233, dice: 17.908387, loss: 11.735310
IoU in current test batch is 0.9047941037753543
Epoch 26472: reducing learning rate of group 0 to 7.7870e-04.
training: bce: 0.020153, dice: 0.064805, loss: 0.042479
training IoU in current batch 4800 is 0.3815732454074423
training IoU uptillnow 4800 is 0.000541901893429809
testing: bce: 5.691407, dice: 18.301754, loss: 11.996580
IoU in current test batch is 0.8702372618978462
Epoch 26573: reducing learning rate of group 0 to 7.7792e-04.
training: bce: 0.020054, dice: 0.064623, loss: 0.042339
training IoU in current batch 4900 is 0.9400784180020457
training IoU uptillnow 4900 is 0.0005516450995698228
testing: bce: 5.781534, dice: 18.630409, loss: 12.205971
IoU in current test batch is 0.8909106639816998
Epoch 26674: reducing learning rate of group 0 to 7.7715e-04.
training: bce: 0.019916, dice: 0.064410, loss: 0.042163
training IoU in current batch 5000 is 0.9589556336481441
training IoU uptillnow 5000 is 0.0005615509934996255
testing: bce: 5.858774, dice: 18.947948, loss: 12.403361
IoU in current test batch is 0.9011214477044798
Epoch 26775: reducing learning rate of group 0 to 7.7637e-04.
training: bce: 0.019895, dice: 0.064313, loss: 0.042104
training IoU in current batch 5100 is 0.9336494079871563
training IoU uptillnow 5100 is 0.0005710682087973528
testing: bce: 5.969578, dice: 19.297811, loss: 12.633694
IoU in current test batch is 0.8961889430899656
Epoch 26876: reducing learning rate of group 0 to 7.7559e-04.
training: bce: 0.019852, dice: 0.064253, loss: 0.042053
training IoU in current batch 5200 is 0.9549956650815753
training IoU uptillnow 5200 is 0.0005807791778486832
testing: bce: 6.073594, dice: 19.657711, loss: 12.865653
IoU in current test batch is 0.8310093490968468
Epoch 26977: reducing learning rate of group 0 to 7.7482e-04.
training: bce: 0.019729, dice: 0.064002, loss: 0.041866
training IoU in current batch 5300 is 0.8963933940605792
training IoU uptillnow 5300 is 0.0005896947289685101
testing: bce: 6.152118, dice: 19.957319, loss: 13.054719
IoU in current test batch is 0.8936870742044002
Epoch 27078: reducing learning rate of group 0 to 7.7404e-04.
training: bce: 0.019620, dice: 0.063849, loss: 0.041734
training IoU in current batch 5400 is 0.8969348523861592
training IoU uptillnow 5400 is 0.0005985511426671031
testing: bce: 6.233366, dice: 20.285121, loss: 13.259243
IoU in current test batch is 0.8953320597386898
Epoch 27179: reducing learning rate of group 0 to 7.7327e-04.
training: bce: 0.019511, dice: 0.063702, loss: 0.041606
training IoU in current batch 5500 is 0.9177720207253885
training IoU uptillnow 5500 is 0.0006075977931318733
testing: bce: 6.313543, dice: 20.613156, loss: 13.463350
IoU in current test batch is 0.9035087443076311
Epoch 27280: reducing learning rate of group 0 to 7.7250e-04.
training: bce: 0.019527, dice: 0.063592, loss: 0.041560
training IoU in current batch 5600 is 0.9200557103064066
training IoU uptillnow 5600 is 0.0006166060516467311
testing: bce: 6.433527, dice: 20.951809, loss: 13.692668
IoU in current test batch is 0.8965179390389473
Epoch 27381: reducing learning rate of group 0 to 7.7172e-04.
training: bce: 0.019695, dice: 0.063705, loss: 0.041700
training IoU in current batch 5700 is 0.9000030247118962
training IoU uptillnow 5700 is 0.0006253046065033961
testing: bce: 6.604675, dice: 21.363689, loss: 13.984182
IoU in current test batch is 0.8939040883445865
Epoch 27482: reducing learning rate of group 0 to 7.7095e-04.
training: bce: 0.019745, dice: 0.063743, loss: 0.041744
training IoU in current batch 5800 is 0.9218215319821006
training IoU uptillnow 5800 is 0.0006342043658977123
testing: bce: 6.737654, dice: 21.751333, loss: 14.244493
IoU in current test batch is 0.8902756823108471
Epoch 27583: reducing learning rate of group 0 to 7.7018e-04.
training: bce: 0.019637, dice: 0.063517, loss: 0.041577
training IoU in current batch 5900 is 0.9244489600090311
training IoU uptillnow 5900 is 0.0006430713665044722
testing: bce: 6.816520, dice: 22.047800, loss: 14.432160
IoU in current test batch is 0.8664632346644237
Epoch 27684: reducing learning rate of group 0 to 7.6941e-04.
training: bce: 0.019545, dice: 0.063386, loss: 0.041466
training IoU in current batch 6000 is 0.8509787493335365
training IoU uptillnow 6000 is 0.0006509902273875311
testing: bce: 6.899493, dice: 22.375181, loss: 14.637337
IoU in current test batch is 0.9016835353922984
Epoch 27785: reducing learning rate of group 0 to 7.6864e-04.
training: bce: 0.019547, dice: 0.063335, loss: 0.041441
training IoU in current batch 6100 is 0.8754213077392226
training IoU uptillnow 6100 is 0.0006591451942882861
testing: bce: 7.014939, dice: 22.729811, loss: 14.872375
IoU in current test batch is 0.8800928109029255
Epoch 27886: reducing learning rate of group 0 to 7.6787e-04.
training: bce: 0.019513, dice: 0.063181, loss: 0.041347
training IoU in current batch 6200 is 0.9477963678602542
training IoU uptillnow 6200 is 0.0006681063987037433
testing: bce: 7.117799, dice: 23.046257, loss: 15.082028
IoU in current test batch is 0.8676711147450363
Epoch 27987: reducing learning rate of group 0 to 7.6710e-04.
training: bce: 0.019423, dice: 0.063042, loss: 0.041232
training IoU in current batch 6300 is 0.9405803255484784
training IoU uptillnow 6300 is 0.0006769176892506166
testing: bce: 7.198893, dice: 23.366149, loss: 15.282521
IoU in current test batch is 0.8728808087756355
Epoch 28088: reducing learning rate of group 0 to 7.6634e-04.
training: bce: 0.019402, dice: 0.062968, loss: 0.041185
training IoU in current batch 6400 is 0.9185865837976854
training IoU uptillnow 6400 is 0.0006854053675071113
testing: bce: 7.305481, dice: 23.709273, loss: 15.507377
IoU in current test batch is 0.890772379354834
Epoch 28189: reducing learning rate of group 0 to 7.6557e-04.
training: bce: 0.019347, dice: 0.062936, loss: 0.041141
training IoU in current batch 6500 is 0.9315565225274227
training IoU uptillnow 6500 is 0.0006939861584323511
testing: bce: 7.398365, dice: 24.067488, loss: 15.732927
IoU in current test batch is 0.9000253114899299
Epoch 28290: reducing learning rate of group 0 to 7.6481e-04.
training: bce: 0.019294, dice: 0.062737, loss: 0.041015
training IoU in current batch 6600 is 0.9215258855585831
training IoU uptillnow 6600 is 0.0007023881612359892
testing: bce: 7.491578, dice: 24.360437, loss: 15.926007
IoU in current test batch is 0.9162699133185914
Epoch 28391: reducing learning rate of group 0 to 7.6404e-04.
training: bce: 0.019208, dice: 0.062635, loss: 0.040921
training IoU in current batch 6700 is 0.8941646794781877
training IoU uptillnow 6700 is 0.0007104098540893623
testing: bce: 7.571236, dice: 24.689068, loss: 16.130152
IoU in current test batch is 0.8932158477384821
Epoch 28492: reducing learning rate of group 0 to 7.6328e-04.
training: bce: 0.019273, dice: 0.062739, loss: 0.041006
training IoU in current batch 6800 is 0.8883509750827384
training IoU uptillnow 6800 is 0.0007183072578186714
testing: bce: 7.710397, dice: 25.099326, loss: 16.404861
IoU in current test batch is 0.8726245611518887
Epoch 28593: reducing learning rate of group 0 to 7.6251e-04.
training: bce: 0.019278, dice: 0.062747, loss: 0.041013
training IoU in current batch 6900 is 0.9057172171407365
training IoU uptillnow 6900 is 0.0007263518387020646
testing: bce: 7.825751, dice: 25.471712, loss: 16.648731
IoU in current test batch is 0.8743939865560099
Epoch 28694: reducing learning rate of group 0 to 7.6175e-04.
training: bce: 0.019331, dice: 0.062818, loss: 0.041074
training IoU in current batch 7000 is 0.8959347265960492
training IoU uptillnow 7000 is 0.0007342267420119998
testing: bce: 7.960847, dice: 25.869796, loss: 16.915322
IoU in current test batch is 0.9000699402091858
Epoch 28795: reducing learning rate of group 0 to 7.6099e-04.
training: bce: 0.019282, dice: 0.062764, loss: 0.041023
training IoU in current batch 7100 is 0.9041026707465613
training IoU uptillnow 7100 is 0.0007421414948840248
testing: bce: 8.054245, dice: 26.216885, loss: 17.135565
IoU in current test batch is 0.8889752603571492
Epoch 28896: reducing learning rate of group 0 to 7.6023e-04.
training: bce: 0.019225, dice: 0.062690, loss: 0.040957
training IoU in current batch 7200 is 0.9498241557065087
training IoU uptillnow 7200 is 0.000750528827147477
testing: bce: 8.143379, dice: 26.554563, loss: 17.348971
IoU in current test batch is 0.8938512348659438
Epoch 28997: reducing learning rate of group 0 to 7.5947e-04.
training: bce: 0.019156, dice: 0.062662, loss: 0.040909
training IoU in current batch 7300 is 0.9403092700905377
training IoU uptillnow 7300 is 0.0007587489492388137
testing: bce: 8.226954, dice: 26.911454, loss: 17.569204
IoU in current test batch is 0.9003445930623425
Epoch 29098: reducing learning rate of group 0 to 7.5871e-04.
training: bce: 0.019148, dice: 0.062733, loss: 0.040941
training IoU in current batch 7400 is 0.9183088018445534
training IoU uptillnow 7400 is 0.00076666056570013
testing: bce: 8.336181, dice: 27.311066, loss: 17.823624
IoU in current test batch is 0.9085218135521897
Epoch 29199: reducing learning rate of group 0 to 7.5795e-04.
training: bce: 0.019057, dice: 0.062551, loss: 0.040804
training IoU in current batch 7500 is 0.9247758582987098
training IoU uptillnow 7500 is 0.0007745918178529688
testing: bce: 8.408548, dice: 27.599722, loss: 18.004135
IoU in current test batch is 0.9091826366379093
training: bce: 0.018990, dice: 0.062461, loss: 0.040725
training IoU in current batch 7600 is 0.9312385784032948
training IoU uptillnow 7600 is 0.0007825424553165343
testing: bce: 8.490687, dice: 27.927200, loss: 18.208944
IoU in current test batch is 0.9126952461503829
Epoch 29300: reducing learning rate of group 0 to 7.5719e-04.
training: bce: 0.018945, dice: 0.062406, loss: 0.040675
training IoU in current batch 7700 is 0.8865896878929919
training IoU uptillnow 7700 is 0.0007899327835625438
testing: bce: 8.582034, dice: 28.269901, loss: 18.425968
IoU in current test batch is 0.8925203759864945
Epoch 29401: reducing learning rate of group 0 to 7.5643e-04.
training: bce: 0.018867, dice: 0.062274, loss: 0.040570
training IoU in current batch 7800 is 0.9364368406882542
training IoU uptillnow 7800 is 0.0007978362525525945
testing: bce: 8.657810, dice: 28.576260, loss: 18.617035
IoU in current test batch is 0.913471987816867
Epoch 29502: reducing learning rate of group 0 to 7.5568e-04.
training: bce: 0.018848, dice: 0.062174, loss: 0.040511
training IoU in current batch 7900 is 0.9330909307662504
training IoU uptillnow 7900 is 0.0008056486405593566
testing: bce: 8.759797, dice: 28.896429, loss: 18.828113
IoU in current test batch is 0.8552417977723514
Epoch 29603: reducing learning rate of group 0 to 7.5492e-04.
training: bce: 0.019142, dice: 0.062340, loss: 0.040741
training IoU in current batch 8000 is 0.8972993695689214
training IoU uptillnow 8000 is 0.0008130067188691335
testing: bce: 9.009266, dice: 29.340058, loss: 19.174662
IoU in current test batch is 0.8841839844421195
Epoch 29704: reducing learning rate of group 0 to 7.5417e-04.
training: bce: 0.019072, dice: 0.062232, loss: 0.040652
training IoU in current batch 8100 is 0.9246669067338855
training IoU uptillnow 8100 is 0.0008206215386797951
testing: bce: 9.088388, dice: 29.655265, loss: 19.371826
IoU in current test batch is 0.8713017659503993
Epoch 29805: reducing learning rate of group 0 to 7.5341e-04.
training: bce: 0.019021, dice: 0.062176, loss: 0.040599
training IoU in current batch 8200 is 0.9143329274656462
training IoU uptillnow 8200 is 0.0008280702172289782
testing: bce: 9.176162, dice: 29.994649, loss: 19.585405
IoU in current test batch is 0.9074046752286654
Epoch 29906: reducing learning rate of group 0 to 7.5266e-04.
training: bce: 0.018944, dice: 0.062022, loss: 0.040483
training IoU in current batch 8300 is 0.9058725032890802
training IoU uptillnow 8300 is 0.0008353752332080934
testing: bce: 9.250313, dice: 30.285134, loss: 19.767724
IoU in current test batch is 0.9012753068789842
Epoch 30007: reducing learning rate of group 0 to 7.5191e-04.
training: bce: 0.018878, dice: 0.061881, loss: 0.040380
training IoU in current batch 8400 is 0.8514669301802569
training IoU uptillnow 8400 is 0.000842029212833983
testing: bce: 9.329209, dice: 30.580251, loss: 19.954730
IoU in current test batch is 0.9100979312779333
Epoch 30108: reducing learning rate of group 0 to 7.5116e-04.
training: bce: 0.018884, dice: 0.061852, loss: 0.040368
training IoU in current batch 8500 is 0.8949604011948151
training IoU uptillnow 8500 is 0.0008491191867561089
testing: bce: 9.442964, dice: 30.929428, loss: 20.186196
IoU in current test batch is 0.879521101113671
Epoch 30209: reducing learning rate of group 0 to 7.5040e-04.
training: bce: 0.018942, dice: 0.061913, loss: 0.040428
training IoU in current batch 8600 is 0.9221655176530277
training IoU uptillnow 8600 is 0.0008564616483801596
testing: bce: 9.583680, dice: 31.324523, loss: 20.454102
IoU in current test batch is 0.9218551890305685
Epoch 30310: reducing learning rate of group 0 to 7.4965e-04.
training: bce: 0.018897, dice: 0.061852, loss: 0.040374
training IoU in current batch 8700 is 0.895666654599428
training IoU uptillnow 8700 is 0.0008634652466267097
testing: bce: 9.671935, dice: 31.657126, loss: 20.664530
IoU in current test batch is 0.91490486158595
Epoch 30411: reducing learning rate of group 0 to 7.4890e-04.
training: bce: 0.018844, dice: 0.061805, loss: 0.040324
training IoU in current batch 8800 is 0.9264623100148406
training IoU uptillnow 8800 is 0.0008707594841789154
testing: bce: 9.755523, dice: 31.996932, loss: 20.876227
IoU in current test batch is 0.9001596434213232
Epoch 30512: reducing learning rate of group 0 to 7.4816e-04.
training: bce: 0.018847, dice: 0.061748, loss: 0.040297
training IoU in current batch 8900 is 0.9376657824933687
training IoU uptillnow 8900 is 0.00087812808915974
testing: bce: 9.867995, dice: 32.330465, loss: 21.099230
IoU in current test batch is 0.9045202256167103
Epoch 30613: reducing learning rate of group 0 to 7.4741e-04.
training: bce: 0.018937, dice: 0.061732, loss: 0.040334
training IoU in current batch 9000 is 0.8963820018365473
training IoU uptillnow 9000 is 0.0008850004406807891
testing: bce: 10.026321, dice: 32.685284, loss: 21.355802
IoU in current test batch is 0.9083042614134151
Epoch 30714: reducing learning rate of group 0 to 7.4666e-04.
training: bce: 0.018916, dice: 0.061685, loss: 0.040301
training IoU in current batch 9100 is 0.9305268054424933
training IoU uptillnow 9100 is 0.0008921976990491685
testing: bce: 10.126845, dice: 33.023335, loss: 21.575090
IoU in current test batch is 0.9007360735464045
Epoch 30815: reducing learning rate of group 0 to 7.4591e-04.
training: bce: 0.018924, dice: 0.061687, loss: 0.040305
training IoU in current batch 9200 is 0.9200453571878544
training IoU uptillnow 9200 is 0.0008992353047392776
testing: bce: 10.242254, dice: 33.386943, loss: 21.814599
IoU in current test batch is 0.8858304759053628
Epoch 30916: reducing learning rate of group 0 to 7.4517e-04.
training: bce: 0.018954, dice: 0.061712, loss: 0.040333
training IoU in current batch 9300 is 0.9225247524752476
training IoU uptillnow 9300 is 0.000906254166686089
testing: bce: 10.369870, dice: 33.763798, loss: 22.066834
IoU in current test batch is 0.8787959183430578
Epoch 31017: reducing learning rate of group 0 to 7.4442e-04.
training: bce: 0.019405, dice: 0.061768, loss: 0.040587
training IoU in current batch 9400 is 0.9287054409005628
training IoU uptillnow 9400 is 0.0009132941365777795
testing: bce: 10.731104, dice: 34.157679, loss: 22.444391
IoU in current test batch is 0.9072060390547703
Epoch 31118: reducing learning rate of group 0 to 7.4368e-04.
training: bce: 0.019387, dice: 0.061720, loss: 0.040554
training IoU in current batch 9500 is 0.9188281574823296
training IoU uptillnow 9500 is 0.0009201834519250978
testing: bce: 10.834977, dice: 34.494413, loss: 22.664695
IoU in current test batch is 0.8541014297189425
Epoch 31219: reducing learning rate of group 0 to 7.4293e-04.
training: bce: 0.019358, dice: 0.061741, loss: 0.040549
training IoU in current batch 9600 is 0.8352625992013345
training IoU uptillnow 9600 is 0.0009261388040403672
testing: bce: 10.932661, dice: 34.869066, loss: 22.900864
IoU in current test batch is 0.8954784749446981
Epoch 31320: reducing learning rate of group 0 to 7.4219e-04.
training: bce: 0.019361, dice: 0.061786, loss: 0.040574
training IoU in current batch 9700 is 0.9474228170133828
training IoU uptillnow 9700 is 0.0009332468844628862
testing: bce: 11.048523, dice: 35.257877, loss: 23.153200
IoU in current test batch is 0.9080804852664857
Epoch 31421: reducing learning rate of group 0 to 7.4145e-04.
training: bce: 0.019348, dice: 0.061784, loss: 0.040566
training IoU in current batch 9800 is 0.9293741355463347
training IoU uptillnow 9800 is 0.00094011884287778
testing: bce: 11.154880, dice: 35.620508, loss: 23.387694
IoU in current test batch is 0.8958924437194522
Epoch 31522: reducing learning rate of group 0 to 7.4071e-04.
training: bce: 0.019436, dice: 0.061811, loss: 0.040624
training IoU in current batch 9900 is 0.9422419773387117
training IoU uptillnow 9900 is 0.0009470830446127523
testing: bce: 11.319858, dice: 35.999632, loss: 23.659745
IoU in current test batch is 0.9016230790552213
Epoch 31623: reducing learning rate of group 0 to 7.3997e-04.
training: bce: 0.019427, dice: 0.061805, loss: 0.040616
training IoU in current batch 10000 is 0.9300003180358108
training IoU uptillnow 10000 is 0.0009538745840938457
testing: bce: 11.429019, dice: 36.359266, loss: 23.894143
IoU in current test batch is 0.8849050074386514
Epoch 31724: reducing learning rate of group 0 to 7.3923e-04.
training: bce: 0.019402, dice: 0.061780, loss: 0.040591
training IoU in current batch 10100 is 0.9117261948061395
training IoU uptillnow 10100 is 0.0009604318568357534
testing: bce: 11.528394, dice: 36.708397, loss: 24.118396
IoU in current test batch is 0.8921439601971527
Epoch 31825: reducing learning rate of group 0 to 7.3849e-04.
training: bce: 0.019350, dice: 0.061724, loss: 0.040537
training IoU in current batch 10200 is 0.880166299828809
training IoU uptillnow 10200 is 0.0009666182386829643
testing: bce: 11.611070, dice: 37.037895, loss: 24.324482
IoU in current test batch is 0.9109701119410062
Epoch 31926: reducing learning rate of group 0 to 7.3775e-04.
training: bce: 0.019292, dice: 0.061657, loss: 0.040474
training IoU in current batch 10300 is 0.9410803514339167
training IoU uptillnow 10300 is 0.0009734004770145166
testing: bce: 11.689904, dice: 37.360258, loss: 24.525081
IoU in current test batch is 0.9117840921924902
Epoch 32027: reducing learning rate of group 0 to 7.3701e-04.
training: bce: 0.019296, dice: 0.061613, loss: 0.040455
training IoU in current batch 10400 is 0.4552099975925237
training IoU uptillnow 10400 is 0.0009750950757111746
testing: bce: 11.806001, dice: 37.696260, loss: 24.751131
IoU in current test batch is 0.8806500052857773
Epoch 32128: reducing learning rate of group 0 to 7.3627e-04.
training: bce: 0.019276, dice: 0.061591, loss: 0.040433
training IoU in current batch 10500 is 0.432589438741453
training IoU uptillnow 10500 is 0.0009765449816741985
testing: bce: 11.906929, dice: 38.044876, loss: 24.975903
IoU in current test batch is 0.8937140010789191
Epoch 32229: reducing learning rate of group 0 to 7.3554e-04.
training: bce: 0.019227, dice: 0.061531, loss: 0.040379
training IoU in current batch 10600 is 0.8454314440393327
training IoU uptillnow 10600 is 0.0009822464053020322
testing: bce: 11.989472, dice: 38.369772, loss: 25.179622
IoU in current test batch is 0.87167468314732
Epoch 32330: reducing learning rate of group 0 to 7.3480e-04.
training: bce: 0.019247, dice: 0.061537, loss: 0.040392
training IoU in current batch 10700 is 0.9314805941542884
training IoU uptillnow 10700 is 0.0009887979142790247
testing: bce: 12.115549, dice: 38.735639, loss: 25.425594
IoU in current test batch is 0.8888728792107446
Epoch 32431: reducing learning rate of group 0 to 7.3407e-04.
training: bce: 0.019208, dice: 0.061472, loss: 0.040340
training IoU in current batch 10800 is 0.9364289508260881
training IoU uptillnow 10800 is 0.0009953598586538183
testing: bce: 12.204086, dice: 39.056652, loss: 25.630369
IoU in current test batch is 0.9021363244575284
Epoch 32532: reducing learning rate of group 0 to 7.3333e-04.
training: bce: 0.019195, dice: 0.061446, loss: 0.040321
training IoU in current batch 10900 is 0.961532563132604
training IoU uptillnow 10900 is 0.0010021382288535777
testing: bce: 12.308615, dice: 39.401532, loss: 25.855073
IoU in current test batch is 0.8868084334121663
Epoch 32633: reducing learning rate of group 0 to 7.3260e-04.
training: bce: 0.019252, dice: 0.061445, loss: 0.040348
training IoU in current batch 11000 is 0.9303974622130995
training IoU uptillnow 11000 is 0.0010085577598786238
testing: bce: 12.458036, dice: 39.762100, loss: 26.110068
IoU in current test batch is 0.8880528941492271
Epoch 32734: reducing learning rate of group 0 to 7.3187e-04.
training: bce: 0.019214, dice: 0.061427, loss: 0.040321
training IoU in current batch 11100 is 0.9136050360883486
training IoU uptillnow 11100 is 0.0010147674926847697
testing: bce: 12.546830, dice: 40.111997, loss: 26.329413
IoU in current test batch is 0.907561220271117
Epoch 32835: reducing learning rate of group 0 to 7.3114e-04.
training: bce: 0.019190, dice: 0.061369, loss: 0.040280
training IoU in current batch 11200 is 0.9514548693586699
training IoU uptillnow 11200 is 0.001021322959975076
testing: bce: 12.643790, dice: 40.435257, loss: 26.539523
IoU in current test batch is 0.8913286445392982
Epoch 32936: reducing learning rate of group 0 to 7.3040e-04.
training: bce: 0.019151, dice: 0.061325, loss: 0.040238
training IoU in current batch 11300 is 0.9464487034949267
training IoU uptillnow 11300 is 0.0010277881298286357
testing: bce: 12.730653, dice: 40.766498, loss: 26.748576
IoU in current test batch is 0.8835762332318262
Epoch 33037: reducing learning rate of group 0 to 7.2967e-04.
training: bce: 0.019115, dice: 0.061261, loss: 0.040188
training IoU in current batch 11400 is 0.9103429467512102
training IoU uptillnow 11400 is 0.0010338506324248352
testing: bce: 12.819211, dice: 41.084497, loss: 26.951854
IoU in current test batch is 0.9107173100735008
Epoch 33138: reducing learning rate of group 0 to 7.2894e-04.
training: bce: 0.019086, dice: 0.061245, loss: 0.040165
training IoU in current batch 11500 is 0.9151056632597389
training IoU uptillnow 11500 is 0.0010399244323598983
testing: bce: 12.911993, dice: 41.433779, loss: 27.172886
IoU in current test batch is 0.8567975696432205
Epoch 33239: reducing learning rate of group 0 to 7.2822e-04.
training: bce: 0.019054, dice: 0.061227, loss: 0.040141
training IoU in current batch 11600 is 0.8629017613945887
training IoU uptillnow 11600 is 0.0010454391914358403
testing: bce: 13.002509, dice: 41.782291, loss: 27.392400
IoU in current test batch is 0.9158423654204305
Maximum training samples requirement meet, I have been training for more than  100002  samples.
Making network now
ResNetUNet(
  (base_model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer0_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer1_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2_1x1): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3_1x1): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4_1x1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (upsample): Upsample(scale_factor=2.0, mode=bilinear)
  (conv_up3): Sequential(
    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up2): Sequential(
    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up1): Sequential(
    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up0): Sequential(
    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size2): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 512, 512]           1,792
              ReLU-2         [-1, 64, 512, 512]               0
            Conv2d-3         [-1, 64, 512, 512]          36,928
              ReLU-4         [-1, 64, 512, 512]               0
            Conv2d-5         [-1, 64, 256, 256]           9,408
            Conv2d-6         [-1, 64, 256, 256]           9,408
       BatchNorm2d-7         [-1, 64, 256, 256]             128
       BatchNorm2d-8         [-1, 64, 256, 256]             128
              ReLU-9         [-1, 64, 256, 256]               0
             ReLU-10         [-1, 64, 256, 256]               0
        MaxPool2d-11         [-1, 64, 128, 128]               0
        MaxPool2d-12         [-1, 64, 128, 128]               0
           Conv2d-13         [-1, 64, 128, 128]          36,864
           Conv2d-14         [-1, 64, 128, 128]          36,864
      BatchNorm2d-15         [-1, 64, 128, 128]             128
      BatchNorm2d-16         [-1, 64, 128, 128]             128
             ReLU-17         [-1, 64, 128, 128]               0
             ReLU-18         [-1, 64, 128, 128]               0
           Conv2d-19         [-1, 64, 128, 128]          36,864
           Conv2d-20         [-1, 64, 128, 128]          36,864
      BatchNorm2d-21         [-1, 64, 128, 128]             128
      BatchNorm2d-22         [-1, 64, 128, 128]             128
             ReLU-23         [-1, 64, 128, 128]               0
             ReLU-24         [-1, 64, 128, 128]               0
       BasicBlock-25         [-1, 64, 128, 128]               0
       BasicBlock-26         [-1, 64, 128, 128]               0
           Conv2d-27         [-1, 64, 128, 128]          36,864
           Conv2d-28         [-1, 64, 128, 128]          36,864
      BatchNorm2d-29         [-1, 64, 128, 128]             128
      BatchNorm2d-30         [-1, 64, 128, 128]             128
             ReLU-31         [-1, 64, 128, 128]               0
             ReLU-32         [-1, 64, 128, 128]               0
           Conv2d-33         [-1, 64, 128, 128]          36,864
           Conv2d-34         [-1, 64, 128, 128]          36,864
      BatchNorm2d-35         [-1, 64, 128, 128]             128
      BatchNorm2d-36         [-1, 64, 128, 128]             128
             ReLU-37         [-1, 64, 128, 128]               0
             ReLU-38         [-1, 64, 128, 128]               0
       BasicBlock-39         [-1, 64, 128, 128]               0
       BasicBlock-40         [-1, 64, 128, 128]               0
           Conv2d-41         [-1, 64, 128, 128]          36,864
           Conv2d-42         [-1, 64, 128, 128]          36,864
      BatchNorm2d-43         [-1, 64, 128, 128]             128
      BatchNorm2d-44         [-1, 64, 128, 128]             128
             ReLU-45         [-1, 64, 128, 128]               0
             ReLU-46         [-1, 64, 128, 128]               0
           Conv2d-47         [-1, 64, 128, 128]          36,864
           Conv2d-48         [-1, 64, 128, 128]          36,864
      BatchNorm2d-49         [-1, 64, 128, 128]             128
      BatchNorm2d-50         [-1, 64, 128, 128]             128
             ReLU-51         [-1, 64, 128, 128]               0
             ReLU-52         [-1, 64, 128, 128]               0
       BasicBlock-53         [-1, 64, 128, 128]               0
       BasicBlock-54         [-1, 64, 128, 128]               0
           Conv2d-55          [-1, 128, 64, 64]          73,728
           Conv2d-56          [-1, 128, 64, 64]          73,728
      BatchNorm2d-57          [-1, 128, 64, 64]             256
      BatchNorm2d-58          [-1, 128, 64, 64]             256
             ReLU-59          [-1, 128, 64, 64]               0
             ReLU-60          [-1, 128, 64, 64]               0
           Conv2d-61          [-1, 128, 64, 64]         147,456
           Conv2d-62          [-1, 128, 64, 64]         147,456
      BatchNorm2d-63          [-1, 128, 64, 64]             256
      BatchNorm2d-64          [-1, 128, 64, 64]             256
           Conv2d-65          [-1, 128, 64, 64]           8,192
           Conv2d-66          [-1, 128, 64, 64]           8,192
      BatchNorm2d-67          [-1, 128, 64, 64]             256
      BatchNorm2d-68          [-1, 128, 64, 64]             256
             ReLU-69          [-1, 128, 64, 64]               0
             ReLU-70          [-1, 128, 64, 64]               0
       BasicBlock-71          [-1, 128, 64, 64]               0
       BasicBlock-72          [-1, 128, 64, 64]               0
           Conv2d-73          [-1, 128, 64, 64]         147,456
           Conv2d-74          [-1, 128, 64, 64]         147,456
      BatchNorm2d-75          [-1, 128, 64, 64]             256
      BatchNorm2d-76          [-1, 128, 64, 64]             256
             ReLU-77          [-1, 128, 64, 64]               0
             ReLU-78          [-1, 128, 64, 64]               0
           Conv2d-79          [-1, 128, 64, 64]         147,456
           Conv2d-80          [-1, 128, 64, 64]         147,456
      BatchNorm2d-81          [-1, 128, 64, 64]             256
      BatchNorm2d-82          [-1, 128, 64, 64]             256
             ReLU-83          [-1, 128, 64, 64]               0
             ReLU-84          [-1, 128, 64, 64]               0
       BasicBlock-85          [-1, 128, 64, 64]               0
       BasicBlock-86          [-1, 128, 64, 64]               0
           Conv2d-87          [-1, 128, 64, 64]         147,456
           Conv2d-88          [-1, 128, 64, 64]         147,456
      BatchNorm2d-89          [-1, 128, 64, 64]             256
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
             ReLU-92          [-1, 128, 64, 64]               0
           Conv2d-93          [-1, 128, 64, 64]         147,456
           Conv2d-94          [-1, 128, 64, 64]         147,456
      BatchNorm2d-95          [-1, 128, 64, 64]             256
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
             ReLU-98          [-1, 128, 64, 64]               0
       BasicBlock-99          [-1, 128, 64, 64]               0
      BasicBlock-100          [-1, 128, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]         147,456
          Conv2d-102          [-1, 128, 64, 64]         147,456
     BatchNorm2d-103          [-1, 128, 64, 64]             256
     BatchNorm2d-104          [-1, 128, 64, 64]             256
            ReLU-105          [-1, 128, 64, 64]               0
            ReLU-106          [-1, 128, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]         147,456
          Conv2d-108          [-1, 128, 64, 64]         147,456
     BatchNorm2d-109          [-1, 128, 64, 64]             256
     BatchNorm2d-110          [-1, 128, 64, 64]             256
            ReLU-111          [-1, 128, 64, 64]               0
            ReLU-112          [-1, 128, 64, 64]               0
      BasicBlock-113          [-1, 128, 64, 64]               0
      BasicBlock-114          [-1, 128, 64, 64]               0
          Conv2d-115          [-1, 256, 32, 32]         294,912
          Conv2d-116          [-1, 256, 32, 32]         294,912
     BatchNorm2d-117          [-1, 256, 32, 32]             512
     BatchNorm2d-118          [-1, 256, 32, 32]             512
            ReLU-119          [-1, 256, 32, 32]               0
            ReLU-120          [-1, 256, 32, 32]               0
          Conv2d-121          [-1, 256, 32, 32]         589,824
          Conv2d-122          [-1, 256, 32, 32]         589,824
     BatchNorm2d-123          [-1, 256, 32, 32]             512
     BatchNorm2d-124          [-1, 256, 32, 32]             512
          Conv2d-125          [-1, 256, 32, 32]          32,768
          Conv2d-126          [-1, 256, 32, 32]          32,768
     BatchNorm2d-127          [-1, 256, 32, 32]             512
     BatchNorm2d-128          [-1, 256, 32, 32]             512
            ReLU-129          [-1, 256, 32, 32]               0
            ReLU-130          [-1, 256, 32, 32]               0
      BasicBlock-131          [-1, 256, 32, 32]               0
      BasicBlock-132          [-1, 256, 32, 32]               0
          Conv2d-133          [-1, 256, 32, 32]         589,824
          Conv2d-134          [-1, 256, 32, 32]         589,824
     BatchNorm2d-135          [-1, 256, 32, 32]             512
     BatchNorm2d-136          [-1, 256, 32, 32]             512
            ReLU-137          [-1, 256, 32, 32]               0
            ReLU-138          [-1, 256, 32, 32]               0
          Conv2d-139          [-1, 256, 32, 32]         589,824
          Conv2d-140          [-1, 256, 32, 32]         589,824
     BatchNorm2d-141          [-1, 256, 32, 32]             512
     BatchNorm2d-142          [-1, 256, 32, 32]             512
            ReLU-143          [-1, 256, 32, 32]               0
            ReLU-144          [-1, 256, 32, 32]               0
      BasicBlock-145          [-1, 256, 32, 32]               0
      BasicBlock-146          [-1, 256, 32, 32]               0
          Conv2d-147          [-1, 256, 32, 32]         589,824
          Conv2d-148          [-1, 256, 32, 32]         589,824
     BatchNorm2d-149          [-1, 256, 32, 32]             512
     BatchNorm2d-150          [-1, 256, 32, 32]             512
            ReLU-151          [-1, 256, 32, 32]               0
            ReLU-152          [-1, 256, 32, 32]               0
          Conv2d-153          [-1, 256, 32, 32]         589,824
          Conv2d-154          [-1, 256, 32, 32]         589,824
     BatchNorm2d-155          [-1, 256, 32, 32]             512
     BatchNorm2d-156          [-1, 256, 32, 32]             512
            ReLU-157          [-1, 256, 32, 32]               0
            ReLU-158          [-1, 256, 32, 32]               0
      BasicBlock-159          [-1, 256, 32, 32]               0
      BasicBlock-160          [-1, 256, 32, 32]               0
          Conv2d-161          [-1, 256, 32, 32]         589,824
          Conv2d-162          [-1, 256, 32, 32]         589,824
     BatchNorm2d-163          [-1, 256, 32, 32]             512
     BatchNorm2d-164          [-1, 256, 32, 32]             512
            ReLU-165          [-1, 256, 32, 32]               0
            ReLU-166          [-1, 256, 32, 32]               0
          Conv2d-167          [-1, 256, 32, 32]         589,824
          Conv2d-168          [-1, 256, 32, 32]         589,824
     BatchNorm2d-169          [-1, 256, 32, 32]             512
     BatchNorm2d-170          [-1, 256, 32, 32]             512
            ReLU-171          [-1, 256, 32, 32]               0
            ReLU-172          [-1, 256, 32, 32]               0
      BasicBlock-173          [-1, 256, 32, 32]               0
      BasicBlock-174          [-1, 256, 32, 32]               0
          Conv2d-175          [-1, 256, 32, 32]         589,824
          Conv2d-176          [-1, 256, 32, 32]         589,824
     BatchNorm2d-177          [-1, 256, 32, 32]             512
     BatchNorm2d-178          [-1, 256, 32, 32]             512
            ReLU-179          [-1, 256, 32, 32]               0
            ReLU-180          [-1, 256, 32, 32]               0
          Conv2d-181          [-1, 256, 32, 32]         589,824
          Conv2d-182          [-1, 256, 32, 32]         589,824
     BatchNorm2d-183          [-1, 256, 32, 32]             512
     BatchNorm2d-184          [-1, 256, 32, 32]             512
            ReLU-185          [-1, 256, 32, 32]               0
            ReLU-186          [-1, 256, 32, 32]               0
      BasicBlock-187          [-1, 256, 32, 32]               0
      BasicBlock-188          [-1, 256, 32, 32]               0
          Conv2d-189          [-1, 256, 32, 32]         589,824
          Conv2d-190          [-1, 256, 32, 32]         589,824
     BatchNorm2d-191          [-1, 256, 32, 32]             512
     BatchNorm2d-192          [-1, 256, 32, 32]             512
            ReLU-193          [-1, 256, 32, 32]               0
            ReLU-194          [-1, 256, 32, 32]               0
          Conv2d-195          [-1, 256, 32, 32]         589,824
          Conv2d-196          [-1, 256, 32, 32]         589,824
     BatchNorm2d-197          [-1, 256, 32, 32]             512
     BatchNorm2d-198          [-1, 256, 32, 32]             512
            ReLU-199          [-1, 256, 32, 32]               0
            ReLU-200          [-1, 256, 32, 32]               0
      BasicBlock-201          [-1, 256, 32, 32]               0
      BasicBlock-202          [-1, 256, 32, 32]               0
          Conv2d-203          [-1, 512, 16, 16]       1,179,648
          Conv2d-204          [-1, 512, 16, 16]       1,179,648
     BatchNorm2d-205          [-1, 512, 16, 16]           1,024
     BatchNorm2d-206          [-1, 512, 16, 16]           1,024
            ReLU-207          [-1, 512, 16, 16]               0
            ReLU-208          [-1, 512, 16, 16]               0
          Conv2d-209          [-1, 512, 16, 16]       2,359,296
          Conv2d-210          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-211          [-1, 512, 16, 16]           1,024
     BatchNorm2d-212          [-1, 512, 16, 16]           1,024
          Conv2d-213          [-1, 512, 16, 16]         131,072
          Conv2d-214          [-1, 512, 16, 16]         131,072
     BatchNorm2d-215          [-1, 512, 16, 16]           1,024
     BatchNorm2d-216          [-1, 512, 16, 16]           1,024
            ReLU-217          [-1, 512, 16, 16]               0
            ReLU-218          [-1, 512, 16, 16]               0
      BasicBlock-219          [-1, 512, 16, 16]               0
      BasicBlock-220          [-1, 512, 16, 16]               0
          Conv2d-221          [-1, 512, 16, 16]       2,359,296
          Conv2d-222          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-223          [-1, 512, 16, 16]           1,024
     BatchNorm2d-224          [-1, 512, 16, 16]           1,024
            ReLU-225          [-1, 512, 16, 16]               0
            ReLU-226          [-1, 512, 16, 16]               0
          Conv2d-227          [-1, 512, 16, 16]       2,359,296
          Conv2d-228          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-229          [-1, 512, 16, 16]           1,024
     BatchNorm2d-230          [-1, 512, 16, 16]           1,024
            ReLU-231          [-1, 512, 16, 16]               0
            ReLU-232          [-1, 512, 16, 16]               0
      BasicBlock-233          [-1, 512, 16, 16]               0
      BasicBlock-234          [-1, 512, 16, 16]               0
          Conv2d-235          [-1, 512, 16, 16]       2,359,296
          Conv2d-236          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-237          [-1, 512, 16, 16]           1,024
     BatchNorm2d-238          [-1, 512, 16, 16]           1,024
            ReLU-239          [-1, 512, 16, 16]               0
            ReLU-240          [-1, 512, 16, 16]               0
          Conv2d-241          [-1, 512, 16, 16]       2,359,296
          Conv2d-242          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-243          [-1, 512, 16, 16]           1,024
     BatchNorm2d-244          [-1, 512, 16, 16]           1,024
            ReLU-245          [-1, 512, 16, 16]               0
            ReLU-246          [-1, 512, 16, 16]               0
      BasicBlock-247          [-1, 512, 16, 16]               0
      BasicBlock-248          [-1, 512, 16, 16]               0
          Conv2d-249          [-1, 512, 16, 16]         262,656
            ReLU-250          [-1, 512, 16, 16]               0
        Upsample-251          [-1, 512, 32, 32]               0
          Conv2d-252          [-1, 256, 32, 32]          65,792
            ReLU-253          [-1, 256, 32, 32]               0
          Conv2d-254          [-1, 512, 32, 32]       3,539,456
            ReLU-255          [-1, 512, 32, 32]               0
        Upsample-256          [-1, 512, 64, 64]               0
          Conv2d-257          [-1, 128, 64, 64]          16,512
            ReLU-258          [-1, 128, 64, 64]               0
          Conv2d-259          [-1, 256, 64, 64]       1,474,816
            ReLU-260          [-1, 256, 64, 64]               0
        Upsample-261        [-1, 256, 128, 128]               0
          Conv2d-262         [-1, 64, 128, 128]           4,160
            ReLU-263         [-1, 64, 128, 128]               0
          Conv2d-264        [-1, 256, 128, 128]         737,536
            ReLU-265        [-1, 256, 128, 128]               0
        Upsample-266        [-1, 256, 256, 256]               0
          Conv2d-267         [-1, 64, 256, 256]           4,160
            ReLU-268         [-1, 64, 256, 256]               0
          Conv2d-269        [-1, 128, 256, 256]         368,768
            ReLU-270        [-1, 128, 256, 256]               0
        Upsample-271        [-1, 128, 512, 512]               0
          Conv2d-272         [-1, 64, 512, 512]         110,656
            ReLU-273         [-1, 64, 512, 512]               0
          Conv2d-274          [-1, 1, 512, 512]              65
================================================================
Total params: 49,192,641
Trainable params: 49,192,641
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 2522.00
Params size (MB): 187.66
Estimated Total Size (MB): 2712.66
----------------------------------------------------------------
Start training now...
training: bce: 2.256118, dice: 0.878349, loss: 2.256118
training IoU in current batch 0 is 0.058326821952710516
training IoU uptillnow 0 is 0.019442273984236837
testing: bce: 0.132713, dice: 0.051668, loss: 0.132713
IoU in current test batch is 0.0
training: bce: 1.361074, dice: 0.896112, loss: 1.361074
training IoU in current batch 100 is 0.0
training IoU uptillnow 100 is 0.00019249776222016673
testing: bce: 8.086383, dice: 5.323958, loss: 8.086383
IoU in current test batch is 0.0
training: bce: 0.809947, dice: 0.799337, loss: 0.809947
training IoU in current batch 200 is 0.617501626545218
training IoU uptillnow 200 is 0.0011207768631806443
testing: bce: 9.576429, dice: 9.450988, loss: 9.576429
IoU in current test batch is 0.15364548731571456
training: bce: 0.564109, dice: 0.673731, loss: 0.564109
training IoU in current batch 300 is 0.700741374394069
training IoU uptillnow 300 is 0.001524440556912511
testing: bce: 9.988047, dice: 11.928994, loss: 9.988047
IoU in current test batch is 0.6979078467821136
training: bce: 0.460126, dice: 0.574788, loss: 0.460126
training IoU in current batch 400 is 0.6456993090302597
training IoU uptillnow 400 is 0.0016810217222961406
testing: bce: 10.853563, dice: 13.558235, loss: 10.853563
IoU in current test batch is 0.5123856697004706
Epoch   476: reducing learning rate of group 0 to 9.9900e-04.
training: bce: 0.383698, dice: 0.546715, loss: 0.383698
training IoU in current batch 500 is 0.6945358939186099
training IoU uptillnow 500 is 0.0018075881742121538
testing: bce: 11.307804, dice: 16.112014, loss: 11.307804
IoU in current test batch is 0.677615752231869
Epoch   577: reducing learning rate of group 0 to 9.9800e-04.
training: bce: 0.325994, dice: 0.495524, loss: 0.325994
training IoU in current batch 600 is 0.7052475304059841
training IoU uptillnow 600 is 0.0018979770140026906
testing: bce: 11.524863, dice: 17.518240, loss: 11.524863
IoU in current test batch is 0.7655301052394733
training: bce: 0.283958, dice: 0.453898, loss: 0.283958
training IoU in current batch 700 is 0.8290200202832138
training IoU uptillnow 700 is 0.0020214325138041204
testing: bce: 11.709093, dice: 18.716623, loss: 11.709093
IoU in current test batch is 0.6893104713809819
training: bce: 0.257902, dice: 0.445938, loss: 0.257902
training IoU in current batch 800 is 0.0
training IoU uptillnow 800 is 0.001769068904090747
testing: bce: 12.151725, dice: 21.011568, loss: 12.151725
IoU in current test batch is 0.0
Epoch   821: reducing learning rate of group 0 to 9.9700e-04.
training: bce: 0.240151, dice: 0.461198, loss: 0.240151
training IoU in current batch 900 is 0.0
training IoU uptillnow 900 is 0.0015727238536922179
testing: bce: 12.728026, dice: 24.443501, loss: 12.728026
IoU in current test batch is 0.0
Epoch   922: reducing learning rate of group 0 to 9.9601e-04.
training: bce: 0.223096, dice: 0.470707, loss: 0.223096
training IoU in current batch 1000 is 0.0
training IoU uptillnow 1000 is 0.0014156085835930953
testing: bce: 13.136413, dice: 27.716359, loss: 13.136413
IoU in current test batch is 0.0
Epoch  1023: reducing learning rate of group 0 to 9.9501e-04.
training: bce: 0.207102, dice: 0.470223, loss: 0.207102
training IoU in current batch 1100 is 0.0
training IoU uptillnow 1100 is 0.0012870337803602982
testing: bce: 13.412905, dice: 30.453846, loss: 13.412905
IoU in current test batch is 0.0
Epoch  1124: reducing learning rate of group 0 to 9.9401e-04.
training: bce: 0.194370, dice: 0.471243, loss: 0.194370
training IoU in current batch 1200 is 0.0
training IoU uptillnow 1200 is 0.0011798702682570261
testing: bce: 13.731692, dice: 33.291945, loss: 13.731692
IoU in current test batch is 0.0
Epoch  1225: reducing learning rate of group 0 to 9.9302e-04.
training: bce: 0.182862, dice: 0.468923, loss: 0.182862
training IoU in current batch 1300 is 0.0
training IoU uptillnow 1300 is 0.0010891807779989918
testing: bce: 13.994322, dice: 35.886378, loss: 13.994322
IoU in current test batch is 0.0
Epoch  1326: reducing learning rate of group 0 to 9.9203e-04.
training: bce: 0.175184, dice: 0.473777, loss: 0.175184
training IoU in current batch 1400 is 0.0
training IoU uptillnow 1400 is 0.001011437681782076
testing: bce: 14.437260, dice: 39.044775, loss: 14.437260
IoU in current test batch is 0.0
Epoch  1427: reducing learning rate of group 0 to 9.9104e-04.
training: bce: 0.166439, dice: 0.470970, loss: 0.166439
training IoU in current batch 1500 is 0.0
training IoU uptillnow 1500 is 0.000944053425833903
testing: bce: 14.695566, dice: 41.583856, loss: 14.695566
IoU in current test batch is 0.0
Epoch  1528: reducing learning rate of group 0 to 9.9004e-04.
training: bce: 0.159314, dice: 0.468467, loss: 0.159314
training IoU in current batch 1600 is 0.7928366906113428
training IoU uptillnow 1600 is 0.0010501580818533017
testing: bce: 15.003618, dice: 44.118570, loss: 15.003618
IoU in current test batch is 0.7073613815168527
Epoch  1629: reducing learning rate of group 0 to 9.8905e-04.
training: bce: 0.152329, dice: 0.464988, loss: 0.152329
training IoU in current batch 1700 is 0.8910248470285134
training IoU uptillnow 1700 is 0.0011630284370311428
testing: bce: 15.241881, dice: 46.526148, loss: 15.241881
IoU in current test batch is 0.8137212407876456
Epoch  1730: reducing learning rate of group 0 to 9.8807e-04.
training: bce: 0.146693, dice: 0.463246, loss: 0.146693
training IoU in current batch 1800 is 0.5544919419362213
training IoU uptillnow 1800 is 0.0012010783002232358
testing: bce: 15.540799, dice: 49.076878, loss: 15.540799
IoU in current test batch is 0.5878176652382598
Epoch  1831: reducing learning rate of group 0 to 9.8708e-04.
training: bce: 0.141161, dice: 0.459651, loss: 0.141161
training IoU in current batch 1900 is 0.8375756654762809
training IoU uptillnow 1900 is 0.0012847627076244825
testing: bce: 15.785134, dice: 51.399821, loss: 15.785134
IoU in current test batch is 0.8573082099298531
Epoch  1932: reducing learning rate of group 0 to 9.8609e-04.
training: bce: 0.135646, dice: 0.450743, loss: 0.135646
training IoU in current batch 2000 is 0.8481559369558826
training IoU uptillnow 2000 is 0.0013618453537461777
testing: bce: 15.966311, dice: 53.055135, loss: 15.966311
IoU in current test batch is 0.8036411187132306
training: bce: 0.130108, dice: 0.435937, loss: 0.130108
training IoU in current batch 2100 is 0.89738836047074
training IoU uptillnow 2100 is 0.0014394012405218224
testing: bce: 16.079759, dice: 53.876722, loss: 16.079759
IoU in current test batch is 0.8251663032116109
Epoch  2132: reducing learning rate of group 0 to 9.8510e-04.
training: bce: 0.126225, dice: 0.429063, loss: 0.126225
training IoU in current batch 2200 is 0.8610518188836913
training IoU uptillnow 2200 is 0.001504406760244243
testing: bce: 16.342368, dice: 55.551048, loss: 16.342368
IoU in current test batch is 0.7703287115890565
Epoch  2233: reducing learning rate of group 0 to 9.8412e-04.
training: bce: 0.122426, dice: 0.426862, loss: 0.122426
training IoU in current batch 2300 is 0.808933933933934
training IoU uptillnow 2300 is 0.0015562120486493803
testing: bce: 16.570686, dice: 57.776993, loss: 16.570686
IoU in current test batch is 0.7779817079198494
Epoch  2334: reducing learning rate of group 0 to 9.8314e-04.
training: bce: 0.119057, dice: 0.424413, loss: 0.119057
training IoU in current batch 2400 is 0.8913920389679579
training IoU uptillnow 2400 is 0.0016151497724274093
testing: bce: 16.814982, dice: 59.942156, loss: 16.814982
IoU in current test batch is 0.7459343011888615
Epoch  2435: reducing learning rate of group 0 to 9.8215e-04.
training: bce: 0.115747, dice: 0.421591, loss: 0.115747
training IoU in current batch 2500 is 0.826591740444256
training IoU uptillnow 2500 is 0.0016607377783871631
testing: bce: 17.028440, dice: 62.023531, loss: 17.028440
IoU in current test batch is 0.8290940856976227
Epoch  2536: reducing learning rate of group 0 to 9.8117e-04.
training: bce: 0.112645, dice: 0.418234, loss: 0.112645
training IoU in current batch 2600 is 0.9085210425011705
training IoU uptillnow 2600 is 0.0017133200812174875
testing: bce: 17.234646, dice: 63.989832, loss: 17.234646
IoU in current test batch is 0.8547682372514034
training: bce: 0.109250, dice: 0.408248, loss: 0.109250
training IoU in current batch 2700 is 0.672942981938378
training IoU uptillnow 2700 is 0.0017329358972822948
testing: bce: 17.357955, dice: 64.863333, loss: 17.357955
IoU in current test batch is 0.756846480697113
Epoch  2732: reducing learning rate of group 0 to 9.8019e-04.
training: bce: 0.106291, dice: 0.399082, loss: 0.106291
training IoU in current batch 2800 is 0.633547996569872
training IoU uptillnow 2800 is 0.0017464628790013455
testing: bce: 17.512948, dice: 65.754614, loss: 17.512948
IoU in current test batch is 0.7674199357033141
Epoch  2893: reducing learning rate of group 0 to 9.7921e-04.
training: bce: 0.103492, dice: 0.391394, loss: 0.103492
training IoU in current batch 2900 is 0.8553321678321678
training IoU uptillnow 2900 is 0.001784540933020852
testing: bce: 17.660551, dice: 66.790196, loss: 17.660551
IoU in current test batch is 0.8020013523234301
training: bce: 0.100581, dice: 0.382120, loss: 0.100581
training IoU in current batch 3000 is 0.899681206355632
training IoU uptillnow 3000 is 0.001825007324940143
testing: bce: 17.755466, dice: 67.455430, loss: 17.755466
IoU in current test batch is 0.8566414408265928
Epoch  3065: reducing learning rate of group 0 to 9.7823e-04.
training: bce: 0.097822, dice: 0.373577, loss: 0.097822
training IoU in current batch 3100 is 0.9104044071616376
training IoU uptillnow 3100 is 0.0018640164843166448
testing: bce: 17.843839, dice: 68.144893, loss: 17.843839
IoU in current test batch is 0.8503054305687912
training: bce: 0.095363, dice: 0.365869, loss: 0.095363
training IoU in current batch 3200 is 0.8882048678323232
training IoU uptillnow 3200 is 0.0018982766032937695
testing: bce: 17.956276, dice: 68.890989, loss: 17.956276
IoU in current test batch is 0.8012620884153071
Epoch  3249: reducing learning rate of group 0 to 9.7725e-04.
training: bce: 0.092924, dice: 0.358333, loss: 0.092924
training IoU in current batch 3300 is 0.8916228999216551
training IoU uptillnow 3300 is 0.0019308061316118067
testing: bce: 18.043737, dice: 69.579789, loss: 18.043737
IoU in current test batch is 0.8253945682218591
Epoch  3368: reducing learning rate of group 0 to 9.7627e-04.
training: bce: 0.090606, dice: 0.350963, loss: 0.090606
training IoU in current batch 3400 is 0.8920093051935312
training IoU uptillnow 3400 is 0.001961460592624253
testing: bce: 18.126462, dice: 70.213157, loss: 18.126462
IoU in current test batch is 0.7994286975263613
Epoch  3469: reducing learning rate of group 0 to 9.7530e-04.
training: bce: 0.088734, dice: 0.345445, loss: 0.088734
training IoU in current batch 3500 is 0.859843335275458
training IoU uptillnow 3500 is 0.0019873013197963165
testing: bce: 18.273913, dice: 71.141428, loss: 18.273913
IoU in current test batch is 0.7688543777426508
Epoch  3570: reducing learning rate of group 0 to 9.7432e-04.
training: bce: 0.086950, dice: 0.340302, loss: 0.086950
training IoU in current batch 3600 is 0.7990305051253551
training IoU uptillnow 3600 is 0.0020060775957554443
testing: bce: 18.418021, dice: 72.084060, loss: 18.418021
IoU in current test batch is 0.8441480402894324
Epoch  3671: reducing learning rate of group 0 to 9.7335e-04.
training: bce: 0.085130, dice: 0.334710, loss: 0.085130
training IoU in current batch 3700 is 0.9080776605944392
training IoU uptillnow 3700 is 0.0020336606257354323
testing: bce: 18.533347, dice: 72.868356, loss: 18.533347
IoU in current test batch is 0.8624280779164034
Epoch  3772: reducing learning rate of group 0 to 9.7237e-04.
training: bce: 0.083612, dice: 0.330185, loss: 0.083612
training IoU in current batch 3800 is 0.8464157484214436
training IoU uptillnow 3800 is 0.002054384782597733
testing: bce: 18.694613, dice: 73.825538, loss: 18.694613
IoU in current test batch is 0.8704189931744815
Epoch  3873: reducing learning rate of group 0 to 9.7140e-04.
training: bce: 0.081808, dice: 0.324404, loss: 0.081808
training IoU in current batch 3900 is 0.8404026892886057
training IoU uptillnow 3900 is 0.002073532629689016
testing: bce: 18.772630, dice: 74.441099, loss: 18.772630
IoU in current test batch is 0.8686249874690781
training: bce: 0.080511, dice: 0.320755, loss: 0.080511
training IoU in current batch 4000 is 0.9118708855383308
training IoU uptillnow 4000 is 0.002097677518186194
testing: bce: 18.948599, dice: 75.490668, loss: 18.948599
IoU in current test batch is 0.8529268691037621
Epoch  4016: reducing learning rate of group 0 to 9.7043e-04.
training: bce: 0.079025, dice: 0.316239, loss: 0.079025
training IoU in current batch 4100 is 0.8069274868670393
training IoU uptillnow 4100 is 0.0021121149912749675
testing: bce: 19.063701, dice: 76.287953, loss: 19.063701
IoU in current test batch is 0.8564616369091695
Epoch  4117: reducing learning rate of group 0 to 9.6946e-04.
training: bce: 0.077666, dice: 0.312242, loss: 0.077666
training IoU in current batch 4200 is 0.8879363135333742
training IoU uptillnow 4200 is 0.002132292870839427
testing: bce: 19.192639, dice: 77.160439, loss: 19.192639
IoU in current test batch is 0.8013597011397866
Epoch  4218: reducing learning rate of group 0 to 9.6849e-04.
training: bce: 0.076247, dice: 0.307667, loss: 0.076247
training IoU in current batch 4300 is 0.8699608610567515
training IoU uptillnow 4300 is 0.00215013934063753
testing: bce: 19.290500, dice: 77.839854, loss: 19.290500
IoU in current test batch is 0.8645519886538802
Epoch  4319: reducing learning rate of group 0 to 9.6752e-04.
training: bce: 0.075005, dice: 0.303665, loss: 0.075005
training IoU in current batch 4400 is 0.8893893773334939
training IoU uptillnow 4400 is 0.0021686463144421375
testing: bce: 19.417583, dice: 78.613592, loss: 19.417583
IoU in current test batch is 0.8289586862962712
Epoch  4420: reducing learning rate of group 0 to 9.6656e-04.
training: bce: 0.073714, dice: 0.299674, loss: 0.073714
training IoU in current batch 4500 is 0.8835812977743484
training IoU uptillnow 4500 is 0.0021859008062914825
testing: bce: 19.516793, dice: 79.343129, loss: 19.516793
IoU in current test batch is 0.8767700129864919
Epoch  4521: reducing learning rate of group 0 to 9.6559e-04.
training: bce: 0.072559, dice: 0.295912, loss: 0.072559
training IoU in current batch 4600 is 0.8415036382405519
training IoU uptillnow 4600 is 0.0021993568228352127
testing: bce: 19.638011, dice: 80.087771, loss: 19.638011
IoU in current test batch is 0.8663435501128866
Epoch  4622: reducing learning rate of group 0 to 9.6462e-04.
training: bce: 0.071326, dice: 0.291946, loss: 0.071326
training IoU in current batch 4700 is 0.8876547968968342
training IoU uptillnow 4700 is 0.0022155128002900996
testing: bce: 19.723618, dice: 80.731537, loss: 19.723618
IoU in current test batch is 0.8782168138385557
Epoch  4723: reducing learning rate of group 0 to 9.6366e-04.
training: bce: 0.070259, dice: 0.288552, loss: 0.070259
training IoU in current batch 4800 is 0.8374213353798926
training IoU uptillnow 4800 is 0.0022275080440096627
testing: bce: 19.842072, dice: 81.490468, loss: 19.842072
IoU in current test batch is 0.8312588456779806
Epoch  4824: reducing learning rate of group 0 to 9.6269e-04.
training: bce: 0.069127, dice: 0.284783, loss: 0.069127
training IoU in current batch 4900 is 0.8588485385296722
training IoU uptillnow 4900 is 0.002240471121295031
testing: bce: 19.928923, dice: 82.101368, loss: 19.928923
IoU in current test batch is 0.870883835243606
Epoch  4925: reducing learning rate of group 0 to 9.6173e-04.
training: bce: 0.068013, dice: 0.281121, loss: 0.068013
training IoU in current batch 5000 is 0.9172576220301257
training IoU uptillnow 5000 is 0.002256808939440843
testing: bce: 20.007801, dice: 82.699209, loss: 20.007801
IoU in current test batch is 0.884570506792978
Epoch  5026: reducing learning rate of group 0 to 9.6077e-04.
training: bce: 0.066945, dice: 0.277497, loss: 0.066945
training IoU in current batch 5100 is 0.9116153846153846
training IoU uptillnow 5100 is 0.0022721374830455697
testing: bce: 20.087303, dice: 83.265306, loss: 20.087303
IoU in current test batch is 0.8196254517437443
Epoch  5127: reducing learning rate of group 0 to 9.5981e-04.
training: bce: 0.065961, dice: 0.274204, loss: 0.065961
training IoU in current batch 5200 is 0.8847076609590167
training IoU uptillnow 5200 is 0.0022851520581942814
testing: bce: 20.180246, dice: 83.890273, loss: 20.180246
IoU in current test batch is 0.8844695484709435
Epoch  5228: reducing learning rate of group 0 to 9.5885e-04.
training: bce: 0.064994, dice: 0.271039, loss: 0.064994
training IoU in current batch 5300 is 0.9022867055669452
training IoU uptillnow 5300 is 0.0022987810016708997
testing: bce: 20.266558, dice: 84.516446, loss: 20.266558
IoU in current test batch is 0.8444861520329316
Epoch  5329: reducing learning rate of group 0 to 9.5789e-04.
training: bce: 0.064168, dice: 0.268415, loss: 0.064168
training IoU in current batch 5400 is 0.8872468527640941
training IoU uptillnow 5400 is 0.0023109770488388822
testing: bce: 20.386622, dice: 85.277175, loss: 20.386622
IoU in current test batch is 0.8864475124508725
Epoch  5430: reducing learning rate of group 0 to 9.5693e-04.
training: bce: 0.063258, dice: 0.265476, loss: 0.063258
training IoU in current batch 5500 is 0.8450851961492739
training IoU uptillnow 5500 is 0.002320174896593691
testing: bce: 20.469637, dice: 85.905069, loss: 20.469637
IoU in current test batch is 0.8749307149496431
Epoch  5531: reducing learning rate of group 0 to 9.5598e-04.
training: bce: 0.062362, dice: 0.262495, loss: 0.062362
training IoU in current batch 5600 is 0.8901656786745706
training IoU uptillnow 5600 is 0.0023317271914039314
testing: bce: 20.546455, dice: 86.484439, loss: 20.546455
IoU in current test batch is 0.8601444580026881
Epoch  5632: reducing learning rate of group 0 to 9.5502e-04.
training: bce: 0.062594, dice: 0.263748, loss: 0.062594
training IoU in current batch 5700 is 0.7701385163916322
training IoU uptillnow 5700 is 0.0023358563125505404
testing: bce: 20.990919, dice: 88.448816, loss: 20.990919
IoU in current test batch is 0.7814908836067423
Epoch  5733: reducing learning rate of group 0 to 9.5406e-04.
training: bce: 0.061969, dice: 0.262072, loss: 0.061969
training IoU in current batch 5800 is 0.8354500403365218
training IoU uptillnow 5800 is 0.002343595963563088
testing: bce: 21.145896, dice: 89.428383, loss: 21.145896
IoU in current test batch is 0.7853317572663031
Epoch  5834: reducing learning rate of group 0 to 9.5311e-04.
training: bce: 0.061236, dice: 0.259986, loss: 0.061236
training IoU in current batch 5900 is 0.8707516339869281
training IoU uptillnow 5900 is 0.0023530674003205865
testing: bce: 21.255974, dice: 90.245641, loss: 21.255974
IoU in current test batch is 0.8482116525362623
Epoch  5935: reducing learning rate of group 0 to 9.5216e-04.
training: bce: 0.060541, dice: 0.257912, loss: 0.060541
training IoU in current batch 6000 is 0.8836280963333069
training IoU uptillnow 6000 is 0.0023629384149424343
testing: bce: 21.371084, dice: 91.043109, loss: 21.371084
IoU in current test batch is 0.8455888869676224
Epoch  6036: reducing learning rate of group 0 to 9.5121e-04.
training: bce: 0.059812, dice: 0.255580, loss: 0.059812
training IoU in current batch 6100 is 0.8566626164439044
training IoU uptillnow 6100 is 0.0023710125608180383
testing: bce: 21.465531, dice: 91.723326, loss: 21.465531
IoU in current test batch is 0.87314729052185
Epoch  6137: reducing learning rate of group 0 to 9.5025e-04.
training: bce: 0.059138, dice: 0.253409, loss: 0.059138
training IoU in current batch 6200 is 0.8822501230920728
training IoU uptillnow 6200 is 0.0023802017429309587
testing: bce: 21.571294, dice: 92.434677, loss: 21.571294
IoU in current test batch is 0.8734254538230689
Epoch  6238: reducing learning rate of group 0 to 9.4930e-04.
training: bce: 0.058730, dice: 0.252371, loss: 0.058730
training IoU in current batch 6300 is 0.9199023199023199
training IoU uptillnow 6300 is 0.0023910911148308173
testing: bce: 21.768250, dice: 93.540471, loss: 21.768250
IoU in current test batch is 0.8197109811297082
Epoch  6339: reducing learning rate of group 0 to 9.4835e-04.
training: bce: 0.058342, dice: 0.251212, loss: 0.058342
training IoU in current batch 6400 is 0.8667196345217996
training IoU uptillnow 6400 is 0.002398870748225212
testing: bce: 21.967437, dice: 94.588651, loss: 21.967437
IoU in current test batch is 0.8576308839118438
Epoch  6440: reducing learning rate of group 0 to 9.4741e-04.
training: bce: 0.057866, dice: 0.249569, loss: 0.057866
training IoU in current batch 6500 is 0.921316570137025
training IoU uptillnow 6500 is 0.002409210457278663
testing: bce: 22.128470, dice: 95.438250, loss: 22.128470
IoU in current test batch is 0.8413118726292002
Epoch  6541: reducing learning rate of group 0 to 9.4646e-04.
training: bce: 0.057230, dice: 0.247564, loss: 0.057230
training IoU in current batch 6600 is 0.8770606060606061
training IoU uptillnow 6600 is 0.002417002078188475
testing: bce: 22.221978, dice: 96.127548, loss: 22.221978
IoU in current test batch is 0.8619154806972853
Epoch  6642: reducing learning rate of group 0 to 9.4551e-04.
training: bce: 0.056581, dice: 0.245340, loss: 0.056581
training IoU in current batch 6700 is 0.8848875306296633
training IoU uptillnow 6700 is 0.0024249504892302656
testing: bce: 22.302746, dice: 96.707398, loss: 22.302746
IoU in current test batch is 0.8751617673864251
training: bce: 0.056046, dice: 0.243913, loss: 0.056046
training IoU in current batch 6800 is 0.8062043898453128
training IoU uptillnow 6800 is 0.002428808708270418
testing: bce: 22.421849, dice: 97.579478, loss: 22.421849
IoU in current test batch is 0.8398380466140133
Epoch  6807: reducing learning rate of group 0 to 9.4457e-04.
training: bce: 0.055448, dice: 0.241932, loss: 0.055448
training IoU in current batch 6900 is 0.9063124755842544
training IoU uptillnow 6900 is 0.002437390549699348
testing: bce: 22.508565, dice: 98.210215, loss: 22.508565
IoU in current test batch is 0.8413640654916383
Epoch  6908: reducing learning rate of group 0 to 9.4362e-04.
training: bce: 0.055444, dice: 0.241471, loss: 0.055444
training IoU in current batch 7000 is 0.8839263803680981
training IoU uptillnow 7000 is 0.0024446613784123074
testing: bce: 22.833132, dice: 99.443315, loss: 22.833132
IoU in current test batch is 0.7944098878025095
Epoch  7009: reducing learning rate of group 0 to 9.4268e-04.
training: bce: 0.054936, dice: 0.239849, loss: 0.054936
training IoU in current batch 7100 is 0.48042089604480226
training IoU uptillnow 7100 is 0.0024327861722216824
testing: bce: 22.946955, dice: 100.186362, loss: 22.946955
IoU in current test batch is 0.6019839159052482
Epoch  7110: reducing learning rate of group 0 to 9.4174e-04.
training: bce: 0.054493, dice: 0.238585, loss: 0.054493
training IoU in current batch 7200 is 0.8543630892678034
training IoU uptillnow 7200 is 0.0024385505215065643
testing: bce: 23.082576, dice: 101.061917, loss: 23.082576
IoU in current test batch is 0.8367976954675461
Epoch  7211: reducing learning rate of group 0 to 9.4079e-04.
training: bce: 0.054060, dice: 0.237277, loss: 0.054060
training IoU in current batch 7300 is 0.8654938202872732
training IoU uptillnow 7300 is 0.0024446651479885667
testing: bce: 23.217368, dice: 101.903699, loss: 23.217368
IoU in current test batch is 0.7427124321410967
Epoch  7312: reducing learning rate of group 0 to 9.3985e-04.
training: bce: 0.053597, dice: 0.235973, loss: 0.053597
training IoU in current batch 7400 is 0.7960971821072305
training IoU uptillnow 7400 is 0.0024474889843039594
testing: bce: 23.333556, dice: 102.731718, loss: 23.333556
IoU in current test batch is 0.8507235597060223
Epoch  7413: reducing learning rate of group 0 to 9.3891e-04.
training: bce: 0.053070, dice: 0.234202, loss: 0.053070
training IoU in current batch 7500 is 0.9197359609681431
training IoU uptillnow 7500 is 0.0024557318526182708
testing: bce: 23.416210, dice: 103.338046, loss: 23.416210
IoU in current test batch is 0.8259617799116455
Epoch  7514: reducing learning rate of group 0 to 9.3797e-04.
training: bce: 0.052554, dice: 0.232463, loss: 0.052554
training IoU in current batch 7600 is 0.8663772310660878
training IoU uptillnow 7600 is 0.0024614178446053167
testing: bce: 23.497812, dice: 103.938299, loss: 23.497812
IoU in current test batch is 0.802481365625581
Epoch  7615: reducing learning rate of group 0 to 9.3704e-04.
training: bce: 0.052094, dice: 0.230995, loss: 0.052094
training IoU in current batch 7700 is 0.9004931906069608
training IoU uptillnow 7700 is 0.0024684328572541227
testing: bce: 23.598405, dice: 104.640686, loss: 23.598405
IoU in current test batch is 0.8427938126258843
Epoch  7716: reducing learning rate of group 0 to 9.3610e-04.
training: bce: 0.051666, dice: 0.229563, loss: 0.051666
training IoU in current batch 7800 is 0.9004240480444282
training IoU uptillnow 7800 is 0.0024752650664097093
testing: bce: 23.708686, dice: 105.342437, loss: 23.708686
IoU in current test batch is 0.8499007039355007
Epoch  7817: reducing learning rate of group 0 to 9.3516e-04.
training: bce: 0.051197, dice: 0.228075, loss: 0.051197
training IoU in current batch 7900 is 0.8860735835958776
training IoU uptillnow 7900 is 0.002481318901944155
testing: bce: 23.794659, dice: 106.001068, loss: 23.794659
IoU in current test batch is 0.8710967856667517
Epoch  7918: reducing learning rate of group 0 to 9.3423e-04.
training: bce: 0.050725, dice: 0.226464, loss: 0.050725
training IoU in current batch 8000 is 0.8764022568868237
training IoU uptillnow 8000 is 0.0024868184889250983
testing: bce: 23.873623, dice: 106.584746, loss: 23.873623
IoU in current test batch is 0.827242611431465
Epoch  8019: reducing learning rate of group 0 to 9.3329e-04.
training: bce: 0.050414, dice: 0.225476, loss: 0.050414
training IoU in current batch 8100 is 0.612105282779704
training IoU uptillnow 8100 is 0.002481307224311765
testing: bce: 24.023743, dice: 107.446175, loss: 24.023743
IoU in current test batch is 0.7245272670946661
Epoch  8120: reducing learning rate of group 0 to 9.3236e-04.
training: bce: 0.050039, dice: 0.224428, loss: 0.050039
training IoU in current batch 8200 is 0.8539160959889509
training IoU uptillnow 8200 is 0.0024857588736510905
testing: bce: 24.139492, dice: 108.266628, loss: 24.139492
IoU in current test batch is 0.8934490117310062
Epoch  8221: reducing learning rate of group 0 to 9.3143e-04.
training: bce: 0.049594, dice: 0.222921, loss: 0.049594
training IoU in current batch 8300 is 0.9126108092650843
training IoU uptillnow 8300 is 0.0024924602006867794
testing: bce: 24.216655, dice: 108.850930, loss: 24.216655
IoU in current test batch is 0.8539458777666984
Epoch  8322: reducing learning rate of group 0 to 9.3050e-04.
training: bce: 0.049184, dice: 0.221598, loss: 0.049184
training IoU in current batch 8400 is 0.9026580935967122
training IoU uptillnow 8400 is 0.002498607089286973
testing: bce: 24.305387, dice: 109.508763, loss: 24.305387
IoU in current test batch is 0.8567492179375122
Epoch  8423: reducing learning rate of group 0 to 9.2957e-04.
training: bce: 0.048777, dice: 0.220294, loss: 0.048777
training IoU in current batch 8500 is 0.8913588291313719
training IoU uptillnow 8500 is 0.002504166305941691
testing: bce: 24.391534, dice: 110.160017, loss: 24.391534
IoU in current test batch is 0.8434232827874428
Epoch  8594: reducing learning rate of group 0 to 9.2864e-04.
training: bce: 0.048382, dice: 0.218968, loss: 0.048382
training IoU in current batch 8600 is 0.9396286231884058
training IoU uptillnow 8600 is 0.0025114669582459153
testing: bce: 24.478619, dice: 110.784920, loss: 24.478619
IoU in current test batch is 0.8802475041996923
Epoch  8695: reducing learning rate of group 0 to 9.2771e-04.
training: bce: 0.047980, dice: 0.217625, loss: 0.047980
training IoU in current batch 8700 is 0.791810321964674
training IoU uptillnow 8700 is 0.0025129369132124288
testing: bce: 24.557080, dice: 111.385726, loss: 24.557080
IoU in current test batch is 0.8523842188290369
Epoch  8796: reducing learning rate of group 0 to 9.2678e-04.
training: bce: 0.047628, dice: 0.216575, loss: 0.047628
training IoU in current batch 8800 is 0.925840189652768
training IoU uptillnow 8800 is 0.0025194497759813963
testing: bce: 24.657172, dice: 112.122203, loss: 24.657172
IoU in current test batch is 0.9017041544139415
Epoch  8897: reducing learning rate of group 0 to 9.2585e-04.
training: bce: 0.047345, dice: 0.215600, loss: 0.047345
training IoU in current batch 8900 is 0.41434311956700015
training IoU uptillnow 8900 is 0.002506661257342014
testing: bce: 24.789386, dice: 112.885705, loss: 24.789386
IoU in current test batch is 0.8521720154296668
Epoch  8998: reducing learning rate of group 0 to 9.2493e-04.
training: bce: 0.046990, dice: 0.214478, loss: 0.046990
training IoU in current batch 9000 is 0.9260040951330918
training IoU uptillnow 9000 is 0.002513105197568303
testing: bce: 24.879770, dice: 113.559776, loss: 24.879770
IoU in current test batch is 0.8440653031075909
Epoch  9099: reducing learning rate of group 0 to 9.2400e-04.
training: bce: 0.046655, dice: 0.213386, loss: 0.046655
training IoU in current batch 9100 is 0.85163810045026
training IoU uptillnow 9100 is 0.002516683798497863
testing: bce: 24.976845, dice: 114.237006, loss: 24.976845
IoU in current test batch is 0.8558519011596429
Epoch  9200: reducing learning rate of group 0 to 9.2308e-04.
training: bce: 0.046334, dice: 0.212360, loss: 0.046334
training IoU in current batch 9200 is 0.8067411840348188
training IoU uptillnow 9200 is 0.002518558089136035
testing: bce: 25.077740, dice: 114.936718, loss: 25.077740
IoU in current test batch is 0.8607126923479784
training: bce: 0.046088, dice: 0.211531, loss: 0.046088
training IoU in current batch 9300 is 0.9386677617106151
training IoU uptillnow 9300 is 0.002525120119561789
testing: bce: 25.215578, dice: 115.732193, loss: 25.215578
IoU in current test batch is 0.8795892488461137
Epoch  9301: reducing learning rate of group 0 to 9.2216e-04.
training: bce: 0.045804, dice: 0.210540, loss: 0.045804
training IoU in current batch 9400 is 0.7378414256406131
training IoU uptillnow 9400 is 0.002524421803417126
testing: bce: 25.329740, dice: 116.428405, loss: 25.329740
IoU in current test batch is 0.7447954311185551
Epoch  9466: reducing learning rate of group 0 to 9.2123e-04.
training: bce: 0.045499, dice: 0.209527, loss: 0.045499
training IoU in current batch 9500 is 0.6746107490524363
training IoU uptillnow 9500 is 0.0025215198004008574
testing: bce: 25.428574, dice: 117.100670, loss: 25.428574
IoU in current test batch is 0.8773852951496133
Epoch  9567: reducing learning rate of group 0 to 9.2031e-04.
training: bce: 0.045178, dice: 0.208405, loss: 0.045178
training IoU in current batch 9600 is 0.9289250353606789
training IoU uptillnow 9600 is 0.0025275076869140825
testing: bce: 25.515040, dice: 117.699989, loss: 25.515040
IoU in current test batch is 0.8620142955824458
Epoch  9668: reducing learning rate of group 0 to 9.1939e-04.
training: bce: 0.044851, dice: 0.207316, loss: 0.044851
training IoU in current batch 9700 is 0.9208247892458419
training IoU uptillnow 9700 is 0.0025330937942972254
testing: bce: 25.594367, dice: 118.304307, loss: 25.594367
IoU in current test batch is 0.8860556998862955
Epoch  9769: reducing learning rate of group 0 to 9.1847e-04.
training: bce: 0.044532, dice: 0.206202, loss: 0.044532
training IoU in current batch 9800 is 0.9122692452593244
training IoU uptillnow 9800 is 0.002538274935914413
testing: bce: 25.674146, dice: 118.881338, loss: 25.674146
IoU in current test batch is 0.8648378247257331
Epoch  9870: reducing learning rate of group 0 to 9.1755e-04.
training: bce: 0.044232, dice: 0.205186, loss: 0.044232
training IoU in current batch 9900 is 0.8798553442271632
training IoU uptillnow 9900 is 0.0025422601516654425
testing: bce: 25.761490, dice: 119.502836, loss: 25.761490
IoU in current test batch is 0.8677555807841951
training: bce: 0.043975, dice: 0.204388, loss: 0.043975
training IoU in current batch 10000 is 0.8800196337086235
training IoU uptillnow 10000 is 0.0025461711468395586
testing: bce: 25.870206, dice: 120.240160, loss: 25.870206
IoU in current test batch is 0.8288671583973518
Epoch 10049: reducing learning rate of group 0 to 9.1664e-04.
training: bce: 0.043697, dice: 0.203490, loss: 0.043697
training IoU in current batch 10100 is 0.880992977487501
training IoU uptillnow 10100 is 0.0025500368246086117
testing: bce: 25.963681, dice: 120.908958, loss: 25.963681
IoU in current test batch is 0.8727475173966116
Epoch 10150: reducing learning rate of group 0 to 9.1572e-04.
training: bce: 0.043415, dice: 0.202525, loss: 0.043415
training IoU in current batch 10200 is 0.8670560941254454
training IoU uptillnow 10200 is 0.0025533713031480644
testing: bce: 26.051553, dice: 121.527107, loss: 26.051553
IoU in current test batch is 0.8332194293830024
Epoch 10251: reducing learning rate of group 0 to 9.1480e-04.
training: bce: 0.043141, dice: 0.201618, loss: 0.043141
training IoU in current batch 10300 is 0.852961862195703
training IoU uptillnow 10300 is 0.0025561849610858463
testing: bce: 26.140987, dice: 122.168533, loss: 26.140987
IoU in current test batch is 0.8734348667561388
Epoch 10352: reducing learning rate of group 0 to 9.1389e-04.
training: bce: 0.042881, dice: 0.200761, loss: 0.042881
training IoU in current batch 10400 is 0.9055966548729495
training IoU uptillnow 10400 is 0.002560631365808059
testing: bce: 26.235619, dice: 122.830008, loss: 26.235619
IoU in current test batch is 0.8239264179541086
Epoch 10453: reducing learning rate of group 0 to 9.1298e-04.
training: bce: 0.042637, dice: 0.199925, loss: 0.042637
training IoU in current batch 10500 is 0.7818718905472637
training IoU uptillnow 10500 is 0.002561065688913949
testing: bce: 26.337265, dice: 123.494618, loss: 26.337265
IoU in current test batch is 0.8722888492997977
Epoch 10554: reducing learning rate of group 0 to 9.1206e-04.
training: bce: 0.042388, dice: 0.199074, loss: 0.042388
training IoU in current batch 10600 is 0.9078808907596485
training IoU uptillnow 10600 is 0.0025654539914038224
testing: bce: 26.432621, dice: 124.140511, loss: 26.432621
IoU in current test batch is 0.8535528391578482
Epoch 10655: reducing learning rate of group 0 to 9.1115e-04.
training: bce: 0.042192, dice: 0.198437, loss: 0.042192
training IoU in current batch 10700 is 0.8892671980190359
training IoU uptillnow 10700 is 0.0025691804655837403
testing: bce: 26.558523, dice: 124.910347, loss: 26.558523
IoU in current test batch is 0.8261641176167684
Epoch 10756: reducing learning rate of group 0 to 9.1024e-04.
training: bce: 0.041980, dice: 0.197761, loss: 0.041980
training IoU in current batch 10800 is 0.8698060034080483
training IoU uptillnow 10800 is 0.0025722373388279746
testing: bce: 26.672380, dice: 125.648120, loss: 26.672380
IoU in current test batch is 0.8752768850633768
Epoch 10857: reducing learning rate of group 0 to 9.0933e-04.
training: bce: 0.041742, dice: 0.197023, loss: 0.041742
training IoU in current batch 10900 is 0.9303835782290463
training IoU uptillnow 10900 is 0.0025770904830832615
testing: bce: 26.766564, dice: 126.338308, loss: 26.766564
IoU in current test batch is 0.8174545657478293
Epoch 10958: reducing learning rate of group 0 to 9.0842e-04.
training: bce: 0.041477, dice: 0.196121, loss: 0.041477
training IoU in current batch 11000 is 0.8617017462081824
training IoU uptillnow 11000 is 0.0025797743179250395
testing: bce: 26.840538, dice: 126.913570, loss: 26.840538
IoU in current test batch is 0.8559966254314939
Epoch 11059: reducing learning rate of group 0 to 9.0751e-04.
training: bce: 0.041226, dice: 0.195220, loss: 0.041226
training IoU in current batch 11100 is 0.8408788872153805
training IoU uptillnow 11100 is 0.002581784544986802
testing: bce: 26.920794, dice: 127.478962, loss: 26.920794
IoU in current test batch is 0.8763242007434113
Epoch 11160: reducing learning rate of group 0 to 9.0660e-04.
training: bce: 0.040986, dice: 0.194446, loss: 0.040986
training IoU in current batch 11200 is 0.889125032696835
training IoU uptillnow 11200 is 0.00258519464733483
testing: bce: 27.004803, dice: 128.116726, loss: 27.004803
IoU in current test batch is 0.8650028041165965
Epoch 11261: reducing learning rate of group 0 to 9.0570e-04.
training: bce: 0.040781, dice: 0.193730, loss: 0.040781
training IoU in current batch 11300 is 0.9454212911003035
training IoU uptillnow 11300 is 0.002590204908872153
testing: bce: 27.109589, dice: 128.784953, loss: 27.109589
IoU in current test batch is 0.8698321714504322
Epoch 11362: reducing learning rate of group 0 to 9.0479e-04.
training: bce: 0.040545, dice: 0.192939, loss: 0.040545
training IoU in current batch 11400 is 0.8815506101938263
training IoU uptillnow 11400 is 0.0025932598788318694
testing: bce: 27.191132, dice: 129.394191, loss: 27.191132
IoU in current test batch is 0.8842216561082402
Epoch 11463: reducing learning rate of group 0 to 9.0389e-04.
training: bce: 0.040308, dice: 0.192103, loss: 0.040308
training IoU in current batch 11500 is 0.9336131724258552
training IoU uptillnow 11500 is 0.0025977706520624953
testing: bce: 27.269284, dice: 129.963666, loss: 27.269284
IoU in current test batch is 0.8387297558026605
Epoch 11564: reducing learning rate of group 0 to 9.0298e-04.
training: bce: 0.040109, dice: 0.191444, loss: 0.040109
training IoU in current batch 11600 is 0.9289739192409631
training IoU uptillnow 11600 is 0.0026020703596630534
testing: bce: 27.370591, dice: 130.643607, loss: 27.370591
IoU in current test batch is 0.8627822665983803
Epoch 11665: reducing learning rate of group 0 to 9.0208e-04.
training: bce: 0.040083, dice: 0.191411, loss: 0.040083
training IoU in current batch 11700 is 0.8630081535881571
training IoU uptillnow 11700 is 0.0026044173683429164
testing: bce: 27.589156, dice: 131.746960, loss: 27.589156
IoU in current test batch is 0.8814206911688565
Epoch 11766: reducing learning rate of group 0 to 9.0118e-04.
training: bce: 0.039856, dice: 0.190646, loss: 0.039856
training IoU in current batch 11800 is 0.9191881918819188
training IoU uptillnow 11800 is 0.002608311472836294
testing: bce: 27.667429, dice: 132.342190, loss: 27.667429
IoU in current test batch is 0.8790158218867351
Epoch 11867: reducing learning rate of group 0 to 9.0028e-04.
training: bce: 0.039658, dice: 0.190066, loss: 0.039658
training IoU in current batch 11900 is 0.8710758819474904
training IoU uptillnow 11900 is 0.0026107925651841805
testing: bce: 27.762998, dice: 133.057520, loss: 27.762998
IoU in current test batch is 0.8478523356764757
Epoch 11968: reducing learning rate of group 0 to 8.9938e-04.
training: bce: 0.039448, dice: 0.189328, loss: 0.039448
training IoU in current batch 12000 is 0.7303671596124426
training IoU uptillnow 12000 is 0.002609324059505687
testing: bce: 27.847771, dice: 133.654230, loss: 27.847771
IoU in current test batch is 0.8758125133739755
Epoch 12069: reducing learning rate of group 0 to 8.9848e-04.
training: bce: 0.039230, dice: 0.188560, loss: 0.039230
training IoU in current batch 12100 is 0.8586450570388057
training IoU uptillnow 12100 is 0.0026114133589902225
testing: bce: 27.924615, dice: 134.221519, loss: 27.924615
IoU in current test batch is 0.8841712816461003
Epoch 12170: reducing learning rate of group 0 to 8.9758e-04.
training: bce: 0.039024, dice: 0.187859, loss: 0.039024
training IoU in current batch 12200 is 0.907000180883227
training IoU uptillnow 12200 is 0.002614789480433442
testing: bce: 28.007850, dice: 134.827807, loss: 28.007850
IoU in current test batch is 0.8776738356807057
Epoch 12271: reducing learning rate of group 0 to 8.9668e-04.
training: bce: 0.038974, dice: 0.187660, loss: 0.038974
training IoU in current batch 12300 is 0.9060345082719228
training IoU uptillnow 12300 is 0.002618084542193784
testing: bce: 28.201102, dice: 135.788366, loss: 28.201102
IoU in current test batch is 0.8294414705350744
Epoch 12372: reducing learning rate of group 0 to 8.9578e-04.
training: bce: 0.038778, dice: 0.187015, loss: 0.038778
training IoU in current batch 12400 is 0.9259786139365057
training IoU uptillnow 12400 is 0.0026218625507220847
testing: bce: 28.287586, dice: 136.421595, loss: 28.287586
IoU in current test batch is 0.8473616108620233
Epoch 12473: reducing learning rate of group 0 to 8.9489e-04.
training: bce: 0.038565, dice: 0.186241, loss: 0.038565
training IoU in current batch 12500 is 0.8978242986412504
training IoU uptillnow 12500 is 0.0026248293942659246
testing: bce: 28.359205, dice: 136.952737, loss: 28.359205
IoU in current test batch is 0.8464678836546916
Epoch 12574: reducing learning rate of group 0 to 8.9399e-04.
training: bce: 0.038471, dice: 0.185960, loss: 0.038471
training IoU in current batch 12600 is 0.8493168595149375
training IoU uptillnow 12600 is 0.0026264659850453644
testing: bce: 28.515814, dice: 137.840259, loss: 28.515814
IoU in current test batch is 0.8670247202523906
Epoch 12675: reducing learning rate of group 0 to 8.9310e-04.
training: bce: 0.038303, dice: 0.185402, loss: 0.038303
training IoU in current batch 12700 is 0.8393453355155482
training IoU uptillnow 12700 is 0.0026278151055871045
testing: bce: 28.616809, dice: 138.517368, loss: 28.616809
IoU in current test batch is 0.8887426460894344
Epoch 12776: reducing learning rate of group 0 to 8.9221e-04.
training: bce: 0.038102, dice: 0.184691, loss: 0.038102
training IoU in current batch 12800 is 0.8421527059059193
training IoU uptillnow 12800 is 0.0026292162506598798
testing: bce: 28.690684, dice: 139.072145, loss: 28.690684
IoU in current test batch is 0.865093456638442
Epoch 12877: reducing learning rate of group 0 to 8.9131e-04.
training: bce: 0.037937, dice: 0.184135, loss: 0.037937
training IoU in current batch 12900 is 0.8521626056534258
training IoU uptillnow 12900 is 0.0026308543079281914
testing: bce: 28.789840, dice: 139.736700, loss: 28.789840
IoU in current test batch is 0.8903124452693985
Epoch 12978: reducing learning rate of group 0 to 8.9042e-04.
training: bce: 0.037742, dice: 0.183468, loss: 0.037742
training IoU in current batch 13000 is 0.8973881548742539
training IoU uptillnow 13000 is 0.002633626706525627
testing: bce: 28.863590, dice: 140.309704, loss: 28.863590
IoU in current test batch is 0.8613244415602486
Epoch 13079: reducing learning rate of group 0 to 8.8953e-04.
training: bce: 0.037557, dice: 0.182809, loss: 0.037557
training IoU in current batch 13100 is 0.9002186816619806
training IoU uptillnow 13100 is 0.0026364287997425392
testing: bce: 28.943360, dice: 140.881363, loss: 28.943360
IoU in current test batch is 0.8709999490328237
Epoch 13180: reducing learning rate of group 0 to 8.8864e-04.
training: bce: 0.037431, dice: 0.182374, loss: 0.037431
training IoU in current batch 13200 is 0.8628738621586476
training IoU uptillnow 13200 is 0.0026382454606580226
testing: bce: 29.066295, dice: 141.619131, loss: 29.066295
IoU in current test batch is 0.8818646454454518
training: bce: 0.037290, dice: 0.181884, loss: 0.037290
training IoU in current batch 13300 is 0.7999487423571193
training IoU uptillnow 13300 is 0.0026384578533142065
testing: bce: 29.176224, dice: 142.307983, loss: 29.176224
IoU in current test batch is 0.8740116664440274
Epoch 13359: reducing learning rate of group 0 to 8.8775e-04.
training: bce: 0.037210, dice: 0.181632, loss: 0.037210
training IoU in current batch 13400 is 0.8991288278775079
training IoU uptillnow 13400 is 0.0026411340583706263
testing: bce: 29.332634, dice: 143.179735, loss: 29.332634
IoU in current test batch is 0.8528482127948382
Epoch 13460: reducing learning rate of group 0 to 8.8687e-04.
training: bce: 0.037056, dice: 0.181147, loss: 0.037056
training IoU in current batch 13500 is 0.8592343394698315
training IoU uptillnow 13500 is 0.002642785642746071
testing: bce: 29.429319, dice: 143.862847, loss: 29.429319
IoU in current test batch is 0.873714144061008
Epoch 13561: reducing learning rate of group 0 to 8.8598e-04.
training: bce: 0.036924, dice: 0.180717, loss: 0.036924
training IoU in current batch 13600 is 0.8995585885666315
training IoU uptillnow 13600 is 0.0026454012076737193
testing: bce: 29.541343, dice: 144.584265, loss: 29.541343
IoU in current test batch is 0.8520525174379583
Epoch 13662: reducing learning rate of group 0 to 8.8509e-04.
training: bce: 0.036970, dice: 0.180549, loss: 0.036970
training IoU in current batch 13700 is 0.9064498262927345
training IoU uptillnow 13700 is 0.0026481462497385468
testing: bce: 29.795299, dice: 145.512020, loss: 29.795299
IoU in current test batch is 0.8750780884426501
Epoch 13763: reducing learning rate of group 0 to 8.8421e-04.
training: bce: 0.036796, dice: 0.179957, loss: 0.036796
training IoU in current batch 13800 is 0.7843999889322376
training IoU uptillnow 13800 is 0.0026479036613756427
testing: bce: 29.871806, dice: 146.093128, loss: 29.871806
IoU in current test batch is 0.8414163210166525
Epoch 13864: reducing learning rate of group 0 to 8.8333e-04.
training: bce: 0.036615, dice: 0.179287, loss: 0.036615
training IoU in current batch 13900 is 0.9066876291961452
training IoU uptillnow 13900 is 0.002650596909601992
testing: bce: 29.940016, dice: 146.603975, loss: 29.940016
IoU in current test batch is 0.8591114011015374
Epoch 13965: reducing learning rate of group 0 to 8.8244e-04.
training: bce: 0.036467, dice: 0.178791, loss: 0.036467
training IoU in current batch 14000 is 0.8872746762955231
training IoU uptillnow 14000 is 0.002652789505450263
testing: bce: 30.034040, dice: 147.250402, loss: 30.034040
IoU in current test batch is 0.8765851629123463
Epoch 14066: reducing learning rate of group 0 to 8.8156e-04.
training: bce: 0.036306, dice: 0.178234, loss: 0.036306
training IoU in current batch 14100 is 0.898254576415496
training IoU uptillnow 14100 is 0.0026552105565525587
testing: bce: 30.114851, dice: 147.840254, loss: 30.114851
IoU in current test batch is 0.878064407864833
Epoch 14167: reducing learning rate of group 0 to 8.8068e-04.
training: bce: 0.036169, dice: 0.177792, loss: 0.036169
training IoU in current batch 14200 is 0.8521166922570739
training IoU uptillnow 14200 is 0.0026565145380865185
testing: bce: 30.214175, dice: 148.519431, loss: 30.214175
IoU in current test batch is 0.8805732970247563
Epoch 14268: reducing learning rate of group 0 to 8.7980e-04.
training: bce: 0.036017, dice: 0.177276, loss: 0.036017
training IoU in current batch 14300 is 0.9283523712164985
training IoU uptillnow 14300 is 0.0026595772145844455
testing: bce: 30.298632, dice: 149.130713, loss: 30.298632
IoU in current test batch is 0.8605492031157843
Epoch 14369: reducing learning rate of group 0 to 8.7892e-04.
training: bce: 0.035862, dice: 0.176731, loss: 0.035862
training IoU in current batch 14400 is 0.9249337730442775
training IoU uptillnow 14400 is 0.0026625182281406552
testing: bce: 30.378984, dice: 149.712227, loss: 30.378984
IoU in current test batch is 0.8500296909690772
Epoch 14470: reducing learning rate of group 0 to 8.7804e-04.
training: bce: 0.035736, dice: 0.176279, loss: 0.035736
training IoU in current batch 14500 is 0.7666961671999455
training IoU uptillnow 14500 is 0.002661781283533565
testing: bce: 30.482588, dice: 150.365901, loss: 30.482588
IoU in current test batch is 0.8531527388160434
Epoch 14571: reducing learning rate of group 0 to 8.7716e-04.
training: bce: 0.035581, dice: 0.175726, loss: 0.035581
training IoU in current batch 14600 is 0.911268148528845
training IoU uptillnow 14600 is 0.00266435493747208
testing: bce: 30.559683, dice: 150.928088, loss: 30.559683
IoU in current test batch is 0.8642428157764528
Epoch 14672: reducing learning rate of group 0 to 8.7628e-04.
training: bce: 0.035438, dice: 0.175253, loss: 0.035438
training IoU in current batch 14700 is 0.8341162352134408
training IoU uptillnow 14700 is 0.002665144220604108
testing: bce: 30.645487, dice: 151.552758, loss: 30.645487
IoU in current test batch is 0.8993448762893946
Epoch 14773: reducing learning rate of group 0 to 8.7541e-04.
training: bce: 0.035292, dice: 0.174740, loss: 0.035292
training IoU in current batch 14800 is 0.9315514796826451
training IoU uptillnow 14800 is 0.002668117177690372
testing: bce: 30.726703, dice: 152.137166, loss: 30.726703
IoU in current test batch is 0.888585116789993
Epoch 14874: reducing learning rate of group 0 to 8.7453e-04.
training: bce: 0.035143, dice: 0.174177, loss: 0.035143
training IoU in current batch 14900 is 0.8969812164579607
training IoU uptillnow 14900 is 0.0026702768999271537
testing: bce: 30.803900, dice: 152.671122, loss: 30.803900
IoU in current test batch is 0.8570844211581609
Epoch 14975: reducing learning rate of group 0 to 8.7366e-04.
training: bce: 0.035010, dice: 0.173732, loss: 0.035010
training IoU in current batch 15000 is 0.8367340381093152
training IoU uptillnow 15000 is 0.0026710690908506736
testing: bce: 30.893101, dice: 153.303268, loss: 30.893101
IoU in current test batch is 0.8680659781385903
Epoch 15076: reducing learning rate of group 0 to 8.7278e-04.
training: bce: 0.034876, dice: 0.173301, loss: 0.034876
training IoU in current batch 15100 is 0.9100279900958123
training IoU uptillnow 15100 is 0.0026734686507659247
testing: bce: 30.980286, dice: 153.942566, loss: 30.980286
IoU in current test batch is 0.8698345051716027
Epoch 15177: reducing learning rate of group 0 to 8.7191e-04.
training: bce: 0.034762, dice: 0.172896, loss: 0.034762
training IoU in current batch 15200 is 0.9274195539969872
training IoU uptillnow 15200 is 0.0026762180084565854
testing: bce: 31.083419, dice: 154.599841, loss: 31.083419
IoU in current test batch is 0.8909099209082284
Epoch 15278: reducing learning rate of group 0 to 8.7104e-04.
training: bce: 0.034617, dice: 0.172372, loss: 0.034617
training IoU in current batch 15300 is 0.8851908065915004
training IoU uptillnow 15300 is 0.0026780114730243594
testing: bce: 31.157208, dice: 155.144790, loss: 31.157208
IoU in current test batch is 0.8644039662687875
Epoch 15379: reducing learning rate of group 0 to 8.7017e-04.
training: bce: 0.034481, dice: 0.171892, loss: 0.034481
training IoU in current batch 15400 is 0.916162489196197
training IoU uptillnow 15400 is 0.002680451986568694
testing: bce: 31.237912, dice: 155.723721, loss: 31.237912
IoU in current test batch is 0.8614474435775271
Epoch 15480: reducing learning rate of group 0 to 8.6930e-04.
training: bce: 0.034337, dice: 0.171398, loss: 0.034337
training IoU in current batch 15500 is 0.9436098410624864
training IoU uptillnow 15500 is 0.002683451239199532
testing: bce: 31.309421, dice: 156.284786, loss: 31.309421
IoU in current test batch is 0.8602408615074777
Epoch 15581: reducing learning rate of group 0 to 8.6843e-04.
training: bce: 0.034204, dice: 0.170966, loss: 0.034204
training IoU in current batch 15600 is 0.8991274092892252
training IoU uptillnow 15600 is 0.002685461623951137
testing: bce: 31.389243, dice: 156.896549, loss: 31.389243
IoU in current test batch is 0.8449627904378485
Epoch 15682: reducing learning rate of group 0 to 8.6756e-04.
training: bce: 0.034066, dice: 0.170462, loss: 0.034066
training IoU in current batch 15700 is 0.9197008258453228
training IoU uptillnow 15700 is 0.002687883175416224
testing: bce: 31.462935, dice: 157.436926, loss: 31.462935
IoU in current test batch is 0.8496629420335888
Epoch 15783: reducing learning rate of group 0 to 8.6669e-04.
training: bce: 0.033931, dice: 0.169967, loss: 0.033931
training IoU in current batch 15800 is 0.9002205725399046
training IoU uptillnow 15800 is 0.0026898631264723814
testing: bce: 31.538299, dice: 157.979437, loss: 31.538299
IoU in current test batch is 0.9003601722130952
Epoch 15884: reducing learning rate of group 0 to 8.6583e-04.
training: bce: 0.033821, dice: 0.169586, loss: 0.033821
training IoU in current batch 15900 is 0.907972177635099
training IoU uptillnow 15900 is 0.0026919806712744562
testing: bce: 31.634943, dice: 158.622863, loss: 31.634943
IoU in current test batch is 0.8272542838636108
Epoch 15985: reducing learning rate of group 0 to 8.6496e-04.
training: bce: 0.033685, dice: 0.169075, loss: 0.033685
training IoU in current batch 16000 is 0.8750384217732696
training IoU uptillnow 16000 is 0.0026933856713867604
testing: bce: 31.705694, dice: 159.139114, loss: 31.705694
IoU in current test batch is 0.8500161759009429
Epoch 16086: reducing learning rate of group 0 to 8.6409e-04.
training: bce: 0.033555, dice: 0.168624, loss: 0.033555
training IoU in current batch 16100 is 0.9071332436069987
training IoU uptillnow 16100 is 0.0026954376669603474
testing: bce: 31.780157, dice: 159.706328, loss: 31.780157
IoU in current test batch is 0.8727543912031288
Epoch 16187: reducing learning rate of group 0 to 8.6323e-04.
training: bce: 0.033434, dice: 0.168180, loss: 0.033434
training IoU in current batch 16200 is 0.9189720006706427
training IoU uptillnow 16200 is 0.0026977079116074377
testing: bce: 31.862229, dice: 160.276009, loss: 31.862229
IoU in current test batch is 0.866956507683291
Epoch 16288: reducing learning rate of group 0 to 8.6237e-04.
training: bce: 0.033322, dice: 0.167764, loss: 0.033322
training IoU in current batch 16300 is 0.864441456713626
training IoU uptillnow 16300 is 0.0026988352265621724
testing: bce: 31.952181, dice: 160.865749, loss: 31.952181
IoU in current test batch is 0.8487213901622409
Epoch 16389: reducing learning rate of group 0 to 8.6150e-04.
training: bce: 0.033193, dice: 0.167289, loss: 0.033193
training IoU in current batch 16400 is 0.8574747614437838
training IoU uptillnow 16400 is 0.002699807203748018
testing: bce: 32.023552, dice: 161.394452, loss: 32.023552
IoU in current test batch is 0.8785033976320741
Epoch 16490: reducing learning rate of group 0 to 8.6064e-04.
training: bce: 0.033079, dice: 0.166870, loss: 0.033079
training IoU in current batch 16500 is 0.916762411561952
training IoU uptillnow 16500 is 0.002701965057826307
testing: bce: 32.107991, dice: 161.971516, loss: 32.107991
IoU in current test batch is 0.8944593209873815
Epoch 16591: reducing learning rate of group 0 to 8.5978e-04.
training: bce: 0.032971, dice: 0.166525, loss: 0.032971
training IoU in current batch 16600 is 0.8968972427613848
training IoU uptillnow 16600 is 0.002703698040285466
testing: bce: 32.197477, dice: 162.616925, loss: 32.197477
IoU in current test batch is 0.8653432255875377
Epoch 16692: reducing learning rate of group 0 to 8.5892e-04.
training: bce: 0.032896, dice: 0.166271, loss: 0.032896
training IoU in current batch 16700 is 0.9242101526446574
training IoU uptillnow 16700 is 0.002705955404925488
testing: bce: 32.317277, dice: 163.346212, loss: 32.317277
IoU in current test batch is 0.8392257985699295
Epoch 16793: reducing learning rate of group 0 to 8.5806e-04.
training: bce: 0.032775, dice: 0.165842, loss: 0.032775
training IoU in current batch 16800 is 0.9143404266796127
training IoU uptillnow 16800 is 0.002707990081536046
testing: bce: 32.391643, dice: 163.901103, loss: 32.391643
IoU in current test batch is 0.8287723500053119
Epoch 16894: reducing learning rate of group 0 to 8.5721e-04.
training: bce: 0.032650, dice: 0.165379, loss: 0.032650
training IoU in current batch 16900 is 0.8722155989207504
training IoU uptillnow 16900 is 0.0027091698652660016
testing: bce: 32.459452, dice: 164.415501, loss: 32.459452
IoU in current test batch is 0.8803534899130403
Epoch 16995: reducing learning rate of group 0 to 8.5635e-04.
training: bce: 0.032531, dice: 0.164944, loss: 0.032531
training IoU in current batch 17000 is 0.9339247157982362
training IoU uptillnow 17000 is 0.0027115456815163877
testing: bce: 32.532890, dice: 164.953412, loss: 32.532890
IoU in current test batch is 0.8817751056186944
training: bce: 0.032420, dice: 0.164505, loss: 0.032420
training IoU in current batch 17100 is 0.9050374302842316
training IoU uptillnow 17100 is 0.002713330640014513
testing: bce: 32.613057, dice: 165.482286, loss: 32.613057
IoU in current test batch is 0.8687058366228199
Epoch 17183: reducing learning rate of group 0 to 8.5549e-04.
training: bce: 0.032327, dice: 0.164195, loss: 0.032327
training IoU in current batch 17200 is 0.9160086333965045
training IoU uptillnow 17200 is 0.0027153074522423323
testing: bce: 32.709000, dice: 166.136349, loss: 32.709000
IoU in current test batch is 0.856321670923336
Epoch 17284: reducing learning rate of group 0 to 8.5464e-04.
training: bce: 0.032216, dice: 0.163794, loss: 0.032216
training IoU in current batch 17300 is 0.9021071224220713
training IoU uptillnow 17300 is 0.002716993576103176
testing: bce: 32.786021, dice: 166.694186, loss: 32.786021
IoU in current test batch is 0.8645095355525398
Epoch 17385: reducing learning rate of group 0 to 8.5378e-04.
training: bce: 0.032102, dice: 0.163376, loss: 0.032102
training IoU in current batch 17400 is 0.9306029280579314
training IoU uptillnow 17400 is 0.002719206185631881
testing: bce: 32.859431, dice: 167.229331, loss: 32.859431
IoU in current test batch is 0.8962594859549577
Epoch 17486: reducing learning rate of group 0 to 8.5293e-04.
training: bce: 0.032000, dice: 0.163021, loss: 0.032000
training IoU in current batch 17500 is 0.9095742294405312
training IoU uptillnow 17500 is 0.00272099298588617
testing: bce: 32.943084, dice: 167.825668, loss: 32.943084
IoU in current test batch is 0.8500540761333313
Epoch 17587: reducing learning rate of group 0 to 8.5208e-04.
training: bce: 0.031891, dice: 0.162613, loss: 0.031891
training IoU in current batch 17600 is 0.9119508738781295
training IoU uptillnow 17600 is 0.002722804492393609
testing: bce: 33.018106, dice: 168.361510, loss: 33.018106
IoU in current test batch is 0.8715092282770582
Epoch 17688: reducing learning rate of group 0 to 8.5122e-04.
training: bce: 0.031780, dice: 0.162208, loss: 0.031780
training IoU in current batch 17700 is 0.8938280675973549
training IoU uptillnow 17700 is 0.0027242542545516655
testing: bce: 33.090919, dice: 168.896333, loss: 33.090919
IoU in current test batch is 0.8802677919061649
Epoch 17789: reducing learning rate of group 0 to 8.5037e-04.
training: bce: 0.031705, dice: 0.161987, loss: 0.031705
training IoU in current batch 17800 is 0.9213653987481311
training IoU uptillnow 17800 is 0.0027262033795518083
testing: bce: 33.198735, dice: 169.619690, loss: 33.198735
IoU in current test batch is 0.8769989603210308
Epoch 17890: reducing learning rate of group 0 to 8.4952e-04.
training: bce: 0.031614, dice: 0.161691, loss: 0.031614
training IoU in current batch 17900 is 0.8701681454364957
training IoU uptillnow 17900 is 0.002727177387178402
testing: bce: 33.289374, dice: 170.260338, loss: 33.289374
IoU in current test batch is 0.8933296631030236
Epoch 17991: reducing learning rate of group 0 to 8.4867e-04.
training: bce: 0.031515, dice: 0.161363, loss: 0.031515
training IoU in current batch 18000 is 0.9106786711493532
training IoU uptillnow 18000 is 0.0027288907263446674
testing: bce: 33.371157, dice: 170.864233, loss: 33.371157
IoU in current test batch is 0.8767616921667899
Epoch 18092: reducing learning rate of group 0 to 8.4782e-04.
training: bce: 0.031410, dice: 0.160971, loss: 0.031410
training IoU in current batch 18100 is 0.919941857214737
training IoU uptillnow 18100 is 0.0027307557179530747
testing: bce: 33.444505, dice: 171.396198, loss: 33.444505
IoU in current test batch is 0.8386142583205208
Epoch 18193: reducing learning rate of group 0 to 8.4698e-04.
training: bce: 0.031366, dice: 0.160825, loss: 0.031366
training IoU in current batch 18200 is 0.6407313357034028
training IoU uptillnow 18200 is 0.002727486751418589
testing: bce: 33.582060, dice: 172.187078, loss: 33.582060
IoU in current test batch is 0.8405757520562765
Epoch 18294: reducing learning rate of group 0 to 8.4613e-04.
training: bce: 0.031272, dice: 0.160489, loss: 0.031272
training IoU in current batch 18300 is 0.8832611185816027
training IoU uptillnow 18300 is 0.002728670932486218
testing: bce: 33.665002, dice: 172.770776, loss: 33.665002
IoU in current test batch is 0.8975951125928793
Epoch 18395: reducing learning rate of group 0 to 8.4528e-04.
training: bce: 0.031245, dice: 0.160347, loss: 0.031245
training IoU in current batch 18400 is 0.8548638255950678
training IoU uptillnow 18400 is 0.0027293278269638586
testing: bce: 33.820178, dice: 173.561737, loss: 33.820178
IoU in current test batch is 0.8744333245269118
Epoch 18496: reducing learning rate of group 0 to 8.4444e-04.
training: bce: 0.031240, dice: 0.160264, loss: 0.031240
training IoU in current batch 18500 is 0.7997233415528959
training IoU uptillnow 18500 is 0.002728984151729434
testing: bce: 33.998473, dice: 174.414453, loss: 33.998473
IoU in current test batch is 0.7715285103340921
Epoch 18597: reducing learning rate of group 0 to 8.4359e-04.
training: bce: 0.031162, dice: 0.160018, loss: 0.031162
training IoU in current batch 18600 is 0.9071223287710213
training IoU uptillnow 18600 is 0.00273056878128792
testing: bce: 34.096507, dice: 175.088418, loss: 34.096507
IoU in current test batch is 0.8964285794884708
Epoch 18698: reducing learning rate of group 0 to 8.4275e-04.
training: bce: 0.031083, dice: 0.159760, loss: 0.031083
training IoU in current batch 18700 is 0.8817177532285669
training IoU uptillnow 18700 is 0.002731683643574111
testing: bce: 34.193395, dice: 175.745946, loss: 34.193395
IoU in current test batch is 0.852398832725709
Epoch 18799: reducing learning rate of group 0 to 8.4191e-04.
training: bce: 0.030991, dice: 0.159413, loss: 0.030991
training IoU in current batch 18800 is 0.8341814401400744
training IoU uptillnow 18800 is 0.0027319438486530577
testing: bce: 34.274324, dice: 176.301935, loss: 34.274324
IoU in current test batch is 0.8744385005504899
Epoch 18900: reducing learning rate of group 0 to 8.4106e-04.
training: bce: 0.030898, dice: 0.159089, loss: 0.030898
training IoU in current batch 18900 is 0.8878483547001647
training IoU uptillnow 18900 is 0.002733147756737361
testing: bce: 34.352937, dice: 176.879085, loss: 34.352937
IoU in current test batch is 0.8524307933248878
training: bce: 0.030831, dice: 0.158844, loss: 0.030831
training IoU in current batch 19000 is 0.8801070682707298
training IoU uptillnow 19000 is 0.0027342031878769416
testing: bce: 34.459533, dice: 177.541204, loss: 34.459533
IoU in current test batch is 0.8507199227726027
Epoch 19001: reducing learning rate of group 0 to 8.4022e-04.
training: bce: 0.030736, dice: 0.158519, loss: 0.030736
training IoU in current batch 19100 is 0.9145777969307382
training IoU uptillnow 19100 is 0.002735849119862486
testing: bce: 34.534444, dice: 178.110337, loss: 34.534444
IoU in current test batch is 0.8686281353135022
Epoch 19121: reducing learning rate of group 0 to 8.3938e-04.
training: bce: 0.030651, dice: 0.158206, loss: 0.030651
training IoU in current batch 19200 is 0.9161901005718793
training IoU uptillnow 19200 is 0.00273750589754096
testing: bce: 34.619764, dice: 178.689413, loss: 34.619764
IoU in current test batch is 0.8784203278410232
Epoch 19222: reducing learning rate of group 0 to 8.3854e-04.
training: bce: 0.030570, dice: 0.157905, loss: 0.030570
training IoU in current batch 19300 is 0.916059921332329
training IoU uptillnow 19300 is 0.002739143259198733
testing: bce: 34.707475, dice: 179.278035, loss: 34.707475
IoU in current test batch is 0.8409599983828977
Epoch 19323: reducing learning rate of group 0 to 8.3771e-04.
training: bce: 0.030477, dice: 0.157563, loss: 0.030477
training IoU in current batch 19400 is 0.9291282429607771
training IoU uptillnow 19400 is 0.002740988271744498
testing: bce: 34.781805, dice: 179.816892, loss: 34.781805
IoU in current test batch is 0.8384358782143135
Epoch 19424: reducing learning rate of group 0 to 8.3687e-04.
training: bce: 0.030389, dice: 0.157257, loss: 0.030389
training IoU in current batch 19500 is 0.9092259282567653
training IoU uptillnow 19500 is 0.002742474168993074
testing: bce: 34.860223, dice: 180.392768, loss: 34.860223
IoU in current test batch is 0.8838522451401394
Epoch 19525: reducing learning rate of group 0 to 8.3603e-04.
training: bce: 0.030302, dice: 0.156925, loss: 0.030302
training IoU in current batch 19600 is 0.9080793961420185
training IoU uptillnow 19600 is 0.00274392540694767
testing: bce: 34.938176, dice: 180.934217, loss: 34.938176
IoU in current test batch is 0.8841176434068624
Epoch 19626: reducing learning rate of group 0 to 8.3519e-04.
training: bce: 0.030246, dice: 0.156704, loss: 0.030246
training IoU in current batch 19700 is 0.8295438254437022
training IoU uptillnow 19700 is 0.0027440331206569466
testing: bce: 35.051738, dice: 181.602067, loss: 35.051738
IoU in current test batch is 0.8835469409260779
Epoch 19727: reducing learning rate of group 0 to 8.3436e-04.
training: bce: 0.030156, dice: 0.156400, loss: 0.030156
training IoU in current batch 19800 is 0.7879669270237589
training IoU uptillnow 19800 is 0.002743439833968171
testing: bce: 35.124720, dice: 182.169771, loss: 35.124720
IoU in current test batch is 0.8746712965661912
Epoch 19828: reducing learning rate of group 0 to 8.3353e-04.
training: bce: 0.030076, dice: 0.156106, loss: 0.030076
training IoU in current batch 19900 is 0.9120826614107388
training IoU uptillnow 19900 is 0.002744931395719177
testing: bce: 35.208568, dice: 182.744557, loss: 35.208568
IoU in current test batch is 0.8644371310242711
Epoch 19929: reducing learning rate of group 0 to 8.3269e-04.
training: bce: 0.029988, dice: 0.155774, loss: 0.029988
training IoU in current batch 20000 is 0.909851999688966
training IoU uptillnow 20000 is 0.0027463708667618447
testing: bce: 35.282076, dice: 183.272982, loss: 35.282076
IoU in current test batch is 0.8870220307303635
Epoch 20030: reducing learning rate of group 0 to 8.3186e-04.
training: bce: 0.029904, dice: 0.155462, loss: 0.029904
training IoU in current batch 20100 is 0.8264135593220339
training IoU uptillnow 20100 is 0.002746412362198116
testing: bce: 35.358491, dice: 183.820642, loss: 35.358491
IoU in current test batch is 0.868549138244835
Epoch 20131: reducing learning rate of group 0 to 8.3103e-04.
training: bce: 0.029816, dice: 0.155147, loss: 0.029816
training IoU in current batch 20200 is 0.9266154886908439
training IoU uptillnow 20200 is 0.0027481068621408814
testing: bce: 35.430266, dice: 184.359829, loss: 35.430266
IoU in current test batch is 0.8910804464187196
Epoch 20232: reducing learning rate of group 0 to 8.3020e-04.
training: bce: 0.029733, dice: 0.154851, loss: 0.029733
training IoU in current batch 20300 is 0.8291051017563423
training IoU uptillnow 20300 is 0.0027481835914171747
testing: bce: 35.506381, dice: 184.919981, loss: 35.506381
IoU in current test batch is 0.882110504683171
Epoch 20333: reducing learning rate of group 0 to 8.2937e-04.
training: bce: 0.029654, dice: 0.154569, loss: 0.029654
training IoU in current batch 20400 is 0.8554216867469879
training IoU uptillnow 20400 is 0.0027486895569633378
testing: bce: 35.586751, dice: 185.491770, loss: 35.586751
IoU in current test batch is 0.8614934665810409
Epoch 20434: reducing learning rate of group 0 to 8.2854e-04.
training: bce: 0.029572, dice: 0.154250, loss: 0.029572
training IoU in current batch 20500 is 0.6394257986251516
training IoU uptillnow 20500 is 0.0027456786295538805
testing: bce: 35.661958, dice: 186.016008, loss: 35.661958
IoU in current test batch is 0.8138465485789492
Epoch 20535: reducing learning rate of group 0 to 8.2771e-04.
training: bce: 0.029502, dice: 0.154003, loss: 0.029502
training IoU in current batch 20600 is 0.913379241127436
training IoU uptillnow 20600 is 0.0027471296214517057
testing: bce: 35.750714, dice: 186.624881, loss: 35.750714
IoU in current test batch is 0.8296464109506249
Epoch 20636: reducing learning rate of group 0 to 8.2688e-04.
training: bce: 0.029504, dice: 0.153960, loss: 0.029504
training IoU in current batch 20700 is 0.9033928612897982
training IoU uptillnow 20700 is 0.0027484057912801245
testing: bce: 35.926964, dice: 187.478028, loss: 35.926964
IoU in current test batch is 0.8529095261679259
Epoch 20737: reducing learning rate of group 0 to 8.2605e-04.
training: bce: 0.029436, dice: 0.153772, loss: 0.029436
training IoU in current batch 20800 is 0.8680068095426879
training IoU uptillnow 20800 is 0.0027491026339344624
testing: bce: 36.017284, dice: 188.153349, loss: 36.017284
IoU in current test batch is 0.8588293086849487
Epoch 20838: reducing learning rate of group 0 to 8.2523e-04.
training: bce: 0.029355, dice: 0.153477, loss: 0.029355
training IoU in current batch 20900 is 0.9110603029436982
training IoU uptillnow 20900 is 0.0027504794342911175
testing: bce: 36.091689, dice: 188.695375, loss: 36.091689
IoU in current test batch is 0.8695073346530421
Epoch 20939: reducing learning rate of group 0 to 8.2440e-04.
training: bce: 0.029275, dice: 0.153183, loss: 0.029275
training IoU in current batch 21000 is 0.8994974874371859
training IoU uptillnow 21000 is 0.002751659594873151
testing: bce: 36.165014, dice: 189.234458, loss: 36.165014
IoU in current test batch is 0.8795582796946145
Epoch 21040: reducing learning rate of group 0 to 8.2358e-04.
training: bce: 0.029234, dice: 0.153066, loss: 0.029234
training IoU in current batch 21100 is 0.8555150288863449
training IoU uptillnow 21100 is 0.0027521337769881283
testing: bce: 36.285889, dice: 189.990914, loss: 36.285889
IoU in current test batch is 0.7529412894573703
Epoch 21141: reducing learning rate of group 0 to 8.2275e-04.
training: bce: 0.029177, dice: 0.152893, loss: 0.029177
training IoU in current batch 21200 is 0.8765939543012493
training IoU uptillnow 21200 is 0.0027529348999100786
testing: bce: 36.387450, dice: 190.675722, loss: 36.387450
IoU in current test batch is 0.9030998738646929
Epoch 21242: reducing learning rate of group 0 to 8.2193e-04.
training: bce: 0.029132, dice: 0.152745, loss: 0.029132
training IoU in current batch 21300 is 0.8911251677209124
training IoU uptillnow 21300 is 0.002753955895759223
testing: bce: 36.502560, dice: 191.390034, loss: 36.502560
IoU in current test batch is 0.9039036728402728
Epoch 21343: reducing learning rate of group 0 to 8.2111e-04.
training: bce: 0.029054, dice: 0.152468, loss: 0.029054
training IoU in current batch 21400 is 0.8900273224043715
training IoU uptillnow 21400 is 0.0027549502504416617
testing: bce: 36.575139, dice: 191.939728, loss: 36.575139
IoU in current test batch is 0.8938827525135525
Epoch 21444: reducing learning rate of group 0 to 8.2029e-04.
training: bce: 0.028987, dice: 0.152232, loss: 0.028987
training IoU in current batch 21500 is 0.9100724131914151
training IoU uptillnow 21500 is 0.0027562461178906004
testing: bce: 36.661715, dice: 192.538276, loss: 36.661715
IoU in current test batch is 0.8887237371176447
Epoch 21545: reducing learning rate of group 0 to 8.1947e-04.
training: bce: 0.028931, dice: 0.152024, loss: 0.028931
training IoU in current batch 21600 is 0.9167704826781881
training IoU uptillnow 21600 is 0.0027576333476069873
testing: bce: 36.761330, dice: 193.168541, loss: 36.761330
IoU in current test batch is 0.8565848038151335
Epoch 21646: reducing learning rate of group 0 to 8.1865e-04.
training: bce: 0.383510, dice: 0.996399, loss: 0.383510
training IoU in current batch 0 is 0.0
training IoU uptillnow 0 is 0.0
testing: bce: 0.022559, dice: 0.058612, loss: 0.022559
IoU in current test batch is 0.0
Epoch 21747: reducing learning rate of group 0 to 8.1783e-04.
training: bce: 0.054917, dice: 0.306175, loss: 0.054917
training IoU in current batch 100 is 0.8028502915690229
training IoU uptillnow 100 is 1.227599834203399e-05
testing: bce: 0.326271, dice: 1.819039, loss: 0.326271
IoU in current test batch is 0.8325600264167087
Epoch 21848: reducing learning rate of group 0 to 8.1701e-04.
training: bce: 0.037040, dice: 0.218018, loss: 0.037040
training IoU in current batch 200 is 0.8596331060454766
training IoU uptillnow 200 is 2.5304161303112625e-05
testing: bce: 0.437946, dice: 2.577741, loss: 0.437946
IoU in current test batch is 0.8264562137153516
Epoch 21949: reducing learning rate of group 0 to 8.1620e-04.
training: bce: 0.029893, dice: 0.184466, loss: 0.029893
training IoU in current batch 300 is 0.8915244173964673
training IoU uptillnow 300 is 3.869708810622677e-05
testing: bce: 0.529286, dice: 3.266140, loss: 0.529286
IoU in current test batch is 0.8449638873232085
Epoch 22050: reducing learning rate of group 0 to 8.1538e-04.
training: bce: 0.026905, dice: 0.168919, loss: 0.026905
training IoU in current batch 400 is 0.9049596677581663
training IoU uptillnow 400 is 5.217145524538662e-05
testing: bce: 0.634653, dice: 3.984496, loss: 0.634653
IoU in current test batch is 0.8290725064149634
Epoch 22151: reducing learning rate of group 0 to 8.1456e-04.
training: bce: 0.024197, dice: 0.155551, loss: 0.024197
training IoU in current batch 500 is 0.8462283224649031
training IoU uptillnow 500 is 6.464257965816871e-05
testing: bce: 0.713094, dice: 4.584172, loss: 0.713094
IoU in current test batch is 0.8196163849181828
Epoch 22252: reducing learning rate of group 0 to 8.1375e-04.
training: bce: 0.022878, dice: 0.147951, loss: 0.022878
training IoU in current batch 600 is 0.918981761813646
training IoU uptillnow 600 is 7.808935077799227e-05
testing: bce: 0.808789, dice: 5.230491, loss: 0.808789
IoU in current test batch is 0.8617479825212582
Epoch 22353: reducing learning rate of group 0 to 8.1294e-04.
training: bce: 0.021600, dice: 0.143026, loss: 0.021600
training IoU in current batch 700 is 0.8943857128194601
training IoU uptillnow 700 is 9.105004880754677e-05
testing: bce: 0.890692, dice: 5.897712, loss: 0.890692
IoU in current test batch is 0.8486351553826337
Epoch 22454: reducing learning rate of group 0 to 8.1212e-04.
training: bce: 0.020967, dice: 0.139489, loss: 0.020967
training IoU in current batch 800 is 0.9227034223510063
training IoU uptillnow 800 is 0.00010431506225508369
testing: bce: 0.987899, dice: 6.572386, loss: 0.987899
IoU in current test batch is 0.8412169240624756
Epoch 22555: reducing learning rate of group 0 to 8.1131e-04.
training: bce: 0.023530, dice: 0.143516, loss: 0.023530
training IoU in current batch 900 is 0.7592229382646867
training IoU uptillnow 900 is 0.00011505146962364064
testing: bce: 1.247089, dice: 7.606335, loss: 1.247089
IoU in current test batch is 0.8327423113285153
Epoch 22656: reducing learning rate of group 0 to 8.1050e-04.
training: bce: 0.022800, dice: 0.141492, loss: 0.022800
training IoU in current batch 1000 is 0.9063270575266166
training IoU uptillnow 1000 is 0.00012785340232025628
testing: bce: 1.342537, dice: 8.331363, loss: 1.342537
IoU in current test batch is 0.8557498748963175
Epoch 22757: reducing learning rate of group 0 to 8.0969e-04.
training: bce: 0.022443, dice: 0.139879, loss: 0.022443
training IoU in current batch 1100 is 0.8614414490668246
training IoU uptillnow 1100 is 0.00013988681501573503
testing: bce: 1.453499, dice: 9.059218, loss: 1.453499
IoU in current test batch is 0.8701952652182896
Epoch 22858: reducing learning rate of group 0 to 8.0888e-04.
training: bce: 0.021753, dice: 0.136761, loss: 0.021753
training IoU in current batch 1200 is 0.878798429982274
training IoU uptillnow 1200 is 0.00015206778132545196
testing: bce: 1.536768, dice: 9.661778, loss: 1.536768
IoU in current test batch is 0.8189284986919275
Epoch 22959: reducing learning rate of group 0 to 8.0807e-04.
training: bce: 0.021510, dice: 0.136384, loss: 0.021510
training IoU in current batch 1300 is 0.941858768602276
training IoU uptillnow 1300 is 0.000165056744140012
testing: bce: 1.646172, dice: 10.437423, loss: 1.646172
IoU in current test batch is 0.8680378418162311
Epoch 23060: reducing learning rate of group 0 to 8.0726e-04.
training: bce: 0.020869, dice: 0.133458, loss: 0.020869
training IoU in current batch 1400 is 0.9310540575409625
training IoU uptillnow 1400 is 0.00017777733626553807
testing: bce: 1.719848, dice: 10.998521, loss: 1.719848
IoU in current test batch is 0.8868983954635359
Epoch 23161: reducing learning rate of group 0 to 8.0645e-04.
training: bce: 0.020757, dice: 0.132751, loss: 0.020757
training IoU in current batch 1500 is 0.9049040782619744
training IoU uptillnow 1500 is 0.0001900125500210311
testing: bce: 1.832752, dice: 11.721127, loss: 1.832752
IoU in current test batch is 0.8437795775183973
Epoch 23262: reducing learning rate of group 0 to 8.0565e-04.
training: bce: 0.020698, dice: 0.132244, loss: 0.020698
training IoU in current batch 1600 is 0.850298435231489
training IoU uptillnow 1600 is 0.00020136154387260732
testing: bce: 1.949307, dice: 12.454311, loss: 1.949307
IoU in current test batch is 0.8645418973753826
Epoch 23363: reducing learning rate of group 0 to 8.0484e-04.
training: bce: 0.020446, dice: 0.130963, loss: 0.020446
training IoU in current batch 1700 is 0.8702171991413057
training IoU uptillnow 1700 is 0.00021289728085237264
testing: bce: 2.045815, dice: 13.104044, loss: 2.045815
IoU in current test batch is 0.8342511541104799
Epoch 23464: reducing learning rate of group 0 to 8.0404e-04.
training: bce: 0.020118, dice: 0.129709, loss: 0.020118
training IoU in current batch 1800 is 0.8932949674789438
training IoU uptillnow 1800 is 0.0002246621855789433
testing: bce: 2.131340, dice: 13.741534, loss: 2.131340
IoU in current test batch is 0.8846014501984306
Epoch 23565: reducing learning rate of group 0 to 8.0323e-04.
training: bce: 0.019915, dice: 0.128645, loss: 0.019915
training IoU in current batch 1900 is 0.8504306969459671
training IoU uptillnow 1900 is 0.0002357219601731846
testing: bce: 2.226979, dice: 14.385542, loss: 2.226979
IoU in current test batch is 0.8503607793362892
Epoch 23666: reducing learning rate of group 0 to 8.0243e-04.
training: bce: 0.019746, dice: 0.127638, loss: 0.019746
training IoU in current batch 2000 is 0.9057827769579865
training IoU uptillnow 2000 is 0.0002474669136036492
testing: bce: 2.324188, dice: 15.023751, loss: 2.324188
IoU in current test batch is 0.8730655433446701
Epoch 23767: reducing learning rate of group 0 to 8.0163e-04.
training: bce: 0.019444, dice: 0.126449, loss: 0.019444
training IoU in current batch 2100 is 0.9096241289315791
training IoU uptillnow 2100 is 0.0002591669703942722
testing: bce: 2.403043, dice: 15.627631, loss: 2.403043
IoU in current test batch is 0.8504661761182042
Epoch 23868: reducing learning rate of group 0 to 8.0083e-04.
training: bce: 0.019106, dice: 0.124660, loss: 0.019106
training IoU in current batch 2200 is 0.9269582803951565
training IoU uptillnow 2200 is 0.00027101087819450753
testing: bce: 2.473678, dice: 16.139860, loss: 2.473678
IoU in current test batch is 0.8612059132235459
Epoch 23969: reducing learning rate of group 0 to 8.0003e-04.
training: bce: 0.018975, dice: 0.124126, loss: 0.018975
training IoU in current batch 2300 is 0.9214241228980693
training IoU uptillnow 2300 is 0.00028267922346450366
testing: bce: 2.568378, dice: 16.800769, loss: 2.568378
IoU in current test batch is 0.8842556800571498
Epoch 24070: reducing learning rate of group 0 to 7.9923e-04.
training: bce: 0.018931, dice: 0.123535, loss: 0.018931
training IoU in current batch 2400 is 0.9154724706389398
training IoU uptillnow 2400 is 0.0002941684171519115
testing: bce: 2.673691, dice: 17.447476, loss: 2.673691
IoU in current test batch is 0.8825948552039595
Epoch 24171: reducing learning rate of group 0 to 7.9843e-04.
training: bce: 0.019615, dice: 0.125048, loss: 0.019615
training IoU in current batch 2500 is 0.9395668012764395
training IoU uptillnow 2500 is 0.0003058945366578463
testing: bce: 2.885771, dice: 18.396826, loss: 2.885771
IoU in current test batch is 0.8610078810949929
Epoch 24272: reducing learning rate of group 0 to 7.9763e-04.
training: bce: 0.019402, dice: 0.124162, loss: 0.019402
training IoU in current batch 2600 is 0.8644346052435202
training IoU uptillnow 2600 is 0.00031649352491911056
testing: bce: 2.968512, dice: 18.996727, loss: 2.968512
IoU in current test batch is 0.8629647360241499
Epoch 24373: reducing learning rate of group 0 to 7.9683e-04.
training: bce: 0.019228, dice: 0.123348, loss: 0.019228
training IoU in current batch 2700 is 0.9275013218964808
training IoU uptillnow 2700 is 0.00032786720339480384
testing: bce: 3.055016, dice: 19.597772, loss: 3.055016
IoU in current test batch is 0.8702722327094886
Epoch 24474: reducing learning rate of group 0 to 7.9603e-04.
training: bce: 0.019170, dice: 0.123134, loss: 0.019170
training IoU in current batch 2800 is 0.9333020059326265
training IoU uptillnow 2800 is 0.0003392269563868336
testing: bce: 3.158517, dice: 20.288151, loss: 3.158517
IoU in current test batch is 0.884250996635144
Epoch 24575: reducing learning rate of group 0 to 7.9524e-04.
training: bce: 0.018999, dice: 0.122352, loss: 0.018999
training IoU in current batch 2900 is 0.8751610095735423
training IoU uptillnow 2900 is 0.0003497065352846316
testing: bce: 3.242162, dice: 20.878952, loss: 3.242162
IoU in current test batch is 0.8415044080089457
Epoch 24676: reducing learning rate of group 0 to 7.9444e-04.
training: bce: 0.018922, dice: 0.121914, loss: 0.018922
training IoU in current batch 3000 is 0.9072988811933937
training IoU uptillnow 3000 is 0.0003605349687611229
testing: bce: 3.340328, dice: 21.521338, loss: 3.340328
IoU in current test batch is 0.878489445750161
Epoch 24777: reducing learning rate of group 0 to 7.9365e-04.
training: bce: 0.018877, dice: 0.121495, loss: 0.018877
training IoU in current batch 3100 is 0.895501160266487
training IoU uptillnow 3100 is 0.0003711175046433561
testing: bce: 3.443442, dice: 22.162202, loss: 3.443442
IoU in current test batch is 0.8932491035607312
Epoch 24878: reducing learning rate of group 0 to 7.9285e-04.
training: bce: 0.018721, dice: 0.120713, loss: 0.018721
training IoU in current batch 3200 is 0.914890914287529
training IoU uptillnow 3200 is 0.0003818746085643002
testing: bce: 3.525018, dice: 22.729578, loss: 3.525018
IoU in current test batch is 0.8631524619559517
Epoch 24979: reducing learning rate of group 0 to 7.9206e-04.
training: bce: 0.018578, dice: 0.120187, loss: 0.018578
training IoU in current batch 3300 is 0.9506288302035392
training IoU uptillnow 3300 is 0.00039302216119942347
testing: bce: 3.607473, dice: 23.337496, loss: 3.607473
IoU in current test batch is 0.8646955918846585
Epoch 25080: reducing learning rate of group 0 to 7.9127e-04.
training: bce: 0.018400, dice: 0.119313, loss: 0.018400
training IoU in current batch 3400 is 0.9339105897974992
training IoU uptillnow 3400 is 0.0004038588669290074
testing: bce: 3.681100, dice: 23.869539, loss: 3.681100
IoU in current test batch is 0.8749154575538324
Epoch 25181: reducing learning rate of group 0 to 7.9048e-04.
training: bce: 0.018225, dice: 0.118396, loss: 0.018225
training IoU in current batch 3500 is 0.9084883760051139
training IoU uptillnow 3500 is 0.00041427329438835153
testing: bce: 3.753243, dice: 24.382625, loss: 3.753243
IoU in current test batch is 0.89270371638243
Epoch 25282: reducing learning rate of group 0 to 7.8969e-04.
training: bce: 0.018202, dice: 0.118327, loss: 0.018202
training IoU in current batch 3600 is 0.8711751273493764
training IoU uptillnow 3600 is 0.0004241137837036726
testing: bce: 3.855690, dice: 25.064427, loss: 3.855690
IoU in current test batch is 0.8835968948601696
Epoch 25383: reducing learning rate of group 0 to 7.8890e-04.
training: bce: 0.018085, dice: 0.118018, loss: 0.018085
training IoU in current batch 3700 is 0.9566851751756671
training IoU uptillnow 3700 is 0.0004349989679564884
testing: bce: 3.937285, dice: 25.693312, loss: 3.937285
IoU in current test batch is 0.8814118126506777
Epoch 25484: reducing learning rate of group 0 to 7.8811e-04.
training: bce: 0.018062, dice: 0.117818, loss: 0.018062
training IoU in current batch 3800 is 0.8573768343815513
training IoU uptillnow 3800 is 0.00044450062996948984
testing: bce: 4.038405, dice: 26.342637, loss: 4.038405
IoU in current test batch is 0.8727834122162891
Epoch 25585: reducing learning rate of group 0 to 7.8732e-04.
training: bce: 0.017945, dice: 0.117472, loss: 0.017945
training IoU in current batch 3900 is 0.9050934085301374
training IoU uptillnow 3900 is 0.000454549369807241
testing: bce: 4.117917, dice: 26.956478, loss: 4.117917
IoU in current test batch is 0.8483357553865214
Epoch 25686: reducing learning rate of group 0 to 7.8653e-04.
training: bce: 0.017987, dice: 0.117493, loss: 0.017987
training IoU in current batch 4000 is 0.9039816430741388
training IoU uptillnow 4000 is 0.0004645054895495492
testing: bce: 4.233359, dice: 27.652380, loss: 4.233359
IoU in current test batch is 0.828419318999198
Epoch 25787: reducing learning rate of group 0 to 7.8575e-04.
training: bce: 0.018082, dice: 0.117529, loss: 0.018082
training IoU in current batch 4100 is 0.8858746795599831
training IoU uptillnow 4100 is 0.00047415048997196676
testing: bce: 4.362014, dice: 28.352073, loss: 4.362014
IoU in current test batch is 0.8675232919528614
Epoch 25888: reducing learning rate of group 0 to 7.8496e-04.
training: bce: 0.018340, dice: 0.118166, loss: 0.018340
training IoU in current batch 4200 is 0.8501145412894077
training IoU uptillnow 4200 is 0.0004832607781868679
testing: bce: 4.532145, dice: 29.201010, loss: 4.532145
IoU in current test batch is 0.8460424827455213
Epoch 25989: reducing learning rate of group 0 to 7.8418e-04.
training: bce: 0.018438, dice: 0.118307, loss: 0.018438
training IoU in current batch 4300 is 0.9194811777173663
training IoU uptillnow 4300 is 0.0004931903031132949
testing: bce: 4.664843, dice: 29.931737, loss: 4.664843
IoU in current test batch is 0.8590446187426874
Epoch 26090: reducing learning rate of group 0 to 7.8339e-04.
training: bce: 0.018421, dice: 0.118169, loss: 0.018421
training IoU in current batch 4400 is 0.9359668684294361
training IoU uptillnow 4400 is 0.0005032542849459315
testing: bce: 4.768870, dice: 30.591980, loss: 4.768870
IoU in current test batch is 0.8652975474217937
Epoch 26191: reducing learning rate of group 0 to 7.8261e-04.
training: bce: 0.018328, dice: 0.118008, loss: 0.018328
training IoU in current batch 4500 is 0.9343278378243761
training IoU uptillnow 4500 is 0.0005132205896830892
testing: bce: 4.852524, dice: 31.244240, loss: 4.852524
IoU in current test batch is 0.8595319435228576
Epoch 26292: reducing learning rate of group 0 to 7.8183e-04.
training: bce: 0.018225, dice: 0.117548, loss: 0.018225
training IoU in current batch 4600 is 0.91106509359418
training IoU uptillnow 4600 is 0.0005228162667006971
testing: bce: 4.932557, dice: 31.814105, loss: 4.932557
IoU in current test batch is 0.8894933259007378
Epoch 26393: reducing learning rate of group 0 to 7.8104e-04.
training: bce: 0.018109, dice: 0.117191, loss: 0.018109
training IoU in current batch 4700 is 0.8700633978944919
training IoU uptillnow 4700 is 0.0005318215510174178
testing: bce: 5.007591, dice: 32.406835, loss: 5.007591
IoU in current test batch is 0.8832838809174854
Epoch 26494: reducing learning rate of group 0 to 7.8026e-04.
training: bce: 0.018022, dice: 0.116739, loss: 0.018022
training IoU in current batch 4800 is 0.9034699670135782
training IoU uptillnow 4800 is 0.0005411790793407933
testing: bce: 5.089747, dice: 32.968514, loss: 5.089747
IoU in current test batch is 0.8998237054385823
Epoch 26595: reducing learning rate of group 0 to 7.7948e-04.
training: bce: 0.017910, dice: 0.116336, loss: 0.017910
training IoU in current batch 4900 is 0.9046225719203344
training IoU uptillnow 4900 is 0.0005504806939788647
testing: bce: 5.163320, dice: 33.539111, loss: 5.163320
IoU in current test batch is 0.8450468896966804
Epoch 26696: reducing learning rate of group 0 to 7.7870e-04.
training: bce: 0.017889, dice: 0.116141, loss: 0.017889
training IoU in current batch 5000 is 0.876423845061486
training IoU uptillnow 5000 is 0.0005593605895702233
testing: bce: 5.262522, dice: 34.165844, loss: 5.262522
IoU in current test batch is 0.8850201486074549
Epoch 26797: reducing learning rate of group 0 to 7.7792e-04.
training: bce: 0.017828, dice: 0.115753, loss: 0.017828
training IoU in current batch 5100 is 0.8932640750670241
training IoU uptillnow 5100 is 0.0005683836728811183
testing: bce: 5.349442, dice: 34.732778, loss: 5.349442
IoU in current test batch is 0.872135634351055
Epoch 26898: reducing learning rate of group 0 to 7.7715e-04.
training: bce: 0.017860, dice: 0.115900, loss: 0.017860
training IoU in current batch 5200 is 0.847290188402251
training IoU uptillnow 5200 is 0.0005767699812644877
testing: bce: 5.463977, dice: 35.458590, loss: 5.463977
IoU in current test batch is 0.8347477206744409
Epoch 26999: reducing learning rate of group 0 to 7.7637e-04.
training: bce: 0.017805, dice: 0.115664, loss: 0.017805
training IoU in current batch 5300 is 0.9252348850788258
training IoU uptillnow 5300 is 0.000586056449050901
testing: bce: 5.552013, dice: 36.066820, loss: 5.552013
IoU in current test batch is 0.8580107096696575
training: bce: 0.017704, dice: 0.115320, loss: 0.017704
training IoU in current batch 5400 is 0.9107718405428329
training IoU uptillnow 5400 is 0.0005950964847929375
testing: bce: 5.624789, dice: 36.637870, loss: 5.624789
IoU in current test batch is 0.8787317413579165
Epoch 27100: reducing learning rate of group 0 to 7.7559e-04.
training: bce: 0.017591, dice: 0.114811, loss: 0.017591
training IoU in current batch 5500 is 0.8661264551472266
training IoU uptillnow 5500 is 0.0006035229248629049
testing: bce: 5.692298, dice: 37.151537, loss: 5.692298
IoU in current test batch is 0.890518718388636
Epoch 27201: reducing learning rate of group 0 to 7.7482e-04.
training: bce: 0.017477, dice: 0.114245, loss: 0.017477
training IoU in current batch 5600 is 0.9003170028818444
training IoU uptillnow 5600 is 0.0006123050997764943
testing: bce: 5.758305, dice: 37.640338, loss: 5.758305
IoU in current test batch is 0.859649927589368
Epoch 27302: reducing learning rate of group 0 to 7.7404e-04.
training: bce: 0.017392, dice: 0.113856, loss: 0.017392
training IoU in current batch 5700 is 0.9093083280054935
training IoU uptillnow 5700 is 0.0006211325547408806
testing: bce: 5.832304, dice: 38.181968, loss: 5.832304
IoU in current test batch is 0.8787622552916213
Epoch 27403: reducing learning rate of group 0 to 7.7327e-04.
training: bce: 0.017313, dice: 0.113520, loss: 0.017313
training IoU in current batch 5800 is 0.9085885847519882
training IoU uptillnow 5800 is 0.00062988708587215
testing: bce: 5.907927, dice: 38.737157, loss: 5.907927
IoU in current test batch is 0.8499016795059283
Epoch 27504: reducing learning rate of group 0 to 7.7250e-04.
training: bce: 0.017290, dice: 0.113515, loss: 0.017290
training IoU in current batch 5900 is 0.9200047772602412
training IoU uptillnow 5900 is 0.0006387160550931474
testing: bce: 6.001671, dice: 39.403127, loss: 6.001671
IoU in current test batch is 0.8759541939259496
Epoch 27605: reducing learning rate of group 0 to 7.7172e-04.
training: bce: 0.017271, dice: 0.113479, loss: 0.017271
training IoU in current batch 6000 is 0.8533028394915053
training IoU uptillnow 6000 is 0.0006466786065126848
testing: bce: 6.096643, dice: 40.058131, loss: 6.096643
IoU in current test batch is 0.8815384678839877
Epoch 27706: reducing learning rate of group 0 to 7.7095e-04.
training: bce: 0.017198, dice: 0.113181, loss: 0.017198
training IoU in current batch 6100 is 0.9199297687669781
training IoU uptillnow 6100 is 0.0006553827574337061
testing: bce: 6.172083, dice: 40.618530, loss: 6.172083
IoU in current test batch is 0.8765951357510096
Epoch 27807: reducing learning rate of group 0 to 7.7018e-04.
training: bce: 0.017147, dice: 0.112878, loss: 0.017147
training IoU in current batch 6200 is 0.9214203010420687
training IoU uptillnow 6200 is 0.0006640423210395837
testing: bce: 6.254707, dice: 41.173986, loss: 6.254707
IoU in current test batch is 0.8782191598078717
Epoch 27908: reducing learning rate of group 0 to 7.6941e-04.
training: bce: 0.017079, dice: 0.112535, loss: 0.017079
training IoU in current batch 6300 is 0.8679047161886475
training IoU uptillnow 6300 is 0.0006720029403238311
testing: bce: 6.330464, dice: 41.710769, loss: 6.330464
IoU in current test batch is 0.8654694696349101
Epoch 28009: reducing learning rate of group 0 to 7.6864e-04.
training: bce: 0.017069, dice: 0.112437, loss: 0.017069
training IoU in current batch 6400 is 0.862015503875969
training IoU uptillnow 6400 is 0.000679837040226308
testing: bce: 6.426958, dice: 42.335701, loss: 6.426958
IoU in current test batch is 0.8695520288928225
Epoch 28110: reducing learning rate of group 0 to 7.6787e-04.
training: bce: 0.017076, dice: 0.112402, loss: 0.017076
training IoU in current batch 6500 is 0.8281519632569623
training IoU uptillnow 6500 is 0.0006872153008786612
testing: bce: 6.530083, dice: 42.983895, loss: 6.530083
IoU in current test batch is 0.8452398710892854
Epoch 28211: reducing learning rate of group 0 to 7.6710e-04.
training: bce: 0.017003, dice: 0.112185, loss: 0.017003
training IoU in current batch 6600 is 0.8805229506632313
training IoU uptillnow 6600 is 0.0006951582733215309
testing: bce: 6.602034, dice: 43.560863, loss: 6.602034
IoU in current test batch is 0.8506822021476041
Epoch 28312: reducing learning rate of group 0 to 7.6634e-04.
training: bce: 0.016947, dice: 0.111995, loss: 0.016947
training IoU in current batch 6700 is 0.9073605075844419
training IoU uptillnow 6700 is 0.0007033603041382913
testing: bce: 6.680146, dice: 44.145694, loss: 6.680146
IoU in current test batch is 0.8480082499758762
Epoch 28413: reducing learning rate of group 0 to 7.6557e-04.
training: bce: 0.016909, dice: 0.111855, loss: 0.016909
training IoU in current batch 6800 is 0.8461512377076975
training IoU uptillnow 6800 is 0.0007107888789507615
testing: bce: 6.764688, dice: 44.748595, loss: 6.764688
IoU in current test batch is 0.8634178582294547
Epoch 28514: reducing learning rate of group 0 to 7.6481e-04.
training: bce: 0.016920, dice: 0.111771, loss: 0.016920
training IoU in current batch 6900 is 0.8884422649194736
training IoU uptillnow 6900 is 0.0007186584081026758
testing: bce: 6.868485, dice: 45.372562, loss: 6.868485
IoU in current test batch is 0.8840881615488961
Epoch 28615: reducing learning rate of group 0 to 7.6404e-04.
training: bce: 0.017040, dice: 0.112200, loss: 0.017040
training IoU in current batch 7000 is 0.910996174775066
training IoU uptillnow 7000 is 0.0007267350475027253
testing: bce: 7.017334, dice: 46.206592, loss: 7.017334
IoU in current test batch is 0.8784071389399958
Epoch 28716: reducing learning rate of group 0 to 7.6328e-04.
training: bce: 0.017139, dice: 0.112531, loss: 0.017139
training IoU in current batch 7100 is 0.7848971130221131
training IoU uptillnow 7100 is 0.0007332961192477634
testing: bce: 7.159211, dice: 47.004681, loss: 7.159211
IoU in current test batch is 0.8416176689653392
Epoch 28817: reducing learning rate of group 0 to 7.6251e-04.
training: bce: 0.017100, dice: 0.112381, loss: 0.017100
training IoU in current batch 7200 is 0.8304686533943366
training IoU uptillnow 7200 is 0.00074033740895503
testing: bce: 7.243451, dice: 47.603070, loss: 7.243451
IoU in current test batch is 0.8609475174633143
Epoch 28918: reducing learning rate of group 0 to 7.6175e-04.
training: bce: 0.017113, dice: 0.112251, loss: 0.017113
training IoU in current batch 7300 is 0.8356948047628551
training IoU uptillnow 7300 is 0.0007473902087490109
testing: bce: 7.349407, dice: 48.208544, loss: 7.349407
IoU in current test batch is 0.8508667014743937
Epoch 29019: reducing learning rate of group 0 to 7.6099e-04.
training: bce: 0.017090, dice: 0.112160, loss: 0.017090
training IoU in current batch 7400 is 0.9182510178792707
training IoU uptillnow 7400 is 0.0007553401967817093
testing: bce: 7.440250, dice: 48.829307, loss: 7.440250
IoU in current test batch is 0.8746212021347304
Epoch 29120: reducing learning rate of group 0 to 7.6023e-04.
training: bce: 0.017035, dice: 0.111961, loss: 0.017035
training IoU in current batch 7500 is 0.9253106803458445
training IoU uptillnow 7500 is 0.0007633163225957656
testing: bce: 7.516295, dice: 49.401216, loss: 7.516295
IoU in current test batch is 0.8482304815974137
Epoch 29221: reducing learning rate of group 0 to 7.5947e-04.
training: bce: 0.017035, dice: 0.111966, loss: 0.017035
training IoU in current batch 7600 is 0.9284024821214903
training IoU uptillnow 7600 is 0.0007712731779466502
testing: bce: 7.616779, dice: 50.062194, loss: 7.616779
IoU in current test batch is 0.8736260454291788
Epoch 29322: reducing learning rate of group 0 to 7.5871e-04.
training: bce: 0.016962, dice: 0.111643, loss: 0.016962
training IoU in current batch 7700 is 0.9182888121120193
training IoU uptillnow 7700 is 0.000779061237569417
testing: bce: 7.683767, dice: 50.574235, loss: 7.683767
IoU in current test batch is 0.8796141047318716
Epoch 29423: reducing learning rate of group 0 to 7.5795e-04.
training: bce: 0.017009, dice: 0.111704, loss: 0.017009
training IoU in current batch 7800 is 0.8779734979511469
training IoU uptillnow 7800 is 0.0007863409565149574
testing: bce: 7.805082, dice: 51.258982, loss: 7.805082
IoU in current test batch is 0.8742354236420053
Epoch 29524: reducing learning rate of group 0 to 7.5719e-04.
training: bce: 0.016980, dice: 0.111630, loss: 0.016980
training IoU in current batch 7900 is 0.9185671407332604
training IoU uptillnow 7900 is 0.0007940286237872408
testing: bce: 7.891673, dice: 51.881790, loss: 7.891673
IoU in current test batch is 0.8890039052468008
Epoch 29625: reducing learning rate of group 0 to 7.5643e-04.
training: bce: 0.016948, dice: 0.111477, loss: 0.016948
training IoU in current batch 8000 is 0.8578423424884075
training IoU uptillnow 8000 is 0.0008009829869225073
testing: bce: 7.976484, dice: 52.466272, loss: 7.976484
IoU in current test batch is 0.8511197356308997
Epoch 29726: reducing learning rate of group 0 to 7.5568e-04.
training: bce: 0.016882, dice: 0.111166, loss: 0.016882
training IoU in current batch 8100 is 0.9088732000124405
training IoU uptillnow 8100 is 0.0008084614914408034
testing: bce: 8.044614, dice: 52.973906, loss: 8.044614
IoU in current test batch is 0.872845386798295
Epoch 29827: reducing learning rate of group 0 to 7.5492e-04.
training: bce: 0.016818, dice: 0.110905, loss: 0.016818
training IoU in current batch 8200 is 0.8405109768224585
training IoU uptillnow 8200 is 0.0008151278518576398
testing: bce: 8.113220, dice: 53.501926, loss: 8.113220
IoU in current test batch is 0.8783909163544572
Epoch 29928: reducing learning rate of group 0 to 7.5417e-04.
training: bce: 0.016792, dice: 0.110710, loss: 0.016792
training IoU in current batch 8300 is 0.90689608454139
training IoU uptillnow 8300 is 0.0008224873821796853
testing: bce: 8.199495, dice: 54.059212, loss: 8.199495
IoU in current test batch is 0.889840330593844
Epoch 30029: reducing learning rate of group 0 to 7.5341e-04.
training: bce: 0.016756, dice: 0.110593, loss: 0.016756
training IoU in current batch 8400 is 0.9242739263205769
training IoU uptillnow 8400 is 0.0008299904576134248
testing: bce: 8.280564, dice: 54.652705, loss: 8.280564
IoU in current test batch is 0.874515753858753
Epoch 30130: reducing learning rate of group 0 to 7.5266e-04.
training: bce: 0.016772, dice: 0.110637, loss: 0.016772
training IoU in current batch 8500 is 0.9000278646397721
training IoU uptillnow 8500 is 0.0008371762272310378
testing: bce: 8.386835, dice: 55.325206, loss: 8.386835
IoU in current test batch is 0.874605923089599
Epoch 30231: reducing learning rate of group 0 to 7.5191e-04.
training: bce: 0.016768, dice: 0.110643, loss: 0.016768
training IoU in current batch 8600 is 0.854055160714808
training IoU uptillnow 8600 is 0.0008438088157078859
testing: bce: 8.483690, dice: 55.978734, loss: 8.483690
IoU in current test batch is 0.8390817182268911
Epoch 30332: reducing learning rate of group 0 to 7.5116e-04.
training: bce: 0.016724, dice: 0.110468, loss: 0.016724
training IoU in current batch 8700 is 0.8518742312091327
training IoU uptillnow 8700 is 0.0008503738550335084
testing: bce: 8.559532, dice: 56.540189, loss: 8.559532
IoU in current test batch is 0.8779469945432505
Epoch 30433: reducing learning rate of group 0 to 7.5040e-04.
training: bce: 0.016717, dice: 0.110424, loss: 0.016717
training IoU in current batch 8800 is 0.9549270461371107
training IoU uptillnow 8800 is 0.000858022105193367
testing: bce: 8.654663, dice: 57.167345, loss: 8.654663
IoU in current test batch is 0.8923422632586507
Epoch 30534: reducing learning rate of group 0 to 7.4965e-04.
training: bce: 0.016660, dice: 0.110143, loss: 0.016660
training IoU in current batch 8900 is 0.9402694786174576
training IoU uptillnow 8900 is 0.0008654606982985897
testing: bce: 8.723028, dice: 57.669839, loss: 8.723028
IoU in current test batch is 0.8631125070687201
Epoch 30635: reducing learning rate of group 0 to 7.4890e-04.
training: bce: 0.016635, dice: 0.110032, loss: 0.016635
training IoU in current batch 9000 is 0.9291570185868953
training IoU uptillnow 9000 is 0.0008727301750531751
testing: bce: 8.807562, dice: 58.258744, loss: 8.807562
IoU in current test batch is 0.9056765120304291
Epoch 30736: reducing learning rate of group 0 to 7.4816e-04.
training: bce: 0.016598, dice: 0.109848, loss: 0.016598
training IoU in current batch 9100 is 0.9238608443153897
training IoU uptillnow 9100 is 0.000879895129509879
testing: bce: 8.885862, dice: 58.807207, loss: 8.885862
IoU in current test batch is 0.8980294512634037
Epoch 30837: reducing learning rate of group 0 to 7.4741e-04.
training: bce: 0.016554, dice: 0.109693, loss: 0.016554
training IoU in current batch 9200 is 0.9333695034361598
training IoU uptillnow 9200 is 0.0008871162833888778
testing: bce: 8.959365, dice: 59.369950, loss: 8.959365
IoU in current test batch is 0.885989952980936
Epoch 30938: reducing learning rate of group 0 to 7.4666e-04.
training: bce: 0.016531, dice: 0.109587, loss: 0.016531
training IoU in current batch 9300 is 0.9307946927762788
training IoU uptillnow 9300 is 0.0008942631630422069
testing: bce: 9.044201, dice: 59.957050, loss: 9.044201
IoU in current test batch is 0.8873969623283792
Epoch 31039: reducing learning rate of group 0 to 7.4591e-04.
training: bce: 0.016534, dice: 0.109506, loss: 0.016534
training IoU in current batch 9400 is 0.9032241028755512
training IoU uptillnow 9400 is 0.0009010685773397728
testing: bce: 9.143240, dice: 60.557079, loss: 9.143240
IoU in current test batch is 0.7816240474086342
Epoch 31140: reducing learning rate of group 0 to 7.4517e-04.
training: bce: 0.016552, dice: 0.109632, loss: 0.016552
training IoU in current batch 9500 is 0.8952584416355113
training IoU uptillnow 9500 is 0.0009077452639683365
testing: bce: 9.250867, dice: 61.271461, loss: 9.250867
IoU in current test batch is 0.8393117931852032
Epoch 31241: reducing learning rate of group 0 to 7.4442e-04.
training: bce: 0.016523, dice: 0.109495, loss: 0.016523
training IoU in current batch 9600 is 0.8638839532783782
training IoU uptillnow 9600 is 0.0009140451614559603
testing: bce: 9.331732, dice: 61.838868, loss: 9.331732
IoU in current test batch is 0.8835765190661697
Epoch 31342: reducing learning rate of group 0 to 7.4368e-04.
training: bce: 0.016556, dice: 0.109580, loss: 0.016556
training IoU in current batch 9700 is 0.9106516204500104
training IoU uptillnow 9700 is 0.0009208014042586484
testing: bce: 9.447698, dice: 62.531270, loss: 9.447698
IoU in current test batch is 0.8622390168092391
Epoch 31443: reducing learning rate of group 0 to 7.4293e-04.
training: bce: 0.016518, dice: 0.109377, loss: 0.016518
training IoU in current batch 9800 is 0.9244089507393057
training IoU uptillnow 9800 is 0.0009276603304963386
testing: bce: 9.523171, dice: 63.058913, loss: 9.523171
IoU in current test batch is 0.8742342526332809
Epoch 31544: reducing learning rate of group 0 to 7.4219e-04.
training: bce: 0.016471, dice: 0.109191, loss: 0.016471
training IoU in current batch 9900 is 0.8561090688048166
training IoU uptillnow 9900 is 0.0009337553829188692
testing: bce: 9.593134, dice: 63.594194, loss: 9.593134
IoU in current test batch is 0.8756235037738895
Epoch 31645: reducing learning rate of group 0 to 7.4145e-04.
training: bce: 0.016441, dice: 0.109059, loss: 0.016441
training IoU in current batch 10000 is 0.8877768881772085
training IoU uptillnow 10000 is 0.0009401449756980654
testing: bce: 9.672061, dice: 64.158743, loss: 9.672061
IoU in current test batch is 0.8433670956758752
Epoch 31746: reducing learning rate of group 0 to 7.4071e-04.
training: bce: 0.016448, dice: 0.109043, loss: 0.016448
training IoU in current batch 10100 is 0.7623849464392297
training IoU uptillnow 10100 is 0.0009451800014185037
testing: bce: 9.772827, dice: 64.790934, loss: 9.772827
IoU in current test batch is 0.818809797341017
Epoch 31847: reducing learning rate of group 0 to 7.3997e-04.
training: bce: 0.016429, dice: 0.109004, loss: 0.016429
training IoU in current batch 10200 is 0.9162802278173726
training IoU uptillnow 10200 is 0.0009517915607433919
testing: bce: 9.858169, dice: 65.409035, loss: 9.858169
IoU in current test batch is 0.8306020826454721
Epoch 31948: reducing learning rate of group 0 to 7.3923e-04.
training: bce: 0.016428, dice: 0.109014, loss: 0.016428
training IoU in current batch 10300 is 0.8661360540583852
training IoU uptillnow 10300 is 0.0009578394626791771
testing: bce: 9.954288, dice: 66.056139, loss: 9.954288
IoU in current test batch is 0.842907682706622
Epoch 32049: reducing learning rate of group 0 to 7.3849e-04.
training: bce: 0.016462, dice: 0.109063, loss: 0.016462
training IoU in current batch 10400 is 0.731373140054257
training IoU uptillnow 10400 is 0.0009624502757762748
testing: bce: 10.071563, dice: 66.727320, loss: 10.071563
IoU in current test batch is 0.8378558387454561
Epoch 32150: reducing learning rate of group 0 to 7.3775e-04.
training: bce: 0.016433, dice: 0.108983, loss: 0.016433
training IoU in current batch 10500 is 0.8538154324563275
training IoU uptillnow 10500 is 0.0009682999688375941
testing: bce: 10.150722, dice: 67.319327, loss: 10.150722
IoU in current test batch is 0.8537377475543457
Epoch 32251: reducing learning rate of group 0 to 7.3701e-04.
training: bce: 0.016386, dice: 0.108735, loss: 0.016386
training IoU in current batch 10600 is 0.9178251467408094
training IoU uptillnow 10600 is 0.0009747740158560619
testing: bce: 10.218259, dice: 67.806132, loss: 10.218259
IoU in current test batch is 0.8770141726256062
Epoch 32352: reducing learning rate of group 0 to 7.3627e-04.
training: bce: 0.016383, dice: 0.108736, loss: 0.016383
training IoU in current batch 10700 is 0.8487197556964999
training IoU uptillnow 10700 is 0.0009804971388081161
testing: bce: 10.312315, dice: 68.446250, loss: 10.312315
IoU in current test batch is 0.8543214717223752
Epoch 32453: reducing learning rate of group 0 to 7.3554e-04.
training: bce: 0.016350, dice: 0.108564, loss: 0.016350
training IoU in current batch 10800 is 0.9165180618535691
training IoU uptillnow 10800 is 0.0009868804097846406
testing: bce: 10.387795, dice: 68.976536, loss: 10.387795
IoU in current test batch is 0.8816949973007611
Epoch 32554: reducing learning rate of group 0 to 7.3480e-04.
training: bce: 0.016313, dice: 0.108377, loss: 0.016313
training IoU in current batch 10900 is 0.9015941685433211
training IoU uptillnow 10900 is 0.0009930719235434129
testing: bce: 10.460165, dice: 69.495053, loss: 10.460165
IoU in current test batch is 0.8983087046277464
Epoch 32655: reducing learning rate of group 0 to 7.3407e-04.
training: bce: 0.016323, dice: 0.108419, loss: 0.016323
training IoU in current batch 11000 is 0.8600030849915162
training IoU uptillnow 11000 is 0.000998801602523316
testing: bce: 10.563112, dice: 70.160000, loss: 10.563112
IoU in current test batch is 0.8607864292945715
Epoch 32756: reducing learning rate of group 0 to 7.3333e-04.
training: bce: 0.016329, dice: 0.108370, loss: 0.016329
training IoU in current batch 11100 is 0.9047789298057443
training IoU uptillnow 11100 is 0.0010049513835095837
testing: bce: 10.662660, dice: 70.765317, loss: 10.662660
IoU in current test batch is 0.8815216678726996
Epoch 32857: reducing learning rate of group 0 to 7.3260e-04.
training: bce: 0.016290, dice: 0.108192, loss: 0.016290
training IoU in current batch 11200 is 0.9266781154334354
training IoU uptillnow 11200 is 0.0010112856560564993
testing: bce: 10.733238, dice: 71.286121, loss: 10.733238
IoU in current test batch is 0.8548154695775589
Epoch 32958: reducing learning rate of group 0 to 7.3187e-04.
training: bce: 0.016271, dice: 0.108144, loss: 0.016271
training IoU in current batch 11300 is 0.9419859448969391
training IoU uptillnow 11300 is 0.0010177361636128628
testing: bce: 10.816484, dice: 71.890430, loss: 10.816484
IoU in current test batch is 0.8800626342684883
Epoch 33059: reducing learning rate of group 0 to 7.3114e-04.
training: bce: 0.016242, dice: 0.108014, loss: 0.016242
training IoU in current batch 11400 is 0.955713866039953
training IoU uptillnow 11400 is 0.0010242859422327632
testing: bce: 10.892628, dice: 72.439163, loss: 10.892628
IoU in current test batch is 0.8771331435173236
Epoch 33160: reducing learning rate of group 0 to 7.3040e-04.
training: bce: 0.016337, dice: 0.108306, loss: 0.016337
training IoU in current batch 11500 is 0.7605081319211853
training IoU uptillnow 11500 is 0.0010288363674260499
testing: bce: 11.052363, dice: 73.271950, loss: 11.052363
IoU in current test batch is 0.843858762455586
Epoch 33261: reducing learning rate of group 0 to 7.2967e-04.
training: bce: 0.016315, dice: 0.108273, loss: 0.016315
training IoU in current batch 11600 is 0.897693255854904
training IoU uptillnow 11600 is 0.0010347326872020967
testing: bce: 11.133420, dice: 73.886737, loss: 11.133420
IoU in current test batch is 0.8716756140557012
Maximum training samples requirement meet, I have been training for more than  100002  samples.
Making network now
ResNetUNet(
  (base_model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer0_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer1_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2_1x1): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3_1x1): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4_1x1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (upsample): Upsample(scale_factor=2.0, mode=bilinear)
  (conv_up3): Sequential(
    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up2): Sequential(
    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up1): Sequential(
    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up0): Sequential(
    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size2): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 512, 512]           1,792
              ReLU-2         [-1, 64, 512, 512]               0
            Conv2d-3         [-1, 64, 512, 512]          36,928
              ReLU-4         [-1, 64, 512, 512]               0
            Conv2d-5         [-1, 64, 256, 256]           9,408
            Conv2d-6         [-1, 64, 256, 256]           9,408
       BatchNorm2d-7         [-1, 64, 256, 256]             128
       BatchNorm2d-8         [-1, 64, 256, 256]             128
              ReLU-9         [-1, 64, 256, 256]               0
             ReLU-10         [-1, 64, 256, 256]               0
        MaxPool2d-11         [-1, 64, 128, 128]               0
        MaxPool2d-12         [-1, 64, 128, 128]               0
           Conv2d-13         [-1, 64, 128, 128]          36,864
           Conv2d-14         [-1, 64, 128, 128]          36,864
      BatchNorm2d-15         [-1, 64, 128, 128]             128
      BatchNorm2d-16         [-1, 64, 128, 128]             128
             ReLU-17         [-1, 64, 128, 128]               0
             ReLU-18         [-1, 64, 128, 128]               0
           Conv2d-19         [-1, 64, 128, 128]          36,864
           Conv2d-20         [-1, 64, 128, 128]          36,864
      BatchNorm2d-21         [-1, 64, 128, 128]             128
      BatchNorm2d-22         [-1, 64, 128, 128]             128
             ReLU-23         [-1, 64, 128, 128]               0
             ReLU-24         [-1, 64, 128, 128]               0
       BasicBlock-25         [-1, 64, 128, 128]               0
       BasicBlock-26         [-1, 64, 128, 128]               0
           Conv2d-27         [-1, 64, 128, 128]          36,864
           Conv2d-28         [-1, 64, 128, 128]          36,864
      BatchNorm2d-29         [-1, 64, 128, 128]             128
      BatchNorm2d-30         [-1, 64, 128, 128]             128
             ReLU-31         [-1, 64, 128, 128]               0
             ReLU-32         [-1, 64, 128, 128]               0
           Conv2d-33         [-1, 64, 128, 128]          36,864
           Conv2d-34         [-1, 64, 128, 128]          36,864
      BatchNorm2d-35         [-1, 64, 128, 128]             128
      BatchNorm2d-36         [-1, 64, 128, 128]             128
             ReLU-37         [-1, 64, 128, 128]               0
             ReLU-38         [-1, 64, 128, 128]               0
       BasicBlock-39         [-1, 64, 128, 128]               0
       BasicBlock-40         [-1, 64, 128, 128]               0
           Conv2d-41          [-1, 128, 64, 64]          73,728
           Conv2d-42          [-1, 128, 64, 64]          73,728
      BatchNorm2d-43          [-1, 128, 64, 64]             256
      BatchNorm2d-44          [-1, 128, 64, 64]             256
             ReLU-45          [-1, 128, 64, 64]               0
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]         147,456
           Conv2d-48          [-1, 128, 64, 64]         147,456
      BatchNorm2d-49          [-1, 128, 64, 64]             256
      BatchNorm2d-50          [-1, 128, 64, 64]             256
           Conv2d-51          [-1, 128, 64, 64]           8,192
           Conv2d-52          [-1, 128, 64, 64]           8,192
      BatchNorm2d-53          [-1, 128, 64, 64]             256
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
             ReLU-56          [-1, 128, 64, 64]               0
       BasicBlock-57          [-1, 128, 64, 64]               0
       BasicBlock-58          [-1, 128, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]         147,456
           Conv2d-60          [-1, 128, 64, 64]         147,456
      BatchNorm2d-61          [-1, 128, 64, 64]             256
      BatchNorm2d-62          [-1, 128, 64, 64]             256
             ReLU-63          [-1, 128, 64, 64]               0
             ReLU-64          [-1, 128, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]         147,456
           Conv2d-66          [-1, 128, 64, 64]         147,456
      BatchNorm2d-67          [-1, 128, 64, 64]             256
      BatchNorm2d-68          [-1, 128, 64, 64]             256
             ReLU-69          [-1, 128, 64, 64]               0
             ReLU-70          [-1, 128, 64, 64]               0
       BasicBlock-71          [-1, 128, 64, 64]               0
       BasicBlock-72          [-1, 128, 64, 64]               0
           Conv2d-73          [-1, 256, 32, 32]         294,912
           Conv2d-74          [-1, 256, 32, 32]         294,912
      BatchNorm2d-75          [-1, 256, 32, 32]             512
      BatchNorm2d-76          [-1, 256, 32, 32]             512
             ReLU-77          [-1, 256, 32, 32]               0
             ReLU-78          [-1, 256, 32, 32]               0
           Conv2d-79          [-1, 256, 32, 32]         589,824
           Conv2d-80          [-1, 256, 32, 32]         589,824
      BatchNorm2d-81          [-1, 256, 32, 32]             512
      BatchNorm2d-82          [-1, 256, 32, 32]             512
           Conv2d-83          [-1, 256, 32, 32]          32,768
           Conv2d-84          [-1, 256, 32, 32]          32,768
      BatchNorm2d-85          [-1, 256, 32, 32]             512
      BatchNorm2d-86          [-1, 256, 32, 32]             512
             ReLU-87          [-1, 256, 32, 32]               0
             ReLU-88          [-1, 256, 32, 32]               0
       BasicBlock-89          [-1, 256, 32, 32]               0
       BasicBlock-90          [-1, 256, 32, 32]               0
           Conv2d-91          [-1, 256, 32, 32]         589,824
           Conv2d-92          [-1, 256, 32, 32]         589,824
      BatchNorm2d-93          [-1, 256, 32, 32]             512
      BatchNorm2d-94          [-1, 256, 32, 32]             512
             ReLU-95          [-1, 256, 32, 32]               0
             ReLU-96          [-1, 256, 32, 32]               0
           Conv2d-97          [-1, 256, 32, 32]         589,824
           Conv2d-98          [-1, 256, 32, 32]         589,824
      BatchNorm2d-99          [-1, 256, 32, 32]             512
     BatchNorm2d-100          [-1, 256, 32, 32]             512
            ReLU-101          [-1, 256, 32, 32]               0
            ReLU-102          [-1, 256, 32, 32]               0
      BasicBlock-103          [-1, 256, 32, 32]               0
      BasicBlock-104          [-1, 256, 32, 32]               0
          Conv2d-105          [-1, 512, 16, 16]       1,179,648
          Conv2d-106          [-1, 512, 16, 16]       1,179,648
     BatchNorm2d-107          [-1, 512, 16, 16]           1,024
     BatchNorm2d-108          [-1, 512, 16, 16]           1,024
            ReLU-109          [-1, 512, 16, 16]               0
            ReLU-110          [-1, 512, 16, 16]               0
          Conv2d-111          [-1, 512, 16, 16]       2,359,296
          Conv2d-112          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-113          [-1, 512, 16, 16]           1,024
     BatchNorm2d-114          [-1, 512, 16, 16]           1,024
          Conv2d-115          [-1, 512, 16, 16]         131,072
          Conv2d-116          [-1, 512, 16, 16]         131,072
     BatchNorm2d-117          [-1, 512, 16, 16]           1,024
     BatchNorm2d-118          [-1, 512, 16, 16]           1,024
            ReLU-119          [-1, 512, 16, 16]               0
            ReLU-120          [-1, 512, 16, 16]               0
      BasicBlock-121          [-1, 512, 16, 16]               0
      BasicBlock-122          [-1, 512, 16, 16]               0
          Conv2d-123          [-1, 512, 16, 16]       2,359,296
          Conv2d-124          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-125          [-1, 512, 16, 16]           1,024
     BatchNorm2d-126          [-1, 512, 16, 16]           1,024
            ReLU-127          [-1, 512, 16, 16]               0
            ReLU-128          [-1, 512, 16, 16]               0
          Conv2d-129          [-1, 512, 16, 16]       2,359,296
          Conv2d-130          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-131          [-1, 512, 16, 16]           1,024
     BatchNorm2d-132          [-1, 512, 16, 16]           1,024
            ReLU-133          [-1, 512, 16, 16]               0
            ReLU-134          [-1, 512, 16, 16]               0
      BasicBlock-135          [-1, 512, 16, 16]               0
      BasicBlock-136          [-1, 512, 16, 16]               0
          Conv2d-137          [-1, 512, 16, 16]         262,656
            ReLU-138          [-1, 512, 16, 16]               0
        Upsample-139          [-1, 512, 32, 32]               0
          Conv2d-140          [-1, 256, 32, 32]          65,792
            ReLU-141          [-1, 256, 32, 32]               0
          Conv2d-142          [-1, 512, 32, 32]       3,539,456
            ReLU-143          [-1, 512, 32, 32]               0
        Upsample-144          [-1, 512, 64, 64]               0
          Conv2d-145          [-1, 128, 64, 64]          16,512
            ReLU-146          [-1, 128, 64, 64]               0
          Conv2d-147          [-1, 256, 64, 64]       1,474,816
            ReLU-148          [-1, 256, 64, 64]               0
        Upsample-149        [-1, 256, 128, 128]               0
          Conv2d-150         [-1, 64, 128, 128]           4,160
            ReLU-151         [-1, 64, 128, 128]               0
          Conv2d-152        [-1, 256, 128, 128]         737,536
            ReLU-153        [-1, 256, 128, 128]               0
        Upsample-154        [-1, 256, 256, 256]               0
          Conv2d-155         [-1, 64, 256, 256]           4,160
            ReLU-156         [-1, 64, 256, 256]               0
          Conv2d-157        [-1, 128, 256, 256]         368,768
            ReLU-158        [-1, 128, 256, 256]               0
        Upsample-159        [-1, 128, 512, 512]               0
          Conv2d-160         [-1, 64, 512, 512]         110,656
            ReLU-161         [-1, 64, 512, 512]               0
          Conv2d-162          [-1, 1, 512, 512]              65
================================================================
Total params: 28,976,321
Trainable params: 28,976,321
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 2172.00
Params size (MB): 110.54
Estimated Total Size (MB): 2285.54
----------------------------------------------------------------
Start training now...
training: bce: 0.523415, dice: 0.980942, loss: 0.752178
training IoU in current batch 0 is 0.0013548387096774194
training IoU uptillnow 0 is 0.0004516129032258065
testing: bce: 0.030789, dice: 0.057702, loss: 0.044246
IoU in current test batch is 0.0
training: bce: 0.360439, dice: 0.652011, loss: 0.506225
training IoU in current batch 100 is 0.5809476685280742
training IoU uptillnow 100 is 0.0019217904529298736
testing: bce: 2.141433, dice: 3.873713, loss: 3.007573
IoU in current test batch is 0.5453659350202935
training: bce: 0.219069, dice: 0.435029, loss: 0.327049
training IoU in current batch 200 is 0.8732857609970957
training IoU uptillnow 200 is 0.0024139108925951033
testing: bce: 2.590166, dice: 5.143584, loss: 3.866875
IoU in current test batch is 0.722969346119198
training: bce: 0.170486, dice: 0.348922, loss: 0.259704
training IoU in current batch 300 is 0.75903486111481
training IoU uptillnow 300 is 0.0024525173082498974
testing: bce: 3.018597, dice: 6.177978, loss: 4.598288
IoU in current test batch is 0.745247142007985
training: bce: 0.138519, dice: 0.291739, loss: 0.215129
training IoU in current batch 400 is 0.8844557033284076
training IoU uptillnow 400 is 0.002576125380447269
testing: bce: 3.267425, dice: 6.881611, loss: 5.074518
IoU in current test batch is 0.7678561485751564
training: bce: 0.117482, dice: 0.254700, loss: 0.186091
training IoU in current batch 500 is 0.8939142426641469
training IoU uptillnow 500 is 0.0026566820195224297
testing: bce: 3.462252, dice: 7.506163, loss: 5.484208
IoU in current test batch is 0.8372303107344601
Epoch   511: reducing learning rate of group 0 to 9.9900e-04.
training: bce: 0.112757, dice: 0.243127, loss: 0.177942
training IoU in current batch 600 is 0.8005901999359473
training IoU uptillnow 600 is 0.0026586707017627062
testing: bce: 3.986299, dice: 8.595256, loss: 6.290778
IoU in current test batch is 0.7558253603424063
Epoch   612: reducing learning rate of group 0 to 9.9800e-04.
training: bce: 0.107574, dice: 0.223626, loss: 0.165600
training IoU in current batch 700 is 0.8812470779300672
training IoU uptillnow 700 is 0.0026984452464138024
testing: bce: 4.435865, dice: 9.221264, loss: 6.828564
IoU in current test batch is 0.8394076535702609
Epoch   713: reducing learning rate of group 0 to 9.9700e-04.
training: bce: 0.100653, dice: 0.212372, loss: 0.156513
training IoU in current batch 800 is 0.7465761414806
training IoU uptillnow 800 is 0.002672245732288317
testing: bce: 4.742536, dice: 10.006484, loss: 7.374510
IoU in current test batch is 0.8074875083160648
Epoch   814: reducing learning rate of group 0 to 9.9601e-04.
training: bce: 0.093831, dice: 0.202781, loss: 0.148306
training IoU in current batch 900 is 0.9155813103040873
training IoU uptillnow 900 is 0.0027143869052877966
testing: bce: 4.973040, dice: 10.747375, loss: 7.860208
IoU in current test batch is 0.7955869059534343
Epoch   915: reducing learning rate of group 0 to 9.9501e-04.
training: bce: 0.086901, dice: 0.191820, loss: 0.139361
training IoU in current batch 1000 is 0.8986717882265828
training IoU uptillnow 1000 is 0.0027424773870194795
testing: bce: 5.116961, dice: 11.294840, loss: 8.205901
IoU in current test batch is 0.8048358649507152
Epoch  1016: reducing learning rate of group 0 to 9.9401e-04.
training: bce: 0.081904, dice: 0.182126, loss: 0.132015
training IoU in current batch 1100 is 0.9165870873987959
training IoU uptillnow 1100 is 0.002770889094949528
testing: bce: 5.304487, dice: 11.795369, loss: 8.549928
IoU in current test batch is 0.8807955254825619
Epoch  1117: reducing learning rate of group 0 to 9.9302e-04.
training: bce: 0.077212, dice: 0.173700, loss: 0.125456
training IoU in current batch 1200 is 0.8185332743101668
training IoU uptillnow 1200 is 0.00276735496945003
testing: bce: 5.454832, dice: 12.271395, loss: 8.863114
IoU in current test batch is 0.8017156701138451
Epoch  1276: reducing learning rate of group 0 to 9.9203e-04.
training: bce: 0.072920, dice: 0.166354, loss: 0.119637
training IoU in current batch 1300 is 0.9012730579371265
training IoU uptillnow 1300 is 0.002785563159842579
testing: bce: 5.580554, dice: 12.730943, loss: 9.155748
IoU in current test batch is 0.8579550236369551
Epoch  1377: reducing learning rate of group 0 to 9.9104e-04.
training: bce: 0.069603, dice: 0.160729, loss: 0.115166
training IoU in current batch 1400 is 0.7627744254718193
training IoU uptillnow 1400 is 0.002768219709335571
testing: bce: 5.736123, dice: 13.245969, loss: 9.491046
IoU in current test batch is 0.8643189344186378
Epoch  1478: reducing learning rate of group 0 to 9.9004e-04.
training: bce: 0.066691, dice: 0.155369, loss: 0.111030
training IoU in current batch 1500 is 0.8897173782321106
training IoU uptillnow 1500 is 0.002781377929506888
testing: bce: 5.888402, dice: 13.718140, loss: 9.803271
IoU in current test batch is 0.8722662060010912
Epoch  1579: reducing learning rate of group 0 to 9.8905e-04.
training: bce: 0.063954, dice: 0.150198, loss: 0.107076
training IoU in current batch 1600 is 0.9141514461508647
training IoU uptillnow 1600 is 0.0027979796507850052
testing: bce: 6.022947, dice: 14.145134, loss: 10.084040
IoU in current test batch is 0.8690368668214914
Epoch  1680: reducing learning rate of group 0 to 9.8807e-04.
training: bce: 0.061352, dice: 0.145476, loss: 0.103414
training IoU in current batch 1700 is 0.8890200708382526
training IoU uptillnow 1700 is 0.002807704552921543
testing: bce: 6.138768, dice: 14.556165, loss: 10.347466
IoU in current test batch is 0.8350413348564966
training: bce: 0.059398, dice: 0.141987, loss: 0.100693
training IoU in current batch 1800 is 0.7340305547011176
training IoU uptillnow 1800 is 0.002787663684667731
testing: bce: 6.292690, dice: 15.042286, loss: 10.667488
IoU in current test batch is 0.8636870318471426
Epoch  1854: reducing learning rate of group 0 to 9.8708e-04.
training: bce: 0.057421, dice: 0.138249, loss: 0.097835
training IoU in current batch 1900 is 0.9121780207373956
training IoU uptillnow 1900 is 0.0028009687723999904
testing: bce: 6.421040, dice: 15.459474, loss: 10.940257
IoU in current test batch is 0.8683319075755189
Epoch  1955: reducing learning rate of group 0 to 9.8609e-04.
training: bce: 0.055733, dice: 0.135213, loss: 0.095473
training IoU in current batch 2000 is 0.8631305064660725
training IoU uptillnow 2000 is 0.0028047735158192935
testing: bce: 6.560095, dice: 15.915368, loss: 11.237731
IoU in current test batch is 0.8591164763742508
Epoch  2056: reducing learning rate of group 0 to 9.8510e-04.
training: bce: 0.054927, dice: 0.133526, loss: 0.094226
training IoU in current batch 2100 is 0.8186741815459809
training IoU uptillnow 2100 is 0.002801162874347009
testing: bce: 6.788372, dice: 16.502189, loss: 11.645281
IoU in current test batch is 0.8789248519323349
Epoch  2157: reducing learning rate of group 0 to 9.8412e-04.
training: bce: 0.053502, dice: 0.131014, loss: 0.092258
training IoU in current batch 2200 is 0.885040466452006
training IoU uptillnow 2200 is 0.002807931252985189
testing: bce: 6.926967, dice: 16.962413, loss: 11.944690
IoU in current test batch is 0.8776446246780316
Epoch  2258: reducing learning rate of group 0 to 9.8314e-04.
training: bce: 0.051953, dice: 0.128281, loss: 0.090117
training IoU in current batch 2300 is 0.9056566455696202
training IoU uptillnow 2300 is 0.0028170978862857923
testing: bce: 7.032042, dice: 17.363210, loss: 12.197626
IoU in current test batch is 0.8530699792072236
Epoch  2374: reducing learning rate of group 0 to 9.8215e-04.
training: bce: 0.050868, dice: 0.126359, loss: 0.088614
training IoU in current batch 2400 is 0.32333586433814226
training IoU uptillnow 2400 is 0.002744656750433009
testing: bce: 7.184372, dice: 17.846348, loss: 12.515360
IoU in current test batch is 0.881818810572567
Epoch  2475: reducing learning rate of group 0 to 9.8117e-04.
training: bce: 0.049607, dice: 0.124333, loss: 0.086970
training IoU in current batch 2500 is 0.8818885544019168
training IoU uptillnow 2500 is 0.002752452502701704
testing: bce: 7.298101, dice: 18.291534, loss: 12.794818
IoU in current test batch is 0.8689427957868389
Epoch  2576: reducing learning rate of group 0 to 9.8019e-04.
training: bce: 0.048388, dice: 0.122272, loss: 0.085330
training IoU in current batch 2600 is 0.932234012774512
training IoU uptillnow 2600 is 0.002766100876655824
testing: bce: 7.403312, dice: 18.707548, loss: 13.055430
IoU in current test batch is 0.8541778251543398
Epoch  2677: reducing learning rate of group 0 to 9.7921e-04.
training: bce: 0.047609, dice: 0.120538, loss: 0.084073
training IoU in current batch 2700 is 0.9057747924246665
training IoU uptillnow 2700 is 0.0027754732732284413
testing: bce: 7.564169, dice: 19.151325, loss: 13.357747
IoU in current test batch is 0.8894299893021618
Epoch  2778: reducing learning rate of group 0 to 9.7823e-04.
training: bce: 0.046782, dice: 0.118666, loss: 0.082724
training IoU in current batch 2800 is 0.8427271222902532
training IoU uptillnow 2800 is 0.002776673456534608
testing: bce: 7.707987, dice: 19.552012, loss: 13.629999
IoU in current test batch is 0.8565242368373442
Epoch  2879: reducing learning rate of group 0 to 9.7725e-04.
training: bce: 0.045729, dice: 0.116681, loss: 0.081205
training IoU in current batch 2900 is 0.786049716706651
training IoU uptillnow 2900 is 0.0027712784984450147
testing: bce: 7.803537, dice: 19.911235, loss: 13.857386
IoU in current test batch is 0.8848363190757386
Epoch  2980: reducing learning rate of group 0 to 9.7627e-04.
training: bce: 0.044956, dice: 0.115081, loss: 0.080018
training IoU in current batch 3000 is 0.8893242170494994
training IoU uptillnow 3000 is 0.002777714205155666
testing: bce: 7.936115, dice: 20.315105, loss: 14.125610
IoU in current test batch is 0.8929356161914747
Epoch  3081: reducing learning rate of group 0 to 9.7530e-04.
training: bce: 0.044227, dice: 0.113826, loss: 0.079026
training IoU in current batch 3100 is 0.8554855515841764
training IoU uptillnow 3100 is 0.00278009744604973
testing: bce: 8.067445, dice: 20.763240, loss: 14.415343
IoU in current test batch is 0.8486013427481992
Epoch  3182: reducing learning rate of group 0 to 9.7432e-04.
training: bce: 0.043391, dice: 0.112454, loss: 0.077923
training IoU in current batch 3200 is 0.8904302845050552
training IoU uptillnow 3200 is 0.0027859707200984787
testing: bce: 8.170254, dice: 21.174502, loss: 14.672378
IoU in current test batch is 0.8960024149407407
Epoch  3283: reducing learning rate of group 0 to 9.7335e-04.
training: bce: 0.042663, dice: 0.111172, loss: 0.076917
training IoU in current batch 3300 is 0.9144591464550997
training IoU uptillnow 3300 is 0.0027939145684702407
testing: bce: 8.284145, dice: 21.586936, loss: 14.935540
IoU in current test batch is 0.8809105711907664
Epoch  3384: reducing learning rate of group 0 to 9.7237e-04.
training: bce: 0.041927, dice: 0.109803, loss: 0.075865
training IoU in current batch 3400 is 0.9160085245079603
training IoU uptillnow 3400 is 0.0028015431241859017
testing: bce: 8.387953, dice: 21.967002, loss: 15.177477
IoU in current test batch is 0.8728336880890276
Epoch  3485: reducing learning rate of group 0 to 9.7140e-04.
training: bce: 0.041194, dice: 0.108456, loss: 0.074825
training IoU in current batch 3500 is 0.9217761587121864
training IoU uptillnow 3500 is 0.002809285028542411
testing: bce: 8.483630, dice: 22.335537, loss: 15.409583
IoU in current test batch is 0.8743596444766543
Epoch  3586: reducing learning rate of group 0 to 9.7043e-04.
training: bce: 0.040558, dice: 0.107212, loss: 0.073885
training IoU in current batch 3600 is 0.888451784901489
training IoU uptillnow 3600 is 0.002813512213244694
testing: bce: 8.591161, dice: 22.709990, loss: 15.650575
IoU in current test batch is 0.8564921997965285
Epoch  3687: reducing learning rate of group 0 to 9.6946e-04.
training: bce: 0.039993, dice: 0.106112, loss: 0.073053
training IoU in current batch 3700 is 0.9120962103202349
training IoU uptillnow 3700 is 0.002819640516076976
testing: bce: 8.706816, dice: 23.101103, loss: 15.903959
IoU in current test batch is 0.8874761200492117
Epoch  3792: reducing learning rate of group 0 to 9.6849e-04.
training: bce: 0.039358, dice: 0.104958, loss: 0.072158
training IoU in current batch 3800 is 0.9434155808150818
training IoU uptillnow 3800 is 0.0028281929519264884
testing: bce: 8.800077, dice: 23.467280, loss: 16.133678
IoU in current test batch is 0.8337763407138208
Epoch  3893: reducing learning rate of group 0 to 9.6752e-04.
training: bce: 0.038794, dice: 0.103887, loss: 0.071340
training IoU in current batch 3900 is 0.9059870161096417
training IoU uptillnow 3900 is 0.0028331087111789616
testing: bce: 8.902159, dice: 23.838931, loss: 16.370545
IoU in current test batch is 0.9119867923932747
Epoch  3994: reducing learning rate of group 0 to 9.6656e-04.
training: bce: 0.038218, dice: 0.102814, loss: 0.070516
training IoU in current batch 4000 is 0.9035976352455364
training IoU uptillnow 4000 is 0.002837579678594762
testing: bce: 8.994661, dice: 24.197523, loss: 16.596092
IoU in current test batch is 0.8909796537731256
Epoch  4095: reducing learning rate of group 0 to 9.6559e-04.
training: bce: 0.037665, dice: 0.101807, loss: 0.069736
training IoU in current batch 4100 is 0.8493277700509968
training IoU uptillnow 4100 is 0.002837421494938139
testing: bce: 9.086153, dice: 24.559337, loss: 16.822745
IoU in current test batch is 0.9045399764058681
Epoch  4196: reducing learning rate of group 0 to 9.6462e-04.
training: bce: 0.037188, dice: 0.100855, loss: 0.069022
training IoU in current batch 4200 is 0.9505565749235474
training IoU uptillnow 4200 is 0.002845302961766839
testing: bce: 9.189789, dice: 24.923131, loss: 17.056460
IoU in current test batch is 0.8947300910135288
Epoch  4297: reducing learning rate of group 0 to 9.6366e-04.
training: bce: 0.036720, dice: 0.099958, loss: 0.068339
training IoU in current batch 4300 is 0.9205353176366059
training IoU uptillnow 4300 is 0.002850491245817568
testing: bce: 9.290144, dice: 25.289417, loss: 17.289780
IoU in current test batch is 0.8877473593506591
Epoch  4398: reducing learning rate of group 0 to 9.6269e-04.
training: bce: 0.036410, dice: 0.099443, loss: 0.067926
training IoU in current batch 4400 is 0.9296470924417773
training IoU uptillnow 4400 is 0.00285613388148344
testing: bce: 9.425862, dice: 25.743936, loss: 17.584899
IoU in current test batch is 0.8546366725817275
Epoch  4499: reducing learning rate of group 0 to 9.6173e-04.
training: bce: 0.035939, dice: 0.098557, loss: 0.067248
training IoU in current batch 4500 is 0.920777713290772
training IoU uptillnow 4500 is 0.0028608689439766446
testing: bce: 9.515351, dice: 26.094517, loss: 17.804934
IoU in current test batch is 0.8979867372035077
Epoch  4600: reducing learning rate of group 0 to 9.6077e-04.
training: bce: 0.035761, dice: 0.098027, loss: 0.066894
training IoU in current batch 4600 is 0.8784857238084657
training IoU uptillnow 4600 is 0.002862334208094262
testing: bce: 9.678593, dice: 26.530703, loss: 18.104648
IoU in current test batch is 0.8822118842736428
training: bce: 0.035429, dice: 0.097433, loss: 0.066431
training IoU in current batch 4700 is 0.9029031610924827
training IoU uptillnow 4700 is 0.0028654684985760176
testing: bce: 9.797153, dice: 26.943063, loss: 18.370108
IoU in current test batch is 0.8851451032215305
Epoch  4701: reducing learning rate of group 0 to 9.5981e-04.
training: bce: 0.035126, dice: 0.096811, loss: 0.065969
training IoU in current batch 4800 is 0.8945565283593453
training IoU uptillnow 4800 is 0.0028678927142801445
testing: bce: 9.919989, dice: 27.340588, loss: 18.630289
IoU in current test batch is 0.87282825615475
Epoch  4802: reducing learning rate of group 0 to 9.5885e-04.
training: bce: 0.034845, dice: 0.096270, loss: 0.065558
training IoU in current batch 4900 is 0.9127999528329698
training IoU uptillnow 4900 is 0.002871458798654009
testing: bce: 10.045538, dice: 27.754148, loss: 18.899843
IoU in current test batch is 0.8847164514870455
Epoch  4903: reducing learning rate of group 0 to 9.5789e-04.
training: bce: 0.034478, dice: 0.095531, loss: 0.065005
training IoU in current batch 5000 is 0.8978328880363545
training IoU uptillnow 5000 is 0.0028738846633770744
testing: bce: 10.142647, dice: 28.102969, loss: 19.122808
IoU in current test batch is 0.8901900699718248
Epoch  5004: reducing learning rate of group 0 to 9.5693e-04.
training: bce: 0.034158, dice: 0.094956, loss: 0.064557
training IoU in current batch 5100 is 0.923110191220537
training IoU uptillnow 5100 is 0.0028778672022392203
testing: bce: 10.249348, dice: 28.492346, loss: 19.370847
IoU in current test batch is 0.8733448172094214
Epoch  5105: reducing learning rate of group 0 to 9.5598e-04.
training: bce: 0.033927, dice: 0.094329, loss: 0.064128
training IoU in current batch 5200 is 0.885787743195471
training IoU uptillnow 5200 is 0.0028793045913646257
testing: bce: 10.379749, dice: 28.859261, loss: 19.619505
IoU in current test batch is 0.8954818891273336
Epoch  5206: reducing learning rate of group 0 to 9.5502e-04.
training: bce: 0.033647, dice: 0.093732, loss: 0.063690
training IoU in current batch 5300 is 0.9319360738808432
training IoU uptillnow 5300 is 0.0028835896128367668
testing: bce: 10.491984, dice: 29.227860, loss: 19.859922
IoU in current test batch is 0.8917678276966551
Epoch  5307: reducing learning rate of group 0 to 9.5406e-04.
training: bce: 0.033405, dice: 0.093249, loss: 0.063327
training IoU in current batch 5400 is 0.9264164094394105
training IoU uptillnow 5400 is 0.00288737530225159
testing: bce: 10.612952, dice: 29.625773, loss: 20.119363
IoU in current test batch is 0.9096412463190886
Epoch  5408: reducing learning rate of group 0 to 9.5311e-04.
training: bce: 0.033087, dice: 0.092650, loss: 0.062869
training IoU in current batch 5500 is 0.944788967049241
training IoU uptillnow 5500 is 0.0028921366411823156
testing: bce: 10.706583, dice: 29.980588, loss: 20.343586
IoU in current test batch is 0.9040974975844643
Epoch  5509: reducing learning rate of group 0 to 9.5216e-04.
training: bce: 0.032851, dice: 0.092088, loss: 0.062470
training IoU in current batch 5600 is 0.9132776815246495
training IoU uptillnow 5600 is 0.0028948526257785157
testing: bce: 10.823587, dice: 30.340289, loss: 20.581938
IoU in current test batch is 0.8998091565735793
Epoch  5610: reducing learning rate of group 0 to 9.5121e-04.
training: bce: 0.032802, dice: 0.091672, loss: 0.062237
training IoU in current batch 5700 is 0.9400289904921013
training IoU uptillnow 5700 is 0.0028990374590100276
testing: bce: 11.000317, dice: 30.742568, loss: 20.871442
IoU in current test batch is 0.8972452690057764
Epoch  5711: reducing learning rate of group 0 to 9.5025e-04.
training: bce: 0.032699, dice: 0.091590, loss: 0.062144
training IoU in current batch 5800 is 0.8951080390008148
training IoU uptillnow 5800 is 0.0029004967936820847
testing: bce: 11.158069, dice: 31.253720, loss: 21.205895
IoU in current test batch is 0.8798206241383073
Epoch  5812: reducing learning rate of group 0 to 9.4930e-04.
training: bce: 0.032996, dice: 0.091332, loss: 0.062164
training IoU in current batch 5900 is 0.9009432474210542
training IoU uptillnow 5900 is 0.0029022362846901867
testing: bce: 11.453503, dice: 31.702782, loss: 21.578143
IoU in current test batch is 0.8565750437986227
Epoch  5913: reducing learning rate of group 0 to 9.4835e-04.
training: bce: 0.032820, dice: 0.090991, loss: 0.061905
training IoU in current batch 6000 is 0.9314994727276806
training IoU uptillnow 6000 is 0.002905615087518639
testing: bce: 11.585314, dice: 32.119961, loss: 21.852638
IoU in current test batch is 0.8663999899845701
Epoch  6014: reducing learning rate of group 0 to 9.4741e-04.
training: bce: 0.032545, dice: 0.090527, loss: 0.061536
training IoU in current batch 6100 is 0.7995267260579064
training IoU uptillnow 6100 is 0.002901672684623065
testing: bce: 11.679988, dice: 32.488395, loss: 22.084191
IoU in current test batch is 0.872570001336975
Epoch  6115: reducing learning rate of group 0 to 9.4646e-04.
training: bce: 0.032402, dice: 0.090144, loss: 0.061273
training IoU in current batch 6200 is 0.8779246837937583
training IoU uptillnow 6200 is 0.0029020716997500253
testing: bce: 11.819140, dice: 32.881188, loss: 22.350164
IoU in current test batch is 0.8894360845026745
Epoch  6216: reducing learning rate of group 0 to 9.4551e-04.
training: bce: 0.032183, dice: 0.089759, loss: 0.060971
training IoU in current batch 6300 is 0.9434650175373823
training IoU uptillnow 6300 is 0.0029059252419185895
testing: bce: 11.928469, dice: 33.269001, loss: 22.598735
IoU in current test batch is 0.8643074828968956
Epoch  6317: reducing learning rate of group 0 to 9.4457e-04.
training: bce: 0.031922, dice: 0.089260, loss: 0.060591
training IoU in current batch 6400 is 0.9046530491851633
training IoU uptillnow 6400 is 0.002907637238825822
testing: bce: 12.019721, dice: 33.609008, loss: 22.814365
IoU in current test batch is 0.8925240665991954
Epoch  6418: reducing learning rate of group 0 to 9.4362e-04.
training: bce: 0.031926, dice: 0.089181, loss: 0.060554
training IoU in current batch 6500 is 0.9390541328667621
training IoU uptillnow 6500 is 0.0029110604537783435
testing: bce: 12.208850, dice: 34.104017, loss: 23.156434
IoU in current test batch is 0.8878329564484385
Epoch  6554: reducing learning rate of group 0 to 9.4268e-04.
training: bce: 0.031698, dice: 0.088830, loss: 0.060264
training IoU in current batch 6600 is 0.9163828752128436
training IoU uptillnow 6600 is 0.002913235111106998
testing: bce: 12.308317, dice: 34.492067, loss: 23.400192
IoU in current test batch is 0.88462696613139
Epoch  6655: reducing learning rate of group 0 to 9.4174e-04.
training: bce: 0.031541, dice: 0.088506, loss: 0.060023
training IoU in current batch 6700 is 0.8752497225305217
training IoU uptillnow 6700 is 0.0029132987428633736
testing: bce: 12.432656, dice: 34.887004, loss: 23.659830
IoU in current test batch is 0.8878479747211987
Epoch  6756: reducing learning rate of group 0 to 9.4079e-04.
training: bce: 0.031404, dice: 0.088260, loss: 0.059832
training IoU in current batch 6800 is 0.8332379862700229
training IoU uptillnow 6800 is 0.0029113014073446266
testing: bce: 12.563544, dice: 35.309332, loss: 23.936438
IoU in current test batch is 0.8661646546966222
Epoch  6857: reducing learning rate of group 0 to 9.3985e-04.
training: bce: 0.031156, dice: 0.087806, loss: 0.059481
training IoU in current batch 6900 is 0.8982758620689655
training IoU uptillnow 6900 is 0.0029125034283012793
testing: bce: 12.647529, dice: 35.643929, loss: 24.145729
IoU in current test batch is 0.8939324024471736
Epoch  6958: reducing learning rate of group 0 to 9.3891e-04.
training: bce: 0.030931, dice: 0.087420, loss: 0.059175
training IoU in current batch 7000 is 0.9472913985819897
training IoU uptillnow 7000 is 0.002916004850483425
testing: bce: 12.737960, dice: 36.001441, loss: 24.369701
IoU in current test batch is 0.8889894634770605
training: bce: 0.030713, dice: 0.086991, loss: 0.058852
training IoU in current batch 7100 is 0.9398821663267618
training IoU uptillnow 7100 is 0.002919059852651276
testing: bce: 12.828828, dice: 36.336675, loss: 24.582751
IoU in current test batch is 0.8624891363126985
Epoch  7145: reducing learning rate of group 0 to 9.3797e-04.
training: bce: 0.030506, dice: 0.086564, loss: 0.058535
training IoU in current batch 7200 is 0.8992549760925164
training IoU uptillnow 7200 is 0.002920149378193892
testing: bce: 12.921850, dice: 36.667421, loss: 24.794635
IoU in current test batch is 0.8351016203554376
Epoch  7246: reducing learning rate of group 0 to 9.3704e-04.
training: bce: 0.030275, dice: 0.086094, loss: 0.058184
training IoU in current batch 7300 is 0.930394361062162
training IoU uptillnow 7300 is 0.00292263075278203
testing: bce: 13.002095, dice: 36.974672, loss: 24.988384
IoU in current test batch is 0.8563339146177139
Epoch  7347: reducing learning rate of group 0 to 9.3610e-04.
training: bce: 0.030215, dice: 0.085774, loss: 0.057995
training IoU in current batch 7400 is 0.8881213669421056
training IoU uptillnow 7400 is 0.002923141140617345
testing: bce: 13.154379, dice: 37.341829, loss: 25.248104
IoU in current test batch is 0.8915948279616052
Epoch  7448: reducing learning rate of group 0 to 9.3516e-04.
training: bce: 0.030000, dice: 0.085363, loss: 0.057682
training IoU in current batch 7500 is 0.9385082508250825
training IoU uptillnow 7500 is 0.0029258770384371856
testing: bce: 13.237045, dice: 37.665273, loss: 25.451159
IoU in current test batch is 0.891204501260245
Epoch  7549: reducing learning rate of group 0 to 9.3423e-04.
training: bce: 0.029828, dice: 0.085028, loss: 0.057428
training IoU in current batch 7600 is 0.852393202369816
training IoU uptillnow 7600 is 0.0029247644695137394
testing: bce: 13.336476, dice: 38.017721, loss: 25.677098
IoU in current test batch is 0.8940076386080825
Epoch  7650: reducing learning rate of group 0 to 9.3329e-04.
training: bce: 0.029824, dice: 0.084960, loss: 0.057392
training IoU in current batch 7700 is 0.9099531232966315
training IoU uptillnow 7700 is 0.0029261722426359533
testing: bce: 13.510197, dice: 38.486668, loss: 25.998433
IoU in current test batch is 0.8833045769866763
Epoch  7751: reducing learning rate of group 0 to 9.3236e-04.
training: bce: 0.029646, dice: 0.084686, loss: 0.057166
training IoU in current batch 7800 is 0.8728630379249361
training IoU uptillnow 7800 is 0.0029259590804402583
testing: bce: 13.603865, dice: 38.860797, loss: 26.232331
IoU in current test batch is 0.8888400700020416
Epoch  7852: reducing learning rate of group 0 to 9.3143e-04.
training: bce: 0.029637, dice: 0.084429, loss: 0.057033
training IoU in current batch 7900 is 0.9252029608404967
training IoU uptillnow 7900 is 0.0029279594701254632
testing: bce: 13.774110, dice: 39.239645, loss: 26.506878
IoU in current test batch is 0.8838331008392205
Epoch  7953: reducing learning rate of group 0 to 9.3050e-04.
training: bce: 0.029505, dice: 0.084158, loss: 0.056832
training IoU in current batch 8000 is 0.8954511599680008
training IoU uptillnow 8000 is 0.002928670352887217
testing: bce: 13.886645, dice: 39.608649, loss: 26.747647
IoU in current test batch is 0.8867508212437789
Epoch  8054: reducing learning rate of group 0 to 9.2957e-04.
training: bce: 0.029450, dice: 0.083987, loss: 0.056719
training IoU in current batch 8100 is 0.9114579396870984
training IoU uptillnow 8100 is 0.002930022319056864
testing: bce: 14.033872, dice: 40.022203, loss: 27.028037
IoU in current test batch is 0.9051654473404991
training: bce: 0.029336, dice: 0.083705, loss: 0.056521
training IoU in current batch 8200 is 0.9535857086071036
training IoU uptillnow 8200 is 0.002933053616577087
testing: bce: 14.152075, dice: 40.380255, loss: 27.266165
IoU in current test batch is 0.8809951361913091
Epoch  8232: reducing learning rate of group 0 to 9.2864e-04.
training: bce: 0.029156, dice: 0.083368, loss: 0.056262
training IoU in current batch 8300 is 0.9246148519993076
training IoU uptillnow 8300 is 0.0029348485315281443
testing: bce: 14.236935, dice: 40.708310, loss: 27.472622
IoU in current test batch is 0.9046967792908336
Epoch  8333: reducing learning rate of group 0 to 9.2771e-04.
training: bce: 0.029037, dice: 0.083192, loss: 0.056114
training IoU in current batch 8400 is 0.9078554748313441
training IoU uptillnow 8400 is 0.002935935740010186
testing: bce: 14.349403, dice: 41.111391, loss: 27.730397
IoU in current test batch is 0.8947659912182025
Epoch  8434: reducing learning rate of group 0 to 9.2678e-04.
training: bce: 0.028876, dice: 0.082923, loss: 0.055899
training IoU in current batch 8500 is 0.8912064470253614
training IoU uptillnow 8500 is 0.002936344543877273
testing: bce: 14.439451, dice: 41.466148, loss: 27.952799
IoU in current test batch is 0.9026299852351416
Epoch  8535: reducing learning rate of group 0 to 9.2585e-04.
training: bce: 0.028751, dice: 0.082693, loss: 0.055722
training IoU in current batch 8600 is 0.9192786847943569
training IoU uptillnow 8600 is 0.0029378317865091834
testing: bce: 14.546397, dice: 41.837552, loss: 28.191974
IoU in current test batch is 0.9000937915103239
Epoch  8636: reducing learning rate of group 0 to 9.2493e-04.
training: bce: 0.028620, dice: 0.082486, loss: 0.055553
training IoU in current batch 8700 is 0.9534055727554179
training IoU uptillnow 8700 is 0.0029405922369096222
testing: bce: 14.648451, dice: 42.218152, loss: 28.433301
IoU in current test batch is 0.905046183431432
Epoch  8737: reducing learning rate of group 0 to 9.2400e-04.
training: bce: 0.028482, dice: 0.082222, loss: 0.055352
training IoU in current batch 8800 is 0.8970572574633489
training IoU uptillnow 8800 is 0.0029411557935657014
testing: bce: 14.745148, dice: 42.566797, loss: 28.655973
IoU in current test batch is 0.8916930164355451
Epoch  8838: reducing learning rate of group 0 to 9.2308e-04.
training: bce: 0.028327, dice: 0.081933, loss: 0.055130
training IoU in current batch 8900 is 0.857826662592932
training IoU uptillnow 8900 is 0.00294023754185328
testing: bce: 14.831615, dice: 42.899234, loss: 28.865425
IoU in current test batch is 0.9097316280346145
Epoch  8939: reducing learning rate of group 0 to 9.2216e-04.
training: bce: 0.028183, dice: 0.081697, loss: 0.054940
training IoU in current batch 9000 is 0.8965311183483223
training IoU uptillnow 9000 is 0.002940773032568843
testing: bce: 14.922057, dice: 43.256120, loss: 29.089088
IoU in current test batch is 0.8742908499378541
Epoch  9040: reducing learning rate of group 0 to 9.2123e-04.
training: bce: 0.028068, dice: 0.081488, loss: 0.054778
training IoU in current batch 9100 is 0.9379548913795489
training IoU uptillnow 9100 is 0.0029428139431504235
testing: bce: 15.026252, dice: 43.624765, loss: 29.325509
IoU in current test batch is 0.9052303706241981
Epoch  9141: reducing learning rate of group 0 to 9.2031e-04.
training: bce: 0.027961, dice: 0.081327, loss: 0.054644
training IoU in current batch 9200 is 0.8580427151340179
training IoU uptillnow 9200 is 0.002941915436907946
testing: bce: 15.133448, dice: 44.016932, loss: 29.575190
IoU in current test batch is 0.8896357146200661
Epoch  9242: reducing learning rate of group 0 to 9.1939e-04.
training: bce: 0.027870, dice: 0.081191, loss: 0.054530
training IoU in current batch 9300 is 0.9108530093476987
training IoU uptillnow 9300 is 0.00294292888987986
testing: bce: 15.247980, dice: 44.420871, loss: 29.834426
IoU in current test batch is 0.9036319995833244
Epoch  9343: reducing learning rate of group 0 to 9.1847e-04.
training: bce: 0.027726, dice: 0.080933, loss: 0.054330
training IoU in current batch 9400 is 0.9406861932611938
training IoU uptillnow 9400 is 0.0029449785841073267
testing: bce: 15.332505, dice: 44.756171, loss: 30.044338
IoU in current test batch is 0.8606074016680667
Epoch  9444: reducing learning rate of group 0 to 9.1755e-04.
training: bce: 0.027630, dice: 0.080748, loss: 0.054189
training IoU in current batch 9500 is 0.9047758656073177
training IoU uptillnow 9500 is 0.0029457252525413556
testing: bce: 15.441850, dice: 45.128605, loss: 30.285227
IoU in current test batch is 0.8729872315243177
Epoch  9545: reducing learning rate of group 0 to 9.1664e-04.
training: bce: 0.027503, dice: 0.080508, loss: 0.054005
training IoU in current batch 9600 is 0.8431944812947731
training IoU uptillnow 9600 is 0.0029443183472027575
testing: bce: 15.532465, dice: 45.468316, loss: 30.500391
IoU in current test batch is 0.8625928367495869
Epoch  9646: reducing learning rate of group 0 to 9.1572e-04.
training: bce: 0.027394, dice: 0.080286, loss: 0.053840
training IoU in current batch 9700 is 0.8900637365066499
training IoU uptillnow 9700 is 0.002944550908531343
testing: bce: 15.632075, dice: 45.814946, loss: 30.723511
IoU in current test batch is 0.9008634118241445
Epoch  9747: reducing learning rate of group 0 to 9.1480e-04.
training: bce: 0.027314, dice: 0.080101, loss: 0.053707
training IoU in current batch 9800 is 0.8915488969547198
training IoU uptillnow 9800 is 0.0029448292347019826
testing: bce: 15.747245, dice: 46.180497, loss: 30.963871
IoU in current test batch is 0.8858904824701918
Epoch  9848: reducing learning rate of group 0 to 9.1389e-04.
training: bce: 0.027516, dice: 0.080048, loss: 0.053782
training IoU in current batch 9900 is 0.9261080657791699
training IoU uptillnow 9900 is 0.0029462654295431966
testing: bce: 16.025501, dice: 46.621056, loss: 31.323278
IoU in current test batch is 0.8884228875349434
Epoch  9949: reducing learning rate of group 0 to 9.1298e-04.
training: bce: 0.027388, dice: 0.079814, loss: 0.053601
training IoU in current batch 10000 is 0.936049754573851
training IoU uptillnow 10000 is 0.002948004259850529
testing: bce: 16.112097, dice: 46.954359, loss: 31.533228
IoU in current test batch is 0.8858812056217258
Epoch 10050: reducing learning rate of group 0 to 9.1206e-04.
training: bce: 0.027315, dice: 0.079681, loss: 0.053498
training IoU in current batch 10100 is 0.9416494897829351
training IoU uptillnow 10100 is 0.0029498934527300382
testing: bce: 16.229691, dice: 47.344437, loss: 31.787064
IoU in current test batch is 0.8826132144541093
Epoch 10151: reducing learning rate of group 0 to 9.1115e-04.
training: bce: 0.027212, dice: 0.079486, loss: 0.053349
training IoU in current batch 10200 is 0.928745417415053
training IoU uptillnow 10200 is 0.002951323945871104
testing: bce: 16.328873, dice: 47.696227, loss: 32.012550
IoU in current test batch is 0.8449446938029676
Epoch 10252: reducing learning rate of group 0 to 9.1024e-04.
training: bce: 0.027141, dice: 0.079374, loss: 0.053258
training IoU in current batch 10300 is 0.9298977931923513
training IoU uptillnow 10300 is 0.0029527639552368945
testing: bce: 16.446098, dice: 48.095880, loss: 32.270989
IoU in current test batch is 0.8977914063703474
Epoch 10353: reducing learning rate of group 0 to 9.0933e-04.
training: bce: 0.027073, dice: 0.079208, loss: 0.053140
training IoU in current batch 10400 is 0.9391275877701103
training IoU uptillnow 10400 is 0.002954472073084507
testing: bce: 16.563677, dice: 48.461144, loss: 32.512410
IoU in current test batch is 0.9056999990904557
Epoch 10454: reducing learning rate of group 0 to 9.0842e-04.
training: bce: 0.027202, dice: 0.079205, loss: 0.053204
training IoU in current batch 10500 is 0.9061865534059882
training IoU uptillnow 10500 is 0.00295510201091521
testing: bce: 16.802739, dice: 48.925683, loss: 32.864211
IoU in current test batch is 0.8897790166099585
Epoch 10555: reducing learning rate of group 0 to 9.0751e-04.
training: bce: 0.027074, dice: 0.078950, loss: 0.053012
training IoU in current batch 10600 is 0.8807399064286952
training IoU uptillnow 10600 is 0.002954919930707498
testing: bce: 16.883017, dice: 49.232230, loss: 33.057623
IoU in current test batch is 0.8897844731397764
Epoch 10673: reducing learning rate of group 0 to 9.0660e-04.
training: bce: 0.027002, dice: 0.078857, loss: 0.052929
training IoU in current batch 10700 is 0.9125879497342305
training IoU uptillnow 10700 is 0.0029557333117161883
testing: bce: 16.996697, dice: 49.638313, loss: 33.317505
IoU in current test batch is 0.8813318216868404
Epoch 10774: reducing learning rate of group 0 to 9.0570e-04.
training: bce: 0.026921, dice: 0.078656, loss: 0.052788
training IoU in current batch 10800 is 0.9278381072997225
training IoU uptillnow 10800 is 0.0029570022718058363
testing: bce: 17.104098, dice: 49.974174, loss: 33.539136
IoU in current test batch is 0.9149488379734498
Epoch 10875: reducing learning rate of group 0 to 9.0479e-04.
training: bce: 0.026881, dice: 0.078551, loss: 0.052716
training IoU in current batch 10900 is 0.9317112827361942
training IoU uptillnow 10900 is 0.002958366385226453
testing: bce: 17.237070, dice: 50.369887, loss: 33.803478
IoU in current test batch is 0.8904921809991229
Epoch 10976: reducing learning rate of group 0 to 9.0389e-04.
training: bce: 0.026856, dice: 0.078447, loss: 0.052652
training IoU in current batch 11000 is 0.9030690366819786
training IoU uptillnow 11000 is 0.0029588378308863642
testing: bce: 17.378777, dice: 50.764692, loss: 34.071734
IoU in current test batch is 0.8824760818010609
Epoch 11077: reducing learning rate of group 0 to 9.0298e-04.
training: bce: 0.026749, dice: 0.078263, loss: 0.052506
training IoU in current batch 11100 is 0.9063011765891512
training IoU uptillnow 11100 is 0.0029593978353100875
testing: bce: 17.467112, dice: 51.106056, loss: 34.286584
IoU in current test batch is 0.8857135460056772
Epoch 11178: reducing learning rate of group 0 to 9.0208e-04.
training: bce: 0.026658, dice: 0.078100, loss: 0.052379
training IoU in current batch 11200 is 0.7209842509635755
training IoU uptillnow 11200 is 0.00295443294825746
testing: bce: 17.564424, dice: 51.459006, loss: 34.511715
IoU in current test batch is 0.9131100593810858
Epoch 11279: reducing learning rate of group 0 to 9.0118e-04.
training: bce: 0.026593, dice: 0.077930, loss: 0.052261
training IoU in current batch 11300 is 0.7841701250440773
training IoU uptillnow 11300 is 0.0029514196526956166
testing: bce: 17.677861, dice: 51.805062, loss: 34.741461
IoU in current test batch is 0.9124459739272203
Epoch 11380: reducing learning rate of group 0 to 9.0028e-04.
training: bce: 0.026489, dice: 0.077747, loss: 0.052118
training IoU in current batch 11400 is 0.9217079725291132
training IoU uptillnow 11400 is 0.0029524804390804495
testing: bce: 17.765078, dice: 52.141000, loss: 34.953039
IoU in current test batch is 0.9048474427864643
Epoch 11481: reducing learning rate of group 0 to 8.9938e-04.
training: bce: 0.026444, dice: 0.077663, loss: 0.052054
training IoU in current batch 11500 is 0.9299559995243192
training IoU uptillnow 11500 is 0.0029537618310695574
testing: bce: 17.890225, dice: 52.541242, loss: 35.215734
IoU in current test batch is 0.8935377917974004
Epoch 11582: reducing learning rate of group 0 to 8.9848e-04.
training: bce: 0.026342, dice: 0.077476, loss: 0.051909
training IoU in current batch 11600 is 0.9389954048486769
training IoU uptillnow 11600 is 0.0029552808626337273
testing: bce: 17.975807, dice: 52.870581, loss: 35.423194
IoU in current test batch is 0.8876867824231977
Epoch 11683: reducing learning rate of group 0 to 8.9758e-04.
training: bce: 0.026233, dice: 0.077269, loss: 0.051751
training IoU in current batch 11700 is 0.9204887430704831
training IoU uptillnow 11700 is 0.0029562467198049197
testing: bce: 18.056088, dice: 53.183902, loss: 35.619995
IoU in current test batch is 0.9012853712311658
Epoch 11784: reducing learning rate of group 0 to 8.9668e-04.
training: bce: 0.026268, dice: 0.077203, loss: 0.051736
training IoU in current batch 11800 is 0.9231843151537225
training IoU uptillnow 11800 is 0.0029572723475543264
testing: bce: 18.234858, dice: 53.592542, loss: 35.913700
IoU in current test batch is 0.8645508323094828
Epoch 11885: reducing learning rate of group 0 to 8.9578e-04.
training: bce: 0.026197, dice: 0.077074, loss: 0.051635
training IoU in current batch 11900 is 0.868967461087971
training IoU uptillnow 11900 is 0.002956762187534767
testing: bce: 18.339109, dice: 53.956624, loss: 36.147866
IoU in current test batch is 0.904064622425825
Epoch 11986: reducing learning rate of group 0 to 8.9489e-04.
training: bce: 0.026121, dice: 0.076941, loss: 0.051531
training IoU in current batch 12000 is 0.9180888395668996
training IoU uptillnow 12000 is 0.00295762489851181
testing: bce: 18.440122, dice: 54.315811, loss: 36.377966
IoU in current test batch is 0.9039084641902339
training: bce: 0.026061, dice: 0.076840, loss: 0.051451
training IoU in current batch 12100 is 0.9306025395891553
training IoU uptillnow 12100 is 0.0029588180525221015
testing: bce: 18.551068, dice: 54.696685, loss: 36.623877
IoU in current test batch is 0.8796304412286194
Epoch 12106: reducing learning rate of group 0 to 8.9399e-04.
training: bce: 0.025958, dice: 0.076652, loss: 0.051305
training IoU in current batch 12200 is 0.9325597600508982
training IoU uptillnow 12200 is 0.0029600451198197072
testing: bce: 18.630135, dice: 55.013845, loss: 36.821990
IoU in current test batch is 0.8845710657817888
Epoch 12207: reducing learning rate of group 0 to 8.9310e-04.
training: bce: 0.025894, dice: 0.076522, loss: 0.051208
training IoU in current batch 12300 is 0.9308762494885134
training IoU uptillnow 12300 is 0.002961206616541996
testing: bce: 18.736667, dice: 55.370662, loss: 37.053664
IoU in current test batch is 0.8728824247320757
Epoch 12308: reducing learning rate of group 0 to 8.9221e-04.
training: bce: 0.025813, dice: 0.076364, loss: 0.051089
training IoU in current batch 12400 is 0.9284496425473363
training IoU uptillnow 12400 is 0.002962284154847636
testing: bce: 18.830108, dice: 55.705179, loss: 37.267644
IoU in current test batch is 0.9055047700305101
Epoch 12409: reducing learning rate of group 0 to 8.9131e-04.
training: bce: 0.025732, dice: 0.076204, loss: 0.050968
training IoU in current batch 12500 is 0.9044445653356544
training IoU uptillnow 12500 is 0.002962704369733948
testing: bce: 18.922391, dice: 56.037174, loss: 37.479783
IoU in current test batch is 0.8961949452123055
Epoch 12510: reducing learning rate of group 0 to 8.9042e-04.
training: bce: 0.025657, dice: 0.076058, loss: 0.050857
training IoU in current batch 12600 is 0.8676551546959298
training IoU uptillnow 12600 is 0.0029621447274774007
testing: bce: 19.017782, dice: 56.376705, loss: 37.697243
IoU in current test batch is 0.8939130371017192
Epoch 12611: reducing learning rate of group 0 to 8.8953e-04.
training: bce: 0.025611, dice: 0.075972, loss: 0.050792
training IoU in current batch 12700 is 0.8649113443633991
training IoU uptillnow 12700 is 0.0029615218874417125
testing: bce: 19.134770, dice: 56.760036, loss: 37.947403
IoU in current test batch is 0.8921837008544246
Epoch 12712: reducing learning rate of group 0 to 8.8864e-04.
training: bce: 0.025576, dice: 0.075854, loss: 0.050715
training IoU in current batch 12800 is 0.9101174990708711
training IoU uptillnow 12800 is 0.00296208593016854
testing: bce: 19.259104, dice: 57.118195, loss: 38.188649
IoU in current test batch is 0.8941783399040792
Epoch 12813: reducing learning rate of group 0 to 8.8775e-04.
training: bce: 0.025489, dice: 0.075681, loss: 0.050585
training IoU in current batch 12900 is 0.9176508358749742
training IoU uptillnow 12900 is 0.002962835873501729
testing: bce: 19.342858, dice: 57.432817, loss: 38.387838
IoU in current test batch is 0.8661732708761244
Epoch 12914: reducing learning rate of group 0 to 8.8687e-04.
training: bce: 0.025410, dice: 0.075562, loss: 0.050486
training IoU in current batch 13000 is 0.9108316391659969
training IoU uptillnow 13000 is 0.0029633994423840064
testing: bce: 19.432483, dice: 57.786999, loss: 38.609741
IoU in current test batch is 0.9076827657407831
training: bce: 0.025369, dice: 0.075455, loss: 0.050412
training IoU in current batch 13100 is 0.9370983145342384
training IoU uptillnow 13100 is 0.002964622720042685
testing: bce: 19.550174, dice: 58.149066, loss: 38.849620
IoU in current test batch is 0.8400327263535227
Epoch 13107: reducing learning rate of group 0 to 8.8598e-04.
training: bce: 0.025282, dice: 0.075289, loss: 0.050285
training IoU in current batch 13200 is 0.9274575242718447
training IoU uptillnow 13200 is 0.0029655840287379615
testing: bce: 19.632286, dice: 58.464003, loss: 39.048144
IoU in current test batch is 0.8909778782170535
Epoch 13208: reducing learning rate of group 0 to 8.8509e-04.
training: bce: 0.025201, dice: 0.075151, loss: 0.050176
training IoU in current batch 13300 is 0.8911618875905829
training IoU uptillnow 13300 is 0.0029656212860612002
testing: bce: 19.717768, dice: 58.798658, loss: 39.258213
IoU in current test batch is 0.9149863254726439
Epoch 13309: reducing learning rate of group 0 to 8.8421e-04.
training: bce: 0.025150, dice: 0.075041, loss: 0.050095
training IoU in current batch 13400 is 0.8966928582372125
training IoU uptillnow 13400 is 0.0029657955634141055
testing: bce: 19.825464, dice: 59.154473, loss: 39.489969
IoU in current test batch is 0.8919458495880787
Epoch 13410: reducing learning rate of group 0 to 8.8333e-04.
training: bce: 0.025131, dice: 0.074956, loss: 0.050043
training IoU in current batch 13500 is 0.6990274624052097
training IoU uptillnow 13500 is 0.00296108699351511
testing: bce: 19.958626, dice: 59.527912, loss: 39.743269
IoU in current test batch is 0.9196481544269308
Epoch 13511: reducing learning rate of group 0 to 8.8244e-04.
training: bce: 0.025054, dice: 0.074818, loss: 0.049936
training IoU in current batch 13600 is 0.9172519003656067
training IoU uptillnow 13600 is 0.0029617959071320276
testing: bce: 20.044657, dice: 59.859178, loss: 39.951918
IoU in current test batch is 0.8709477220826712
Epoch 13612: reducing learning rate of group 0 to 8.8156e-04.
training: bce: 0.025054, dice: 0.074734, loss: 0.049894
training IoU in current batch 13700 is 0.9497443430605385
training IoU uptillnow 13700 is 0.0029632849850806187
testing: bce: 20.191943, dice: 60.231529, loss: 40.211736
IoU in current test batch is 0.8956585459496087
Epoch 13713: reducing learning rate of group 0 to 8.8068e-04.
training: bce: 0.025019, dice: 0.074652, loss: 0.049836
training IoU in current batch 13800 is 0.8096587331570331
training IoU uptillnow 13800 is 0.002961369018547586
testing: bce: 20.310766, dice: 60.604541, loss: 40.457653
IoU in current test batch is 0.8824756380413032
Epoch 13814: reducing learning rate of group 0 to 8.7980e-04.
training: bce: 0.024998, dice: 0.074564, loss: 0.049781
training IoU in current batch 13900 is 0.9245585874799358
training IoU uptillnow 13900 is 0.0029622358118697846
testing: bce: 20.440739, dice: 60.971271, loss: 40.706005
IoU in current test batch is 0.852610030717336
Epoch 13915: reducing learning rate of group 0 to 8.7892e-04.
training: bce: 0.025024, dice: 0.074529, loss: 0.049777
training IoU in current batch 14000 is 0.9051532834716857
training IoU uptillnow 14000 is 0.002962628225266703
testing: bce: 20.609569, dice: 61.381126, loss: 40.995347
IoU in current test batch is 0.9044919639473096
Epoch 14016: reducing learning rate of group 0 to 8.7804e-04.
training: bce: 0.024997, dice: 0.074415, loss: 0.049706
training IoU in current batch 14100 is 0.9130323308463452
training IoU uptillnow 14100 is 0.0029632013255968532
testing: bce: 20.734012, dice: 61.724989, loss: 41.229501
IoU in current test batch is 0.8654467603034438
Epoch 14117: reducing learning rate of group 0 to 8.7716e-04.
training: bce: 0.024918, dice: 0.074263, loss: 0.049591
training IoU in current batch 14200 is 0.9123669581249826
training IoU uptillnow 14200 is 0.0029637507366816573
testing: bce: 20.815280, dice: 62.035995, loss: 41.425638
IoU in current test batch is 0.8683874647896414
Epoch 14218: reducing learning rate of group 0 to 8.7628e-04.
training: bce: 0.024902, dice: 0.074167, loss: 0.049534
training IoU in current batch 14300 is 0.8523163377192983
training IoU uptillnow 14300 is 0.002962892780751182
testing: bce: 20.948162, dice: 62.391757, loss: 41.669959
IoU in current test batch is 0.8617147687962866
Epoch 14319: reducing learning rate of group 0 to 8.7541e-04.
training: bce: 0.024885, dice: 0.074091, loss: 0.049488
training IoU in current batch 14400 is 0.9307458535077779
training IoU uptillnow 14400 is 0.00296386211203101
testing: bce: 21.080400, dice: 62.764064, loss: 41.922232
IoU in current test batch is 0.8460292143381773
Epoch 14420: reducing learning rate of group 0 to 8.7453e-04.
training: bce: 0.024818, dice: 0.073983, loss: 0.049401
training IoU in current batch 14500 is 0.9293222380378846
training IoU uptillnow 14500 is 0.0029647853496106843
testing: bce: 21.169751, dice: 63.107726, loss: 42.138739
IoU in current test batch is 0.8838238295943225
Epoch 14521: reducing learning rate of group 0 to 8.7366e-04.
training: bce: 0.024737, dice: 0.073831, loss: 0.049284
training IoU in current batch 14600 is 0.9071418378995434
training IoU uptillnow 14600 is 0.0029651895738194447
testing: bce: 21.245760, dice: 63.411726, loss: 42.328743
IoU in current test batch is 0.9020788653349227
Epoch 14622: reducing learning rate of group 0 to 8.7278e-04.
training: bce: 0.024785, dice: 0.073774, loss: 0.049279
training IoU in current batch 14700 is 0.9395449442816856
training IoU uptillnow 14700 is 0.0029663230130896954
testing: bce: 21.432962, dice: 63.797051, loss: 42.615006
IoU in current test batch is 0.8755070404490982
Epoch 14723: reducing learning rate of group 0 to 8.7191e-04.
training: bce: 0.024723, dice: 0.073679, loss: 0.049201
training IoU in current batch 14800 is 0.9097363381941376
training IoU uptillnow 14800 is 0.0029667698170053594
testing: bce: 21.524696, dice: 64.148526, loss: 42.836611
IoU in current test batch is 0.884823714545243
Epoch 14824: reducing learning rate of group 0 to 8.7104e-04.
training: bce: 0.024668, dice: 0.073570, loss: 0.049119
training IoU in current batch 14900 is 0.8867307566542411
training IoU uptillnow 14900 is 0.0029666959922408608
testing: bce: 21.622101, dice: 64.486296, loss: 43.054199
IoU in current test batch is 0.8669407414398353
Epoch 14925: reducing learning rate of group 0 to 8.7017e-04.
training: bce: 0.024645, dice: 0.073502, loss: 0.049073
training IoU in current batch 15000 is 0.9144602851323829
training IoU uptillnow 15000 is 0.0029672393224068525
testing: bce: 21.746729, dice: 64.858946, loss: 43.302837
IoU in current test batch is 0.8781160036565581
Epoch 15026: reducing learning rate of group 0 to 8.6930e-04.
training: bce: 0.024583, dice: 0.073380, loss: 0.048981
training IoU in current batch 15100 is 0.8675183433274982
training IoU uptillnow 15100 is 0.0029667392792884152
testing: bce: 21.836751, dice: 65.182670, loss: 43.509710
IoU in current test batch is 0.8905063998831463
Epoch 15127: reducing learning rate of group 0 to 8.6843e-04.
training: bce: 0.024577, dice: 0.073332, loss: 0.048955
training IoU in current batch 15200 is 0.9241103743810986
training IoU uptillnow 15200 is 0.0029674867869215658
testing: bce: 21.976251, dice: 65.572156, loss: 43.774204
IoU in current test batch is 0.9149169245109946
Epoch 15228: reducing learning rate of group 0 to 8.6756e-04.
training: bce: 0.024523, dice: 0.073234, loss: 0.048879
training IoU in current batch 15300 is 0.9182795698924732
training IoU uptillnow 15300 is 0.002968097499376438
testing: bce: 22.071914, dice: 65.915137, loss: 43.993526
IoU in current test batch is 0.8965393123682689
Epoch 15329: reducing learning rate of group 0 to 8.6669e-04.
training: bce: 0.024474, dice: 0.073138, loss: 0.048806
training IoU in current batch 15400 is 0.8956756259768918
training IoU uptillnow 15400 is 0.002968211049928652
testing: bce: 22.171827, dice: 66.258731, loss: 44.215279
IoU in current test batch is 0.8983583217394887
Epoch 15430: reducing learning rate of group 0 to 8.6583e-04.
training: bce: 0.024408, dice: 0.073014, loss: 0.048711
training IoU in current batch 15500 is 0.9400435069537799
training IoU uptillnow 15500 is 0.002969277221831006
testing: bce: 22.255624, dice: 66.576337, loss: 44.415980
IoU in current test batch is 0.8932066920384779
Epoch 15531: reducing learning rate of group 0 to 8.6496e-04.
training: bce: 0.024341, dice: 0.072885, loss: 0.048613
training IoU in current batch 15600 is 0.8683161205636872
training IoU uptillnow 15600 is 0.002968797187517274
testing: bce: 22.337780, dice: 66.886648, loss: 44.612214
IoU in current test batch is 0.887775038282402
Epoch 15632: reducing learning rate of group 0 to 8.6409e-04.
training: bce: 0.024314, dice: 0.072782, loss: 0.048548
training IoU in current batch 15700 is 0.8282610259730916
training IoU uptillnow 15700 is 0.0029674728954279784
testing: bce: 22.455732, dice: 67.220718, loss: 44.838225
IoU in current test batch is 0.8888844810193615
Epoch 15733: reducing learning rate of group 0 to 8.6323e-04.
training: bce: 0.024254, dice: 0.072684, loss: 0.048469
training IoU in current batch 15800 is 0.9498259169170601
training IoU uptillnow 15800 is 0.0029687298633053
testing: bce: 22.543782, dice: 67.558040, loss: 45.050911
IoU in current test batch is 0.8981464239583827
Epoch 15834: reducing learning rate of group 0 to 8.6237e-04.
training: bce: 0.024188, dice: 0.072565, loss: 0.048377
training IoU in current batch 15900 is 0.8931975127331813
training IoU uptillnow 15900 is 0.0029687839176360882
testing: bce: 22.624358, dice: 67.874329, loss: 45.249343
IoU in current test batch is 0.9011721731359387
Epoch 15935: reducing learning rate of group 0 to 8.6150e-04.
training: bce: 0.024127, dice: 0.072474, loss: 0.048301
training IoU in current batch 16000 is 0.91427426069259
training IoU uptillnow 16000 is 0.002969276367803823
testing: bce: 22.709209, dice: 68.215230, loss: 45.462219
IoU in current test batch is 0.9065915555351831
Epoch 16036: reducing learning rate of group 0 to 8.6064e-04.
training: bce: 0.024096, dice: 0.072406, loss: 0.048251
training IoU in current batch 16100 is 0.847625160462131
training IoU uptillnow 16100 is 0.002968382888105274
testing: bce: 22.821275, dice: 68.577335, loss: 45.699305
IoU in current test batch is 0.9187405019557049
Epoch 16137: reducing learning rate of group 0 to 8.5978e-04.
training: bce: 0.024034, dice: 0.072304, loss: 0.048169
training IoU in current batch 16200 is 0.9083468193700983
training IoU uptillnow 16200 is 0.002968749778069649
testing: bce: 22.904047, dice: 68.905748, loss: 45.904897
IoU in current test batch is 0.9062339709066524
Epoch 16238: reducing learning rate of group 0 to 8.5892e-04.
training: bce: 0.024012, dice: 0.072227, loss: 0.048120
training IoU in current batch 16300 is 0.9400824753529782
training IoU uptillnow 16300 is 0.0029697611176997757
testing: bce: 23.024699, dice: 69.257587, loss: 46.141143
IoU in current test batch is 0.8793744286488369
Epoch 16339: reducing learning rate of group 0 to 8.5806e-04.
training: bce: 0.024030, dice: 0.072174, loss: 0.048102
training IoU in current batch 16400 is 0.8985038652596925
training IoU uptillnow 16400 is 0.002969915082497649
testing: bce: 23.183356, dice: 69.630985, loss: 46.407170
IoU in current test batch is 0.8820444675326939
Epoch 16440: reducing learning rate of group 0 to 8.5721e-04.
training: bce: 0.023992, dice: 0.072129, loss: 0.048061
training IoU in current batch 16500 is 0.9211547110411513
training IoU uptillnow 16500 is 0.0029705247462814975
testing: bce: 23.288096, dice: 70.011789, loss: 46.649943
IoU in current test batch is 0.9164524530581591
Epoch 16541: reducing learning rate of group 0 to 8.5635e-04.
training: bce: 0.023949, dice: 0.072061, loss: 0.048005
training IoU in current batch 16600 is 0.903227816550349
training IoU uptillnow 16600 is 0.0029707671090441
testing: bce: 23.386746, dice: 70.369679, loss: 46.878213
IoU in current test batch is 0.8669929006118986
Epoch 16642: reducing learning rate of group 0 to 8.5549e-04.
training: bce: 0.023904, dice: 0.071975, loss: 0.047939
training IoU in current batch 16700 is 0.9184870120006077
training IoU uptillnow 16700 is 0.0029713111259550115
testing: bce: 23.483601, dice: 70.708789, loss: 47.096195
IoU in current test batch is 0.9040118822783298
Epoch 16743: reducing learning rate of group 0 to 8.5464e-04.
training: bce: 0.024069, dice: 0.071961, loss: 0.048015
training IoU in current batch 16800 is 0.9144742605191696
training IoU uptillnow 16800 is 0.002971769053513543
testing: bce: 23.787618, dice: 71.118713, loss: 47.453166
IoU in current test batch is 0.8383237976656759
Epoch 16844: reducing learning rate of group 0 to 8.5378e-04.
training: bce: 0.024053, dice: 0.071934, loss: 0.047994
training IoU in current batch 16900 is 0.932496832961332
training IoU uptillnow 16900 is 0.0029725770159005275
testing: bce: 23.913420, dice: 71.514879, loss: 47.714150
IoU in current test batch is 0.8893408068752429
Epoch 16945: reducing learning rate of group 0 to 8.5293e-04.
training: bce: 0.024025, dice: 0.071864, loss: 0.047944
training IoU in current batch 17000 is 0.9346185553934936
training IoU uptillnow 17000 is 0.00297341707336035
testing: bce: 24.026271, dice: 71.868331, loss: 47.947301
IoU in current test batch is 0.9142115924234407
Epoch 17046: reducing learning rate of group 0 to 8.5208e-04.
training: bce: 0.023967, dice: 0.071763, loss: 0.047865
training IoU in current batch 17100 is 0.8654564869193807
training IoU uptillnow 17100 is 0.0029728991965288056
testing: bce: 24.109812, dice: 72.189657, loss: 48.149735
IoU in current test batch is 0.9021162277536741
Epoch 17147: reducing learning rate of group 0 to 8.5122e-04.
training: bce: 0.023915, dice: 0.071672, loss: 0.047794
training IoU in current batch 17200 is 0.9065561631351104
training IoU uptillnow 17200 is 0.002973183800218058
testing: bce: 24.198226, dice: 72.518979, loss: 48.358603
IoU in current test batch is 0.8785626111321216
Epoch 17248: reducing learning rate of group 0 to 8.5037e-04.
training: bce: 0.023864, dice: 0.071575, loss: 0.047719
training IoU in current batch 17300 is 0.7914863579167051
training IoU uptillnow 17300 is 0.0029712480974234466
testing: bce: 24.286447, dice: 72.842174, loss: 48.564310
IoU in current test batch is 0.9119738867934359
Epoch 17349: reducing learning rate of group 0 to 8.4952e-04.
training: bce: 0.023886, dice: 0.071473, loss: 0.047680
training IoU in current batch 17400 is 0.2880002108926029
training IoU uptillnow 17400 is 0.0029596898686179286
testing: bce: 24.449573, dice: 73.159169, loss: 48.804371
IoU in current test batch is 0.8620995621856616
Epoch 17450: reducing learning rate of group 0 to 8.4867e-04.
training: bce: 0.023854, dice: 0.071411, loss: 0.047632
training IoU in current batch 17500 is 0.9505387405923439
training IoU uptillnow 17500 is 0.002960882786737026
testing: bce: 24.557369, dice: 73.515037, loss: 49.036203
IoU in current test batch is 0.859992687594591
Epoch 17551: reducing learning rate of group 0 to 8.4782e-04.
training: bce: 0.023806, dice: 0.071318, loss: 0.047562
training IoU in current batch 17600 is 0.9413521302823015
training IoU uptillnow 17600 is 0.0029618881707921215
testing: bce: 24.648124, dice: 73.839048, loss: 49.243586
IoU in current test batch is 0.9111365153585155
Epoch 17652: reducing learning rate of group 0 to 8.4698e-04.
training: bce: 0.023754, dice: 0.071243, loss: 0.047499
training IoU in current batch 17700 is 0.9144789054315736
training IoU uptillnow 17700 is 0.00296237613671107
testing: bce: 24.733576, dice: 74.181115, loss: 49.457346
IoU in current test batch is 0.9011026910401414
training: bce: 0.023710, dice: 0.071153, loss: 0.047432
training IoU in current batch 17800 is 0.9433208219695947
training IoU uptillnow 17800 is 0.0029633987006298813
testing: bce: 24.827555, dice: 74.505964, loss: 49.666759
IoU in current test batch is 0.8961362726824961
Epoch 17830: reducing learning rate of group 0 to 8.4613e-04.
training: bce: 0.023686, dice: 0.071102, loss: 0.047394
training IoU in current batch 17900 is 0.9089926761916196
training IoU uptillnow 17900 is 0.002963770617766776
testing: bce: 24.941035, dice: 74.870406, loss: 49.905720
IoU in current test batch is 0.9079671512357598
Epoch 17931: reducing learning rate of group 0 to 8.4528e-04.
training: bce: 0.023643, dice: 0.071044, loss: 0.047343
training IoU in current batch 18000 is 0.8376812446692744
training IoU uptillnow 18000 is 0.0029628178940169702
testing: bce: 25.035168, dice: 75.226722, loss: 50.130945
IoU in current test batch is 0.9016820264316943
Epoch 18032: reducing learning rate of group 0 to 8.4444e-04.
training: bce: 0.023649, dice: 0.070962, loss: 0.047305
training IoU in current batch 18100 is 0.9118270721903978
training IoU uptillnow 18100 is 0.0029632411064359028
testing: bce: 25.180404, dice: 75.557517, loss: 50.368961
IoU in current test batch is 0.8469942494882222
Epoch 18133: reducing learning rate of group 0 to 8.4359e-04.
training: bce: 0.023599, dice: 0.070903, loss: 0.047251
training IoU in current batch 18200 is 0.9548415229062764
training IoU uptillnow 18200 is 0.0029644474355931924
testing: bce: 25.265880, dice: 75.911608, loss: 50.588744
IoU in current test batch is 0.9107354309174522
Epoch 18234: reducing learning rate of group 0 to 8.4275e-04.
training: bce: 0.023578, dice: 0.070838, loss: 0.047208
training IoU in current batch 18300 is 0.9203806426931905
training IoU uptillnow 18300 is 0.00296501291310836
testing: bce: 25.382796, dice: 76.259102, loss: 50.820949
IoU in current test batch is 0.9071438970200706
Epoch 18335: reducing learning rate of group 0 to 8.4191e-04.
training: bce: 0.023528, dice: 0.070758, loss: 0.047143
training IoU in current batch 18400 is 0.9396072341065077
training IoU uptillnow 18400 is 0.002965920533349543
testing: bce: 25.467159, dice: 76.589504, loss: 51.028331
IoU in current test batch is 0.8780854640798266
Epoch 18436: reducing learning rate of group 0 to 8.4106e-04.
training: bce: 0.023485, dice: 0.070680, loss: 0.047082
training IoU in current batch 18500 is 0.8865985668547035
training IoU uptillnow 18500 is 0.002965863282513549
testing: bce: 25.558323, dice: 76.920427, loss: 51.239375
IoU in current test batch is 0.9043995324188576
Epoch 18537: reducing learning rate of group 0 to 8.4022e-04.
training: bce: 0.023440, dice: 0.070595, loss: 0.047018
training IoU in current batch 18600 is 0.9328101881209047
training IoU uptillnow 18600 is 0.002966634767977894
testing: bce: 25.647559, dice: 77.243719, loss: 51.445639
IoU in current test batch is 0.9176446792043569
Epoch 18638: reducing learning rate of group 0 to 8.3938e-04.
training: bce: 0.023382, dice: 0.070489, loss: 0.046936
training IoU in current batch 18700 is 0.8975202816470228
training IoU uptillnow 18700 is 0.002966768982748114
testing: bce: 25.721810, dice: 77.542190, loss: 51.632000
IoU in current test batch is 0.8850865001842112
Epoch 18739: reducing learning rate of group 0 to 8.3854e-04.
training: bce: 0.023342, dice: 0.070456, loss: 0.046899
training IoU in current batch 18800 is 0.9273219938831357
training IoU uptillnow 18800 is 0.0029674301408258528
testing: bce: 25.815058, dice: 77.920080, loss: 51.867569
IoU in current test batch is 0.8798512200903604
Epoch 18840: reducing learning rate of group 0 to 8.3771e-04.
training: bce: 0.023313, dice: 0.070382, loss: 0.046848
training IoU in current batch 18900 is 0.9285079928952042
training IoU uptillnow 18900 is 0.00296810521887547
testing: bce: 25.919828, dice: 78.252876, loss: 52.086352
IoU in current test batch is 0.8840055671072462
Epoch 18941: reducing learning rate of group 0 to 8.3687e-04.
training: bce: 0.023297, dice: 0.070347, loss: 0.046822
training IoU in current batch 19000 is 0.925211389128559
training IoU uptillnow 19000 is 0.0029687153591043333
testing: bce: 26.039418, dice: 78.627770, loss: 52.333594
IoU in current test batch is 0.9130374974319767
Epoch 19042: reducing learning rate of group 0 to 8.3603e-04.
training: bce: 0.023252, dice: 0.070274, loss: 0.046763
training IoU in current batch 19100 is 0.9303188528717963
training IoU uptillnow 19100 is 0.002969408241591123
testing: bce: 26.125385, dice: 78.958479, loss: 52.541932
IoU in current test batch is 0.8589917980494728
Epoch 19143: reducing learning rate of group 0 to 8.3519e-04.
training: bce: 0.023259, dice: 0.070205, loss: 0.046732
training IoU in current batch 19200 is 0.8751144339334758
training IoU uptillnow 19200 is 0.0029691355467914796
testing: bce: 26.270229, dice: 79.294606, loss: 52.782417
IoU in current test batch is 0.8702087384795435
Epoch 19244: reducing learning rate of group 0 to 8.3436e-04.
training: bce: 0.023225, dice: 0.070149, loss: 0.046687
training IoU in current batch 19300 is 0.9269123074964999
training IoU uptillnow 19300 is 0.002969760240563116
testing: bce: 26.368638, dice: 79.643611, loss: 53.006124
IoU in current test batch is 0.8992271499824798
Epoch 19345: reducing learning rate of group 0 to 8.3353e-04.
training: bce: 0.023185, dice: 0.070085, loss: 0.046635
training IoU in current batch 19400 is 0.9353066742564394
training IoU uptillnow 19400 is 0.002970522720196254
testing: bce: 26.459158, dice: 79.983076, loss: 53.221117
IoU in current test batch is 0.9191852773065818
Epoch 19446: reducing learning rate of group 0 to 8.3269e-04.
training: bce: 0.023156, dice: 0.070034, loss: 0.046595
training IoU in current batch 19500 is 0.9201986513133871
training IoU uptillnow 19500 is 0.0029710191363672965
testing: bce: 26.562490, dice: 80.337172, loss: 53.449831
IoU in current test batch is 0.8686383319992124
Epoch 19547: reducing learning rate of group 0 to 8.3186e-04.
training: bce: 0.023151, dice: 0.069966, loss: 0.046559
training IoU in current batch 19600 is 0.8648879605013293
training IoU uptillnow 19600 is 0.002970569877308935
testing: bce: 26.693315, dice: 80.670963, loss: 53.682139
IoU in current test batch is 0.8232176719088626
Epoch 19648: reducing learning rate of group 0 to 8.3103e-04.
training: bce: 0.023121, dice: 0.069916, loss: 0.046518
training IoU in current batch 19700 is 0.9240665684107099
training IoU uptillnow 19700 is 0.002971126458281441
testing: bce: 26.794530, dice: 81.024406, loss: 53.909468
IoU in current test batch is 0.8958147669368733
Epoch 19749: reducing learning rate of group 0 to 8.3020e-04.
training: bce: 0.023089, dice: 0.069850, loss: 0.046469
training IoU in current batch 19800 is 0.9386674030929906
training IoU uptillnow 19800 is 0.002971923210391748
testing: bce: 26.892753, dice: 81.358730, loss: 54.125742
IoU in current test batch is 0.8642456468364493
Epoch 19850: reducing learning rate of group 0 to 8.2937e-04.
training: bce: 0.023066, dice: 0.069809, loss: 0.046438
training IoU in current batch 19900 is 0.862345016108562
training IoU uptillnow 19900 is 0.002971433587642322
testing: bce: 27.002520, dice: 81.722206, loss: 54.362363
IoU in current test batch is 0.9085065151458518
Epoch 19951: reducing learning rate of group 0 to 8.2854e-04.
training: bce: 0.023014, dice: 0.069699, loss: 0.046356
training IoU in current batch 20000 is 0.9378843945387265
training IoU uptillnow 20000 is 0.002972207787569759
testing: bce: 27.076156, dice: 82.002426, loss: 54.539291
IoU in current test batch is 0.90480258078757
Epoch 20052: reducing learning rate of group 0 to 8.2771e-04.
training: bce: 0.022973, dice: 0.069633, loss: 0.046303
training IoU in current batch 20100 is 0.9296098940859154
training IoU uptillnow 20100 is 0.002972837068995476
testing: bce: 27.164089, dice: 82.334997, loss: 54.749543
IoU in current test batch is 0.8968511122279762
Epoch 20153: reducing learning rate of group 0 to 8.2688e-04.
training: bce: 0.022934, dice: 0.069561, loss: 0.046248
training IoU in current batch 20200 is 0.9431177816213804
training IoU uptillnow 20200 is 0.0029736830116208032
testing: bce: 27.252317, dice: 82.659489, loss: 54.955903
IoU in current test batch is 0.8897552244979451
Epoch 20254: reducing learning rate of group 0 to 8.2605e-04.
training: bce: 0.022938, dice: 0.069541, loss: 0.046239
training IoU in current batch 20300 is 0.9267062535715173
training IoU uptillnow 20300 is 0.0029742511503017435
testing: bce: 27.391765, dice: 83.044295, loss: 55.218030
IoU in current test batch is 0.9102284188885411
Epoch 20355: reducing learning rate of group 0 to 8.2523e-04.
training: bce: 0.022892, dice: 0.069463, loss: 0.046178
training IoU in current batch 20400 is 0.9177396716909746
training IoU uptillnow 20400 is 0.0029746672136744612
testing: bce: 27.471456, dice: 83.360233, loss: 55.415844
IoU in current test batch is 0.8915601606972933
Epoch 20456: reducing learning rate of group 0 to 8.2440e-04.
training: bce: 0.022854, dice: 0.069388, loss: 0.046121
training IoU in current batch 20500 is 0.8503260405874825
training IoU uptillnow 20500 is 0.002973983114955458
testing: bce: 27.560942, dice: 83.678316, loss: 55.619629
IoU in current test batch is 0.9188813340371038
Epoch 20557: reducing learning rate of group 0 to 8.2358e-04.
training: bce: 0.022834, dice: 0.069333, loss: 0.046084
training IoU in current batch 20600 is 0.9244660194174757
training IoU uptillnow 20600 is 0.002974505275448166
testing: bce: 27.670630, dice: 84.019718, loss: 55.845174
IoU in current test batch is 0.9116798268353554
Epoch 20658: reducing learning rate of group 0 to 8.2275e-04.
training: bce: 0.022812, dice: 0.069300, loss: 0.046056
training IoU in current batch 20700 is 0.9574446603902168
training IoU uptillnow 20700 is 0.0029755534225224746
testing: bce: 27.778124, dice: 84.386923, loss: 56.082524
IoU in current test batch is 0.8703039320246093
Epoch 20759: reducing learning rate of group 0 to 8.2193e-04.
training: bce: 0.022767, dice: 0.069212, loss: 0.045990
training IoU in current batch 20800 is 0.8601430281849659
training IoU uptillnow 20800 is 0.002975032245678865
testing: bce: 27.857395, dice: 84.687147, loss: 56.272271
IoU in current test batch is 0.8921377901229202
Epoch 20860: reducing learning rate of group 0 to 8.2111e-04.
training: bce: 0.022743, dice: 0.069183, loss: 0.045963
training IoU in current batch 20900 is 0.8745926878120236
training IoU uptillnow 20900 is 0.0029747465020000677
testing: bce: 27.962098, dice: 85.057988, loss: 56.510043
IoU in current test batch is 0.8997576911081222
Epoch 20961: reducing learning rate of group 0 to 8.2029e-04.
training: bce: 0.022709, dice: 0.069144, loss: 0.045926
training IoU in current batch 21000 is 0.8723066531577169
training IoU uptillnow 21000 is 0.002974427195023538
testing: bce: 28.053302, dice: 85.417156, loss: 56.735229
IoU in current test batch is 0.8861526159206262
Epoch 21062: reducing learning rate of group 0 to 8.1947e-04.
training: bce: 0.022677, dice: 0.069113, loss: 0.045895
training IoU in current batch 21100 is 0.9371788744174931
training IoU uptillnow 21100 is 0.002975135703560423
testing: bce: 28.147809, dice: 85.785984, loss: 56.966896
IoU in current test batch is 0.8881399286518037
Epoch 21163: reducing learning rate of group 0 to 8.1865e-04.
training: bce: 0.022636, dice: 0.069040, loss: 0.045838
training IoU in current batch 21200 is 0.9472721302247032
training IoU uptillnow 21200 is 0.0029759962198750082
testing: bce: 28.229373, dice: 86.100629, loss: 57.165001
IoU in current test batch is 0.9127259021598959
Epoch 21264: reducing learning rate of group 0 to 8.1783e-04.
training: bce: 0.022642, dice: 0.069019, loss: 0.045830
training IoU in current batch 21300 is 0.9187562288220051
training IoU uptillnow 21300 is 0.0029764024193157154
testing: bce: 28.370200, dice: 86.480383, loss: 57.425291
IoU in current test batch is 0.8947158335999655
Epoch 21365: reducing learning rate of group 0 to 8.1701e-04.
training: bce: 0.022618, dice: 0.068995, loss: 0.045806
training IoU in current batch 21400 is 0.8775222390974181
training IoU uptillnow 21400 is 0.002976162578705506
testing: bce: 28.472982, dice: 86.856784, loss: 57.664883
IoU in current test batch is 0.914238474308828
Epoch 21466: reducing learning rate of group 0 to 8.1620e-04.
training: bce: 0.022589, dice: 0.068944, loss: 0.045766
training IoU in current batch 21500 is 0.9109055374592834
training IoU uptillnow 21500 is 0.0029764425155122845
testing: bce: 28.569213, dice: 87.198458, loss: 57.883836
IoU in current test batch is 0.901550280691867
Epoch 21567: reducing learning rate of group 0 to 8.1538e-04.
training: bce: 0.022550, dice: 0.068871, loss: 0.045710
training IoU in current batch 21600 is 0.9393973881127835
training IoU uptillnow 21600 is 0.00297715952912985
testing: bce: 28.652648, dice: 87.510652, loss: 58.081650
IoU in current test batch is 0.9121236705172333
Epoch 21668: reducing learning rate of group 0 to 8.1456e-04.
training: bce: 0.127573, dice: 0.851923, loss: 0.489748
training IoU in current batch 0 is 0.0006352538253328177
training IoU uptillnow 0 is 9.758123277001808e-09
testing: bce: 0.007504, dice: 0.050113, loss: 0.028809
IoU in current test batch is 0.0
Epoch 21769: reducing learning rate of group 0 to 8.1375e-04.
training: bce: 0.082474, dice: 0.277879, loss: 0.180177
training IoU in current batch 100 is 0.8328175472678399
training IoU uptillnow 100 is 1.2743926622219766e-05
testing: bce: 0.489992, dice: 1.650928, loss: 1.070460
IoU in current test batch is 0.8321536902815385
Epoch 21870: reducing learning rate of group 0 to 8.1294e-04.
training: bce: 0.060722, dice: 0.187760, loss: 0.124241
training IoU in current batch 200 is 0.8871914122705027
training IoU uptillnow 200 is 2.618940964023859e-05
testing: bce: 0.717950, dice: 2.219985, loss: 1.468968
IoU in current test batch is 0.8228210981835767
Epoch 21971: reducing learning rate of group 0 to 8.1212e-04.
training: bce: 0.051258, dice: 0.154487, loss: 0.102872
training IoU in current batch 300 is 0.9340273358955433
training IoU uptillnow 300 is 4.0222296200897254e-05
testing: bce: 0.907572, dice: 2.735321, loss: 1.821446
IoU in current test batch is 0.8758303555393293
Epoch 22072: reducing learning rate of group 0 to 8.1131e-04.
training: bce: 0.042740, dice: 0.134676, loss: 0.088708
training IoU in current batch 400 is 0.9176303396773204
training IoU uptillnow 400 is 5.3880873136297726e-05
testing: bce: 1.008156, dice: 3.176763, loss: 2.092459
IoU in current test batch is 0.8529734056254854
Epoch 22173: reducing learning rate of group 0 to 8.1050e-04.
training: bce: 0.038028, dice: 0.121896, loss: 0.079962
training IoU in current batch 500 is 0.8888918032786886
training IoU uptillnow 500 is 6.698489027350191e-05
testing: bce: 1.120693, dice: 3.592337, loss: 2.356515
IoU in current test batch is 0.8724469844607581
Epoch 22274: reducing learning rate of group 0 to 8.0969e-04.
training: bce: 0.035107, dice: 0.112556, loss: 0.073832
training IoU in current batch 600 is 0.9417677642980936
training IoU uptillnow 600 is 8.076175570274022e-05
testing: bce: 1.241146, dice: 3.979192, loss: 2.610169
IoU in current test batch is 0.8862527069902743
Epoch 22375: reducing learning rate of group 0 to 8.0888e-04.
training: bce: 0.033800, dice: 0.107579, loss: 0.070690
training IoU in current batch 700 is 0.896455443216553
training IoU uptillnow 700 is 9.374132291264693e-05
testing: bce: 1.393759, dice: 4.436068, loss: 2.914913
IoU in current test batch is 0.85296611828181
Epoch 22476: reducing learning rate of group 0 to 8.0807e-04.
training: bce: 0.031952, dice: 0.102649, loss: 0.067301
training IoU in current batch 800 is 0.9237804014836207
training IoU uptillnow 800 is 0.00010701033038834806
testing: bce: 1.505486, dice: 4.836596, loss: 3.171041
IoU in current test batch is 0.8796138379640643
Epoch 22577: reducing learning rate of group 0 to 8.0726e-04.
training: bce: 0.030184, dice: 0.098529, loss: 0.064357
training IoU in current batch 900 is 0.9398441847523651
training IoU uptillnow 900 is 0.00012039884197589763
testing: bce: 1.599778, dice: 5.222026, loss: 3.410902
IoU in current test batch is 0.8723121574167285
Epoch 22678: reducing learning rate of group 0 to 8.0645e-04.
training: bce: 0.029396, dice: 0.095718, loss: 0.062557
training IoU in current batch 1000 is 0.9401007017818476
training IoU uptillnow 1000 is 0.00013367315987882096
testing: bce: 1.730922, dice: 5.636095, loss: 3.683508
IoU in current test batch is 0.898128619176928
Epoch 22779: reducing learning rate of group 0 to 8.0565e-04.
training: bce: 0.028428, dice: 0.092553, loss: 0.060491
training IoU in current batch 1100 is 0.9008902446616448
training IoU uptillnow 1100 is 0.00014625778409955192
testing: bce: 1.841135, dice: 5.994181, loss: 3.917658
IoU in current test batch is 0.8974702954657945
Epoch 22880: reducing learning rate of group 0 to 8.0484e-04.
training: bce: 0.027551, dice: 0.090096, loss: 0.058823
training IoU in current batch 1200 is 0.8651572772029127
training IoU uptillnow 1200 is 0.00015821236840774766
testing: bce: 1.946397, dice: 6.365015, loss: 4.155706
IoU in current test batch is 0.8960856734294158
Epoch 22981: reducing learning rate of group 0 to 8.0404e-04.
training: bce: 0.026598, dice: 0.087890, loss: 0.057244
training IoU in current batch 1300 is 0.8889161697649948
training IoU uptillnow 1300 is 0.00017040733158517766
testing: bce: 2.035531, dice: 6.726133, loss: 4.380832
IoU in current test batch is 0.8818605922509117
Epoch 23082: reducing learning rate of group 0 to 8.0323e-04.
training: bce: 0.025777, dice: 0.086061, loss: 0.055919
training IoU in current batch 1400 is 0.918125899102138
training IoU uptillnow 1400 is 0.0001829182074816652
testing: bce: 2.124289, dice: 7.092452, loss: 4.608370
IoU in current test batch is 0.8939039402103255
Epoch 23183: reducing learning rate of group 0 to 8.0243e-04.
training: bce: 0.025037, dice: 0.084203, loss: 0.054620
training IoU in current batch 1500 is 0.9342626515354601
training IoU uptillnow 1500 is 0.0001955530808910181
testing: bce: 2.210580, dice: 7.434638, loss: 4.822609
IoU in current test batch is 0.8990570957182061
Epoch 23284: reducing learning rate of group 0 to 8.0163e-04.
training: bce: 0.024595, dice: 0.082842, loss: 0.053719
training IoU in current batch 1600 is 0.86244995032664
training IoU uptillnow 1600 is 0.0002070521370578183
testing: bce: 2.316266, dice: 7.801810, loss: 5.059038
IoU in current test batch is 0.9075607019717093
Epoch 23385: reducing learning rate of group 0 to 8.0083e-04.
training: bce: 0.024597, dice: 0.082198, loss: 0.053397
training IoU in current batch 1700 is 0.800894135522894
training IoU uptillnow 1700 is 0.00021757604723453552
testing: bce: 2.461112, dice: 8.224605, loss: 5.342858
IoU in current test batch is 0.810777492256488
Epoch 23486: reducing learning rate of group 0 to 8.0003e-04.
training: bce: 0.024268, dice: 0.081397, loss: 0.052833
training IoU in current batch 1800 is 0.8904490906841519
training IoU uptillnow 1800 is 0.0002292806752701921
testing: bce: 2.571015, dice: 8.623318, loss: 5.597166
IoU in current test batch is 0.8955036927640243
Epoch 23587: reducing learning rate of group 0 to 7.9923e-04.
training: bce: 0.023768, dice: 0.080235, loss: 0.052001
training IoU in current batch 1900 is 0.9147559962751365
training IoU uptillnow 1900 is 0.00024122942941841357
testing: bce: 2.657829, dice: 8.972132, loss: 5.814980
IoU in current test batch is 0.9002760626787247
Epoch 23688: reducing learning rate of group 0 to 7.9843e-04.
training: bce: 0.025781, dice: 0.079733, loss: 0.052757
training IoU in current batch 2000 is 0.9262788712245322
training IoU uptillnow 2000 is 0.0002532394159500452
testing: bce: 3.034632, dice: 9.385032, loss: 6.209832
IoU in current test batch is 0.85211019745679
Epoch 23789: reducing learning rate of group 0 to 7.9763e-04.
training: bce: 0.025336, dice: 0.079250, loss: 0.052293
training IoU in current batch 2100 is 0.7819044209772686
training IoU uptillnow 2100 is 0.00026312642710119726
testing: bce: 3.131176, dice: 9.794428, loss: 6.462802
IoU in current test batch is 0.8942152059623029
Epoch 23890: reducing learning rate of group 0 to 7.9683e-04.
training: bce: 0.024867, dice: 0.078262, loss: 0.051564
training IoU in current batch 2200 is 0.9315519581720376
training IoU uptillnow 2200 is 0.00027501783616732945
testing: bce: 3.219502, dice: 10.132636, loss: 6.676069
IoU in current test batch is 0.8940512167183784
Epoch 23991: reducing learning rate of group 0 to 7.9603e-04.
training: bce: 0.024665, dice: 0.077681, loss: 0.051173
training IoU in current batch 2300 is 0.9052614546533265
training IoU uptillnow 2300 is 0.0002864450042757062
testing: bce: 3.338530, dice: 10.514287, loss: 6.926409
IoU in current test batch is 0.8934582979582166
Epoch 24092: reducing learning rate of group 0 to 7.9524e-04.
training: bce: 0.024418, dice: 0.077254, loss: 0.050836
training IoU in current batch 2400 is 0.887109050650429
training IoU uptillnow 2400 is 0.00029752627051868986
testing: bce: 3.448698, dice: 10.910936, loss: 7.179817
IoU in current test batch is 0.8759454262331584
Epoch 24193: reducing learning rate of group 0 to 7.9444e-04.
training: bce: 0.024079, dice: 0.076515, loss: 0.050297
training IoU in current batch 2500 is 0.8697260990226441
training IoU uptillnow 2500 is 0.00030827652145349753
testing: bce: 3.542416, dice: 11.256674, loss: 7.399545
IoU in current test batch is 0.905150085872382
Epoch 24294: reducing learning rate of group 0 to 7.9365e-04.
training: bce: 0.024015, dice: 0.076112, loss: 0.050064
training IoU in current batch 2600 is 0.9253105028337152
training IoU uptillnow 2600 is 0.00031970076763179197
testing: bce: 3.674365, dice: 11.645141, loss: 7.659753
IoU in current test batch is 0.8980118239013415
Epoch 24395: reducing learning rate of group 0 to 7.9285e-04.
training: bce: 0.024270, dice: 0.075849, loss: 0.050059
training IoU in current batch 2700 is 0.934580315495863
training IoU uptillnow 2700 is 0.0003311580092329713
testing: bce: 3.856119, dice: 12.051017, loss: 7.953568
IoU in current test batch is 0.9006575827385276
Epoch 24496: reducing learning rate of group 0 to 7.9206e-04.
training: bce: 0.023915, dice: 0.075177, loss: 0.049546
training IoU in current batch 2800 is 0.9133106044463385
training IoU uptillnow 2800 is 0.00034223233850748077
testing: bce: 3.940315, dice: 12.386544, loss: 8.163430
IoU in current test batch is 0.8993268014672546
Epoch 24597: reducing learning rate of group 0 to 7.9127e-04.
training: bce: 0.023572, dice: 0.074574, loss: 0.049073
training IoU in current batch 2900 is 0.8943235360107695
training IoU uptillnow 2900 is 0.0003529593552345611
testing: bce: 4.022499, dice: 12.725842, loss: 8.374171
IoU in current test batch is 0.8554753027338228
Epoch 24698: reducing learning rate of group 0 to 7.9048e-04.
training: bce: 0.023722, dice: 0.074311, loss: 0.049016
training IoU in current batch 3000 is 0.8905332518532295
training IoU uptillnow 3000 is 0.0003635483625933042
testing: bce: 4.187563, dice: 13.118134, loss: 8.652848
IoU in current test batch is 0.8744397815819756
Epoch 24799: reducing learning rate of group 0 to 7.8969e-04.
training: bce: 0.023442, dice: 0.073837, loss: 0.048639
training IoU in current batch 3100 is 0.8884846211552888
training IoU uptillnow 3100 is 0.0003740244393725689
testing: bce: 4.276037, dice: 13.468695, loss: 8.872366
IoU in current test batch is 0.8836181060050058
training: bce: 0.023195, dice: 0.073315, loss: 0.048255
training IoU in current batch 3200 is 0.8935860444185616
training IoU uptillnow 3200 is 0.0003844846631022448
testing: bce: 4.367391, dice: 13.804764, loss: 9.086077
IoU in current test batch is 0.8833282131130323
Epoch 24900: reducing learning rate of group 0 to 7.8890e-04.
training: bce: 0.023143, dice: 0.073150, loss: 0.048147
training IoU in current batch 3300 is 0.9439753166612537
training IoU uptillnow 3300 is 0.0003955330620053192
testing: bce: 4.493905, dice: 14.204066, loss: 9.348986
IoU in current test batch is 0.8400651127953468
Epoch 25001: reducing learning rate of group 0 to 7.8811e-04.
training: bce: 0.022938, dice: 0.072644, loss: 0.047791
training IoU in current batch 3400 is 0.8917750477357063
training IoU uptillnow 3400 is 0.00040580019519435124
testing: bce: 4.588971, dice: 14.533114, loss: 9.561043
IoU in current test batch is 0.8955952292163154
Epoch 25102: reducing learning rate of group 0 to 7.8732e-04.
training: bce: 0.022766, dice: 0.072305, loss: 0.047536
training IoU in current batch 3500 is 0.9183088018445534
training IoU uptillnow 3500 is 0.00041633681878279363
testing: bce: 4.688553, dice: 14.890495, loss: 9.789524
IoU in current test batch is 0.9015545281716885
Epoch 25203: reducing learning rate of group 0 to 7.8653e-04.
training: bce: 0.022725, dice: 0.072149, loss: 0.047437
training IoU in current batch 3600 is 0.9384272790535838
training IoU uptillnow 3600 is 0.00042705521448001033
testing: bce: 4.813754, dice: 15.282771, loss: 10.048262
IoU in current test batch is 0.8821306443736123
Epoch 25304: reducing learning rate of group 0 to 7.8575e-04.
training: bce: 0.022483, dice: 0.071705, loss: 0.047094
training IoU in current batch 3700 is 0.9340627623845508
training IoU uptillnow 3700 is 0.0004376319362390726
testing: bce: 4.894788, dice: 15.610509, loss: 10.252648
IoU in current test batch is 0.8816994155937038
Epoch 25405: reducing learning rate of group 0 to 7.8496e-04.
training: bce: 0.022267, dice: 0.071295, loss: 0.046781
training IoU in current batch 3800 is 0.9144870076157334
training IoU uptillnow 3800 is 0.00044786981109847147
testing: bce: 4.978620, dice: 15.940818, loss: 10.459719
IoU in current test batch is 0.8835303777551368
Epoch 25506: reducing learning rate of group 0 to 7.8418e-04.
training: bce: 0.022097, dice: 0.070993, loss: 0.046545
training IoU in current batch 3900 is 0.9278208929194279
training IoU uptillnow 3900 is 0.00045820132085875643
testing: bce: 5.070709, dice: 16.290852, loss: 10.680781
IoU in current test batch is 0.891341768608067
Epoch 25607: reducing learning rate of group 0 to 7.8339e-04.
training: bce: 0.021916, dice: 0.070653, loss: 0.046285
training IoU in current batch 4000 is 0.7430523917995444
training IoU uptillnow 4000 is 0.00046605595115112894
testing: bce: 5.158053, dice: 16.628342, loss: 10.893197
IoU in current test batch is 0.9084094146275785
Epoch 25708: reducing learning rate of group 0 to 7.8261e-04.
training: bce: 0.021762, dice: 0.070413, loss: 0.046087
training IoU in current batch 4100 is 0.9219226081657526
training IoU uptillnow 4100 is 0.0004761606775441575
testing: bce: 5.249646, dice: 16.986035, loss: 11.117841
IoU in current test batch is 0.8806052265848384
Epoch 25809: reducing learning rate of group 0 to 7.8183e-04.
training: bce: 0.021627, dice: 0.070182, loss: 0.045905
training IoU in current batch 4200 is 0.9129253989617381
training IoU uptillnow 4200 is 0.0004860715809637005
testing: bce: 5.344448, dice: 17.343196, loss: 11.343822
IoU in current test batch is 0.9056338853318797
Epoch 25910: reducing learning rate of group 0 to 7.8104e-04.
training: bce: 0.021532, dice: 0.070041, loss: 0.045787
training IoU in current batch 4300 is 0.947821423783349
training IoU uptillnow 4300 is 0.000496353631598242
testing: bce: 5.447568, dice: 17.720408, loss: 11.583988
IoU in current test batch is 0.8978954278327622
Epoch 26011: reducing learning rate of group 0 to 7.8026e-04.
training: bce: 0.021393, dice: 0.069655, loss: 0.045524
training IoU in current batch 4400 is 0.9215904387417219
training IoU uptillnow 4400 is 0.000506221886378092
testing: bce: 5.538343, dice: 18.032377, loss: 11.785360
IoU in current test batch is 0.8585167716337287
Epoch 26112: reducing learning rate of group 0 to 7.7948e-04.
training: bce: 0.021302, dice: 0.069466, loss: 0.045384
training IoU in current batch 4500 is 0.7852518713331985
training IoU uptillnow 4500 is 0.000514280223597173
testing: bce: 5.639943, dice: 18.392183, loss: 12.016063
IoU in current test batch is 0.9011436181775536
Epoch 26213: reducing learning rate of group 0 to 7.7870e-04.
training: bce: 0.021290, dice: 0.069337, loss: 0.045313
training IoU in current batch 4600 is 0.9106035320198271
training IoU uptillnow 4600 is 0.0005238660216319092
testing: bce: 5.761943, dice: 18.765913, loss: 12.263928
IoU in current test batch is 0.9083091672450196
Epoch 26314: reducing learning rate of group 0 to 7.7792e-04.
training: bce: 0.021173, dice: 0.069112, loss: 0.045143
training IoU in current batch 4700 is 0.585216038805262
training IoU uptillnow 4700 is 0.0005292707720399355
testing: bce: 5.854964, dice: 19.111627, loss: 12.483296
IoU in current test batch is 0.8933900915395511
Epoch 26415: reducing learning rate of group 0 to 7.7715e-04.
training: bce: 0.021011, dice: 0.068804, loss: 0.044907
training IoU in current batch 4800 is 0.9140209212699578
training IoU uptillnow 4800 is 0.0005387706423500987
testing: bce: 5.933626, dice: 19.431082, loss: 12.682354
IoU in current test batch is 0.9054909066477274
Epoch 26516: reducing learning rate of group 0 to 7.7637e-04.
training: bce: 0.020939, dice: 0.068576, loss: 0.044758
training IoU in current batch 4900 is 0.9164753471083416
training IoU uptillnow 4900 is 0.0005482298422799648
testing: bce: 6.036681, dice: 19.769990, loss: 12.903335
IoU in current test batch is 0.8592339391719667
Epoch 26617: reducing learning rate of group 0 to 7.7559e-04.
training: bce: 0.020793, dice: 0.068321, loss: 0.044557
training IoU in current batch 5000 is 0.9056983023757054
training IoU uptillnow 5000 is 0.0005574836419015842
testing: bce: 6.116724, dice: 20.098462, loss: 13.107593
IoU in current test batch is 0.9037543814332977
Epoch 26718: reducing learning rate of group 0 to 7.7482e-04.
training: bce: 0.020772, dice: 0.068212, loss: 0.044492
training IoU in current batch 5100 is 0.912189372360279
training IoU uptillnow 5100 is 0.0005667491180183728
testing: bce: 6.232744, dice: 20.467503, loss: 13.350123
IoU in current test batch is 0.9021070832794664
Epoch 26819: reducing learning rate of group 0 to 7.7404e-04.
training: bce: 0.020696, dice: 0.068140, loss: 0.044418
training IoU in current batch 5200 is 0.93284974717072
training IoU uptillnow 5200 is 0.0005762017203946456
testing: bce: 6.331745, dice: 20.846721, loss: 13.589233
IoU in current test batch is 0.9094346355231927
Epoch 26920: reducing learning rate of group 0 to 7.7327e-04.
training: bce: 0.020667, dice: 0.068035, loss: 0.044351
training IoU in current batch 5300 is 0.9192953092141968
training IoU uptillnow 5300 is 0.000585416964753853
testing: bce: 6.444319, dice: 21.215010, loss: 13.829665
IoU in current test batch is 0.9123449352913068
Epoch 27021: reducing learning rate of group 0 to 7.7250e-04.
training: bce: 0.020565, dice: 0.067829, loss: 0.044197
training IoU in current batch 5400 is 0.9183778763801466
training IoU uptillnow 5400 is 0.0005945529153928935
testing: bce: 6.533600, dice: 21.549562, loss: 14.041581
IoU in current test batch is 0.8931631764248736
Epoch 27122: reducing learning rate of group 0 to 7.7172e-04.
training: bce: 0.020459, dice: 0.067680, loss: 0.044070
training IoU in current batch 5500 is 0.8584478736262192
training IoU uptillnow 5500 is 0.000602887253616035
testing: bce: 6.620287, dice: 21.900591, loss: 14.260439
IoU in current test batch is 0.9006517628167029
Epoch 27223: reducing learning rate of group 0 to 7.7095e-04.
training: bce: 0.020397, dice: 0.067506, loss: 0.043951
training IoU in current batch 5600 is 0.9087605884975479
training IoU uptillnow 5600 is 0.0006117748532791943
testing: bce: 6.720239, dice: 22.241086, loss: 14.480662
IoU in current test batch is 0.8969823885539523
Epoch 27324: reducing learning rate of group 0 to 7.7018e-04.
training: bce: 0.020293, dice: 0.067397, loss: 0.043845
training IoU in current batch 5700 is 0.9435751226627768
training IoU uptillnow 5700 is 0.0006210211144310072
testing: bce: 6.805284, dice: 22.601945, loss: 14.703614
IoU in current test batch is 0.8921985806576704
Epoch 27425: reducing learning rate of group 0 to 7.6941e-04.
training: bce: 0.020223, dice: 0.067236, loss: 0.043730
training IoU in current batch 5800 is 0.8922953497034469
training IoU uptillnow 5800 is 0.0006295785570416028
testing: bce: 6.900938, dice: 22.943399, loss: 14.922168
IoU in current test batch is 0.8647969636812345
Epoch 27526: reducing learning rate of group 0 to 7.6864e-04.
training: bce: 0.020122, dice: 0.067059, loss: 0.043590
training IoU in current batch 5900 is 0.8616004735462413
training IoU uptillnow 5900 is 0.000637703278133798
testing: bce: 6.984570, dice: 23.277292, loss: 15.130931
IoU in current test batch is 0.8949401261285291
Epoch 27627: reducing learning rate of group 0 to 7.6787e-04.
training: bce: 0.020013, dice: 0.066829, loss: 0.043421
training IoU in current batch 6000 is 0.8906007988913345
training IoU uptillnow 6000 is 0.0006461183180309243
testing: bce: 7.064733, dice: 23.590475, loss: 15.327604
IoU in current test batch is 0.8750098760621009
Epoch 27728: reducing learning rate of group 0 to 7.6710e-04.
training: bce: 0.019996, dice: 0.066687, loss: 0.043342
training IoU in current batch 6100 is 0.9383373790233599
training IoU uptillnow 6100 is 0.0006550451991294145
testing: bce: 7.176097, dice: 23.932933, loss: 15.554515
IoU in current test batch is 0.9117053386235896
Epoch 27829: reducing learning rate of group 0 to 7.6634e-04.
training: bce: 0.019979, dice: 0.066614, loss: 0.043297
training IoU in current batch 6200 is 0.9042286420940538
training IoU uptillnow 6200 is 0.000663500576457434
testing: bce: 7.287814, dice: 24.298581, loss: 15.793198
IoU in current test batch is 0.9067333911598477
Epoch 27930: reducing learning rate of group 0 to 7.6557e-04.
training: bce: 0.019971, dice: 0.066486, loss: 0.043229
training IoU in current batch 6300 is 0.9176553922436692
training IoU uptillnow 6300 is 0.0006720554004967963
testing: bce: 7.402337, dice: 24.642951, loss: 16.022644
IoU in current test batch is 0.9101622749117586
Epoch 28031: reducing learning rate of group 0 to 7.6481e-04.
training: bce: 0.019909, dice: 0.066351, loss: 0.043130
training IoU in current batch 6400 is 0.9473473473473474
training IoU uptillnow 6400 is 0.0006809015538443444
testing: bce: 7.496379, dice: 24.983221, loss: 16.239800
IoU in current test batch is 0.9069869827814662
Epoch 28132: reducing learning rate of group 0 to 7.6404e-04.
training: bce: 0.019836, dice: 0.066249, loss: 0.043042
training IoU in current batch 6500 is 0.9160614324673461
training IoU uptillnow 6500 is 0.0006893151586471109
testing: bce: 7.585430, dice: 25.334438, loss: 16.459934
IoU in current test batch is 0.8997185601774693
Epoch 28233: reducing learning rate of group 0 to 7.6328e-04.
training: bce: 0.019776, dice: 0.066267, loss: 0.043022
training IoU in current batch 6600 is 0.9152388217345382
training IoU uptillnow 6600 is 0.0006976596141729108
testing: bce: 7.678959, dice: 25.731235, loss: 16.705097
IoU in current test batch is 0.9046927340110174
Epoch 28334: reducing learning rate of group 0 to 7.6251e-04.
training: bce: 0.019741, dice: 0.066204, loss: 0.042973
training IoU in current batch 6700 is 0.8714896001120527
training IoU uptillnow 6700 is 0.0007054318174107062
testing: bce: 7.781517, dice: 26.096210, loss: 16.938863
IoU in current test batch is 0.9033630046220663
Epoch 28435: reducing learning rate of group 0 to 7.6175e-04.
training: bce: 0.019687, dice: 0.066095, loss: 0.042891
training IoU in current batch 6800 is 0.9221303891938568
training IoU uptillnow 6800 is 0.0007137417688021757
testing: bce: 7.875992, dice: 26.442020, loss: 17.159006
IoU in current test batch is 0.9206590246352176
Epoch 28536: reducing learning rate of group 0 to 7.6099e-04.
training: bce: 0.019612, dice: 0.065980, loss: 0.042796
training IoU in current batch 6900 is 0.9409774436090226
training IoU uptillnow 6900 is 0.0007222132712843245
testing: bce: 7.961262, dice: 26.784056, loss: 17.372659
IoU in current test batch is 0.9070816565274845
Epoch 28637: reducing learning rate of group 0 to 7.6023e-04.
training: bce: 0.019532, dice: 0.065842, loss: 0.042687
training IoU in current batch 7000 is 0.9396843141952141
training IoU uptillnow 7000 is 0.0007306107199813038
testing: bce: 8.043536, dice: 27.115476, loss: 17.579506
IoU in current test batch is 0.9149259522998326
Epoch 28738: reducing learning rate of group 0 to 7.5947e-04.
training: bce: 0.019472, dice: 0.065754, loss: 0.042613
training IoU in current batch 7100 is 0.8703710342355261
training IoU uptillnow 7100 is 0.0007381476160257614
testing: bce: 8.133695, dice: 27.465952, loss: 17.799824
IoU in current test batch is 0.8788127923822409
Epoch 28839: reducing learning rate of group 0 to 7.5871e-04.
training: bce: 0.019542, dice: 0.065745, loss: 0.042643
training IoU in current batch 7200 is 0.9329311027820947
training IoU uptillnow 7200 is 0.0007463539230381532
testing: bce: 8.277753, dice: 27.848681, loss: 18.063217
IoU in current test batch is 0.8956182257889563
Epoch 28940: reducing learning rate of group 0 to 7.5795e-04.
training: bce: 0.019499, dice: 0.065750, loss: 0.042624
training IoU in current batch 7300 is 0.930504238209085
training IoU uptillnow 7300 is 0.0007544757398346778
testing: bce: 8.374267, dice: 28.237594, loss: 18.305930
IoU in current test batch is 0.8721552316969394
Epoch 29041: reducing learning rate of group 0 to 7.5719e-04.
training: bce: 0.019481, dice: 0.065643, loss: 0.042562
training IoU in current batch 7400 is 0.9141762976133488
training IoU uptillnow 7400 is 0.0007623547040461662
testing: bce: 8.481227, dice: 28.577963, loss: 18.529595
IoU in current test batch is 0.8481185459991344
Epoch 29142: reducing learning rate of group 0 to 7.5643e-04.
training: bce: 0.019611, dice: 0.065617, loss: 0.042614
training IoU in current batch 7500 is 0.9383761576993765
training IoU uptillnow 7500 is 0.0007704559568599281
testing: bce: 8.653152, dice: 28.952623, loss: 18.802887
IoU in current test batch is 0.898602566275776
Epoch 29243: reducing learning rate of group 0 to 7.5568e-04.
training: bce: 0.019543, dice: 0.065503, loss: 0.042523
training IoU in current batch 7600 is 0.9485668001619944
training IoU uptillnow 7600 is 0.0007786178455186769
testing: bce: 8.737985, dice: 29.287608, loss: 19.012796
IoU in current test batch is 0.9033080304561115
Epoch 29344: reducing learning rate of group 0 to 7.5492e-04.
training: bce: 0.019666, dice: 0.065524, loss: 0.042595
training IoU in current batch 7700 is 0.8955727978092195
training IoU uptillnow 7700 is 0.0007861233720963823
testing: bce: 8.908562, dice: 29.682411, loss: 19.295487
IoU in current test batch is 0.9037762917764924
Epoch 29445: reducing learning rate of group 0 to 7.5417e-04.
training: bce: 0.019597, dice: 0.065402, loss: 0.042500
training IoU in current batch 7800 is 0.906136039321389
training IoU uptillnow 7800 is 0.0007936973724092917
testing: bce: 8.992700, dice: 30.011924, loss: 19.502312
IoU in current test batch is 0.8721992789918528
Epoch 29546: reducing learning rate of group 0 to 7.5341e-04.
training: bce: 0.019587, dice: 0.065340, loss: 0.042464
training IoU in current batch 7900 is 0.894852981739946
training IoU uptillnow 7900 is 0.0008010931355851605
testing: bce: 9.103215, dice: 30.367950, loss: 19.735583
IoU in current test batch is 0.8732603176534897
Epoch 29647: reducing learning rate of group 0 to 7.5266e-04.
training: bce: 0.019515, dice: 0.065211, loss: 0.042363
training IoU in current batch 8000 is 0.9154684425609202
training IoU uptillnow 8000 is 0.0008086704700619885
testing: bce: 9.184479, dice: 30.691500, loss: 19.937990
IoU in current test batch is 0.8966898540517214
Epoch 29748: reducing learning rate of group 0 to 7.5191e-04.
training: bce: 0.019454, dice: 0.065110, loss: 0.042282
training IoU in current batch 8100 is 0.9409521245569166
training IoU uptillnow 8100 is 0.0008164820023163321
testing: bce: 9.270246, dice: 31.027007, loss: 20.148627
IoU in current test batch is 0.9223746628642822
Epoch 29849: reducing learning rate of group 0 to 7.5116e-04.
training: bce: 0.019425, dice: 0.065010, loss: 0.042218
training IoU in current batch 8200 is 0.9419403021031584
training IoU uptillnow 8200 is 0.0008242522999908948
testing: bce: 9.370924, dice: 31.361611, loss: 20.366267
IoU in current test batch is 0.8849936536027605
Epoch 29950: reducing learning rate of group 0 to 7.5040e-04.
training: bce: 0.019435, dice: 0.064998, loss: 0.042216
training IoU in current batch 8300 is 0.9460919188831143
training IoU uptillnow 8300 is 0.000832016924756293
testing: bce: 9.489988, dice: 31.737986, loss: 20.613987
IoU in current test batch is 0.8597435236019136
Epoch 30051: reducing learning rate of group 0 to 7.4965e-04.
training: bce: 0.019509, dice: 0.065139, loss: 0.042324
training IoU in current batch 8400 is 0.933711449840482
training IoU uptillnow 8400 is 0.0008395928535759342
testing: bce: 9.640912, dice: 32.190213, loss: 20.915562
IoU in current test batch is 0.8778265590608807
Epoch 30152: reducing learning rate of group 0 to 7.4890e-04.
training: bce: 0.019452, dice: 0.065010, loss: 0.042231
training IoU in current batch 8500 is 0.9483333800566286
training IoU uptillnow 8500 is 0.0008472800006397736
testing: bce: 9.727218, dice: 32.508767, loss: 21.117993
IoU in current test batch is 0.8469125224266312
Epoch 30253: reducing learning rate of group 0 to 7.4816e-04.
training: bce: 0.019577, dice: 0.065009, loss: 0.042293
training IoU in current batch 8600 is 0.9274784870368833
training IoU uptillnow 8600 is 0.0008546869806930733
testing: bce: 9.904907, dice: 32.890936, loss: 21.397921
IoU in current test batch is 0.8848222862511868
Epoch 30354: reducing learning rate of group 0 to 7.4741e-04.
training: bce: 0.019520, dice: 0.064965, loss: 0.042242
training IoU in current batch 8700 is 0.8922739382541921
training IoU uptillnow 8700 is 0.0008616592158251596
testing: bce: 9.991013, dice: 33.250381, loss: 21.620697
IoU in current test batch is 0.9120259584464874
Epoch 30455: reducing learning rate of group 0 to 7.4666e-04.
training: bce: 0.019512, dice: 0.064892, loss: 0.042202
training IoU in current batch 8800 is 0.8762960059856771
training IoU uptillnow 8800 is 0.0008684111091720244
testing: bce: 10.101416, dice: 33.595070, loss: 21.848243
IoU in current test batch is 0.887904750921566
Epoch 30556: reducing learning rate of group 0 to 7.4591e-04.
training: bce: 0.019540, dice: 0.064911, loss: 0.042225
training IoU in current batch 8900 is 0.924791159987311
training IoU uptillnow 8900 is 0.0008756471421484482
testing: bce: 10.230766, dice: 33.986406, loss: 22.108586
IoU in current test batch is 0.8742837535502218
Epoch 30657: reducing learning rate of group 0 to 7.4517e-04.
training: bce: 0.019482, dice: 0.064794, loss: 0.042138
training IoU in current batch 9000 is 0.91833185971117
training IoU uptillnow 9000 is 0.0008827659012914085
testing: bce: 10.315017, dice: 34.306515, loss: 22.310766
IoU in current test batch is 0.8792044588811168
Epoch 30758: reducing learning rate of group 0 to 7.4442e-04.
training: bce: 0.019413, dice: 0.064668, loss: 0.042041
training IoU in current batch 9100 is 0.9369098616188575
training IoU uptillnow 9100 is 0.0008900394953523546
testing: bce: 10.392980, dice: 34.620156, loss: 22.506568
IoU in current test batch is 0.8942051166917728
Epoch 30859: reducing learning rate of group 0 to 7.4368e-04.
training: bce: 0.019370, dice: 0.064581, loss: 0.041976
training IoU in current batch 9200 is 0.9459247860478929
training IoU uptillnow 9200 is 0.0008973632595103071
testing: bce: 10.483854, dice: 34.953504, loss: 22.718679
IoU in current test batch is 0.8959451715345437
Epoch 30960: reducing learning rate of group 0 to 7.4293e-04.
training: bce: 0.019316, dice: 0.064488, loss: 0.041902
training IoU in current batch 9300 is 0.9220251190509343
training IoU uptillnow 9300 is 0.0009043827879102838
testing: bce: 10.568174, dice: 35.282780, loss: 22.925477
IoU in current test batch is 0.8824768295034042
Epoch 31061: reducing learning rate of group 0 to 7.4219e-04.
training: bce: 0.019265, dice: 0.064413, loss: 0.041839
training IoU in current batch 9400 is 0.9053075200842882
training IoU uptillnow 9400 is 0.0009111779935234801
testing: bce: 10.653774, dice: 35.620585, loss: 23.137180
IoU in current test batch is 0.906884496174451
Epoch 31162: reducing learning rate of group 0 to 7.4145e-04.
training: bce: 0.019208, dice: 0.064292, loss: 0.041750
training IoU in current batch 9500 is 0.9370344962759702
training IoU uptillnow 9500 is 0.0009182686035471866
testing: bce: 10.735097, dice: 35.931470, loss: 23.333283
IoU in current test batch is 0.8828125044223812
Epoch 31263: reducing learning rate of group 0 to 7.4071e-04.
training: bce: 0.019157, dice: 0.064199, loss: 0.041678
training IoU in current batch 9600 is 0.9391530568001156
training IoU uptillnow 9600 is 0.0009253364680385172
testing: bce: 10.819143, dice: 36.257579, loss: 23.538361
IoU in current test batch is 0.8882130984344366
Epoch 31364: reducing learning rate of group 0 to 7.3997e-04.
training: bce: 0.019159, dice: 0.064174, loss: 0.041667
training IoU in current batch 9700 is 0.9062552689259822
training IoU uptillnow 9700 is 0.0009320100808677575
testing: bce: 10.933203, dice: 36.620918, loss: 23.777061
IoU in current test batch is 0.9105711159887598
Epoch 31465: reducing learning rate of group 0 to 7.3923e-04.
training: bce: 0.019110, dice: 0.064081, loss: 0.041595
training IoU in current batch 9800 is 0.9522853575537983
training IoU uptillnow 9800 is 0.0009391284124370007
testing: bce: 11.017337, dice: 36.944326, loss: 23.980831
IoU in current test batch is 0.9155574046999889
Epoch 31566: reducing learning rate of group 0 to 7.3849e-04.
training: bce: 0.019067, dice: 0.064014, loss: 0.041541
training IoU in current batch 9900 is 0.9431912667971998
training IoU uptillnow 9900 is 0.0009461057620474025
testing: bce: 11.105042, dice: 37.282583, loss: 24.193812
IoU in current test batch is 0.9160832071902854
Epoch 31667: reducing learning rate of group 0 to 7.3775e-04.
training: bce: 0.019016, dice: 0.063959, loss: 0.041487
training IoU in current batch 10000 is 0.9324887661740024
training IoU uptillnow 10000 is 0.0009529265510858861
testing: bce: 11.186747, dice: 37.626528, loss: 24.406638
IoU in current test batch is 0.8895804040593038
Epoch 31768: reducing learning rate of group 0 to 7.3701e-04.
training: bce: 0.019240, dice: 0.063959, loss: 0.041599
training IoU in current batch 10100 is 0.9455703883495146
training IoU uptillnow 10100 is 0.0009598415660022775
testing: bce: 11.431948, dice: 38.002645, loss: 24.717297
IoU in current test batch is 0.8778944990571725
Epoch 31869: reducing learning rate of group 0 to 7.3627e-04.
training: bce: 0.019193, dice: 0.063917, loss: 0.041555
training IoU in current batch 10200 is 0.9214552655955537
training IoU uptillnow 10200 is 0.000966461239939528
testing: bce: 11.516943, dice: 38.354063, loss: 24.935503
IoU in current test batch is 0.9067279604027999
Epoch 31970: reducing learning rate of group 0 to 7.3554e-04.
training: bce: 0.019142, dice: 0.063830, loss: 0.041486
training IoU in current batch 10300 is 0.9571291822408606
training IoU uptillnow 10300 is 0.0009734111442130593
testing: bce: 11.598991, dice: 38.677408, loss: 25.138200
IoU in current test batch is 0.9044812006359165
Epoch 32071: reducing learning rate of group 0 to 7.3480e-04.
training: bce: 0.019110, dice: 0.063774, loss: 0.041442
training IoU in current batch 10400 is 0.8398191240056772
training IoU uptillnow 10400 is 0.0009790995739196195
testing: bce: 11.692214, dice: 39.018256, loss: 25.355235
IoU in current test batch is 0.9080904583125251
Epoch 32172: reducing learning rate of group 0 to 7.3407e-04.
training: bce: 0.019079, dice: 0.063678, loss: 0.041378
training IoU in current batch 10500 is 0.9241670435700287
training IoU uptillnow 10500 is 0.0009856258386338446
testing: bce: 11.785399, dice: 39.334020, loss: 25.559710
IoU in current test batch is 0.882907732857734
Epoch 32273: reducing learning rate of group 0 to 7.3333e-04.
training: bce: 0.019037, dice: 0.063589, loss: 0.041313
training IoU in current batch 10600 is 0.8863555970029263
training IoU uptillnow 10600 is 0.0009917214820333573
testing: bce: 11.871362, dice: 39.653104, loss: 25.762233
IoU in current test batch is 0.9115009723841129
Epoch 32374: reducing learning rate of group 0 to 7.3260e-04.
training: bce: 0.018998, dice: 0.063452, loss: 0.041225
training IoU in current batch 10700 is 0.9371039100801148
training IoU uptillnow 10700 is 0.0009983015999908687
testing: bce: 11.958637, dice: 39.941027, loss: 25.949832
IoU in current test batch is 0.905193582242764
Epoch 32475: reducing learning rate of group 0 to 7.3187e-04.
training: bce: 0.019009, dice: 0.063466, loss: 0.041238
training IoU in current batch 10800 is 0.905542644296584
training IoU uptillnow 10800 is 0.001004517519624708
testing: bce: 12.077670, dice: 40.323328, loss: 26.200499
IoU in current test batch is 0.8861071711723557
Epoch 32576: reducing learning rate of group 0 to 7.3114e-04.
training: bce: 0.018967, dice: 0.063392, loss: 0.041180
training IoU in current batch 10900 is 0.9468744854837825
training IoU uptillnow 10900 is 0.0010111179207453252
testing: bce: 12.162457, dice: 40.649477, loss: 26.405967
IoU in current test batch is 0.8775733364604119
Epoch 32677: reducing learning rate of group 0 to 7.3040e-04.
training: bce: 0.019253, dice: 0.063517, loss: 0.041385
training IoU in current batch 11000 is 0.7786058746163963
training IoU uptillnow 11000 is 0.001015962676080624
testing: bce: 12.458798, dice: 41.102918, loss: 26.780858
IoU in current test batch is 0.8266268976897523
Epoch 32778: reducing learning rate of group 0 to 7.2967e-04.
training: bce: 0.019219, dice: 0.063478, loss: 0.041348
training IoU in current batch 11100 is 0.8511003536237854
training IoU uptillnow 11100 is 0.0010215146227350914
testing: bce: 12.549759, dice: 41.451377, loss: 27.000568
IoU in current test batch is 0.8900583492156149
Epoch 32879: reducing learning rate of group 0 to 7.2894e-04.
training: bce: 0.019183, dice: 0.063415, loss: 0.041299
training IoU in current batch 11200 is 0.914565055521383
training IoU uptillnow 11200 is 0.0010276758250522228
testing: bce: 12.639345, dice: 41.782797, loss: 27.211071
IoU in current test batch is 0.9032192676187921
Epoch 32980: reducing learning rate of group 0 to 7.2822e-04.
training: bce: 0.019135, dice: 0.063336, loss: 0.041236
training IoU in current batch 11300 is 0.9483225515270046
training IoU uptillnow 11300 is 0.0010341406715573876
testing: bce: 12.720350, dice: 42.103666, loss: 27.412008
IoU in current test batch is 0.9069601674424953
Epoch 33081: reducing learning rate of group 0 to 7.2749e-04.
training: bce: 0.019097, dice: 0.063285, loss: 0.041191
training IoU in current batch 11400 is 0.916139112631664
training IoU uptillnow 11400 is 0.0010402423524351768
testing: bce: 12.807466, dice: 42.441755, loss: 27.624610
IoU in current test batch is 0.9006068365396672
Epoch 33182: reducing learning rate of group 0 to 7.2676e-04.
training: bce: 0.019056, dice: 0.063203, loss: 0.041129
training IoU in current batch 11500 is 0.9052518701968494
training IoU uptillnow 11500 is 0.001046197966536244
testing: bce: 12.891704, dice: 42.758609, loss: 27.825157
IoU in current test batch is 0.896340105125871
Epoch 33283: reducing learning rate of group 0 to 7.2603e-04.
training: bce: 0.019026, dice: 0.063159, loss: 0.041092
training IoU in current batch 11600 is 0.9389242912962525
training IoU uptillnow 11600 is 0.0010524548724555169
testing: bce: 12.983615, dice: 43.100332, loss: 28.041974
IoU in current test batch is 0.8781716492096634
Maximum training samples requirement meet, I have been training for more than  100002  samples.
Making network now
ResNetUNet(
  (base_model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer0_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer1_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2_1x1): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3_1x1): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4_1x1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (upsample): Upsample(scale_factor=2.0, mode=bilinear)
  (conv_up3): Sequential(
    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up2): Sequential(
    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up1): Sequential(
    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up0): Sequential(
    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size2): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 512, 512]           1,792
              ReLU-2         [-1, 64, 512, 512]               0
            Conv2d-3         [-1, 64, 512, 512]          36,928
              ReLU-4         [-1, 64, 512, 512]               0
            Conv2d-5         [-1, 64, 256, 256]           9,408
            Conv2d-6         [-1, 64, 256, 256]           9,408
       BatchNorm2d-7         [-1, 64, 256, 256]             128
       BatchNorm2d-8         [-1, 64, 256, 256]             128
              ReLU-9         [-1, 64, 256, 256]               0
             ReLU-10         [-1, 64, 256, 256]               0
        MaxPool2d-11         [-1, 64, 128, 128]               0
        MaxPool2d-12         [-1, 64, 128, 128]               0
           Conv2d-13         [-1, 64, 128, 128]          36,864
           Conv2d-14         [-1, 64, 128, 128]          36,864
      BatchNorm2d-15         [-1, 64, 128, 128]             128
      BatchNorm2d-16         [-1, 64, 128, 128]             128
             ReLU-17         [-1, 64, 128, 128]               0
             ReLU-18         [-1, 64, 128, 128]               0
           Conv2d-19         [-1, 64, 128, 128]          36,864
           Conv2d-20         [-1, 64, 128, 128]          36,864
      BatchNorm2d-21         [-1, 64, 128, 128]             128
      BatchNorm2d-22         [-1, 64, 128, 128]             128
             ReLU-23         [-1, 64, 128, 128]               0
             ReLU-24         [-1, 64, 128, 128]               0
       BasicBlock-25         [-1, 64, 128, 128]               0
       BasicBlock-26         [-1, 64, 128, 128]               0
           Conv2d-27         [-1, 64, 128, 128]          36,864
           Conv2d-28         [-1, 64, 128, 128]          36,864
      BatchNorm2d-29         [-1, 64, 128, 128]             128
      BatchNorm2d-30         [-1, 64, 128, 128]             128
             ReLU-31         [-1, 64, 128, 128]               0
             ReLU-32         [-1, 64, 128, 128]               0
           Conv2d-33         [-1, 64, 128, 128]          36,864
           Conv2d-34         [-1, 64, 128, 128]          36,864
      BatchNorm2d-35         [-1, 64, 128, 128]             128
      BatchNorm2d-36         [-1, 64, 128, 128]             128
             ReLU-37         [-1, 64, 128, 128]               0
             ReLU-38         [-1, 64, 128, 128]               0
       BasicBlock-39         [-1, 64, 128, 128]               0
       BasicBlock-40         [-1, 64, 128, 128]               0
           Conv2d-41          [-1, 128, 64, 64]          73,728
           Conv2d-42          [-1, 128, 64, 64]          73,728
      BatchNorm2d-43          [-1, 128, 64, 64]             256
      BatchNorm2d-44          [-1, 128, 64, 64]             256
             ReLU-45          [-1, 128, 64, 64]               0
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]         147,456
           Conv2d-48          [-1, 128, 64, 64]         147,456
      BatchNorm2d-49          [-1, 128, 64, 64]             256
      BatchNorm2d-50          [-1, 128, 64, 64]             256
           Conv2d-51          [-1, 128, 64, 64]           8,192
           Conv2d-52          [-1, 128, 64, 64]           8,192
      BatchNorm2d-53          [-1, 128, 64, 64]             256
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
             ReLU-56          [-1, 128, 64, 64]               0
       BasicBlock-57          [-1, 128, 64, 64]               0
       BasicBlock-58          [-1, 128, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]         147,456
           Conv2d-60          [-1, 128, 64, 64]         147,456
      BatchNorm2d-61          [-1, 128, 64, 64]             256
      BatchNorm2d-62          [-1, 128, 64, 64]             256
             ReLU-63          [-1, 128, 64, 64]               0
             ReLU-64          [-1, 128, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]         147,456
           Conv2d-66          [-1, 128, 64, 64]         147,456
      BatchNorm2d-67          [-1, 128, 64, 64]             256
      BatchNorm2d-68          [-1, 128, 64, 64]             256
             ReLU-69          [-1, 128, 64, 64]               0
             ReLU-70          [-1, 128, 64, 64]               0
       BasicBlock-71          [-1, 128, 64, 64]               0
       BasicBlock-72          [-1, 128, 64, 64]               0
           Conv2d-73          [-1, 256, 32, 32]         294,912
           Conv2d-74          [-1, 256, 32, 32]         294,912
      BatchNorm2d-75          [-1, 256, 32, 32]             512
      BatchNorm2d-76          [-1, 256, 32, 32]             512
             ReLU-77          [-1, 256, 32, 32]               0
             ReLU-78          [-1, 256, 32, 32]               0
           Conv2d-79          [-1, 256, 32, 32]         589,824
           Conv2d-80          [-1, 256, 32, 32]         589,824
      BatchNorm2d-81          [-1, 256, 32, 32]             512
      BatchNorm2d-82          [-1, 256, 32, 32]             512
           Conv2d-83          [-1, 256, 32, 32]          32,768
           Conv2d-84          [-1, 256, 32, 32]          32,768
      BatchNorm2d-85          [-1, 256, 32, 32]             512
      BatchNorm2d-86          [-1, 256, 32, 32]             512
             ReLU-87          [-1, 256, 32, 32]               0
             ReLU-88          [-1, 256, 32, 32]               0
       BasicBlock-89          [-1, 256, 32, 32]               0
       BasicBlock-90          [-1, 256, 32, 32]               0
           Conv2d-91          [-1, 256, 32, 32]         589,824
           Conv2d-92          [-1, 256, 32, 32]         589,824
      BatchNorm2d-93          [-1, 256, 32, 32]             512
      BatchNorm2d-94          [-1, 256, 32, 32]             512
             ReLU-95          [-1, 256, 32, 32]               0
             ReLU-96          [-1, 256, 32, 32]               0
           Conv2d-97          [-1, 256, 32, 32]         589,824
           Conv2d-98          [-1, 256, 32, 32]         589,824
      BatchNorm2d-99          [-1, 256, 32, 32]             512
     BatchNorm2d-100          [-1, 256, 32, 32]             512
            ReLU-101          [-1, 256, 32, 32]               0
            ReLU-102          [-1, 256, 32, 32]               0
      BasicBlock-103          [-1, 256, 32, 32]               0
      BasicBlock-104          [-1, 256, 32, 32]               0
          Conv2d-105          [-1, 512, 16, 16]       1,179,648
          Conv2d-106          [-1, 512, 16, 16]       1,179,648
     BatchNorm2d-107          [-1, 512, 16, 16]           1,024
     BatchNorm2d-108          [-1, 512, 16, 16]           1,024
            ReLU-109          [-1, 512, 16, 16]               0
            ReLU-110          [-1, 512, 16, 16]               0
          Conv2d-111          [-1, 512, 16, 16]       2,359,296
          Conv2d-112          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-113          [-1, 512, 16, 16]           1,024
     BatchNorm2d-114          [-1, 512, 16, 16]           1,024
          Conv2d-115          [-1, 512, 16, 16]         131,072
          Conv2d-116          [-1, 512, 16, 16]         131,072
     BatchNorm2d-117          [-1, 512, 16, 16]           1,024
     BatchNorm2d-118          [-1, 512, 16, 16]           1,024
            ReLU-119          [-1, 512, 16, 16]               0
            ReLU-120          [-1, 512, 16, 16]               0
      BasicBlock-121          [-1, 512, 16, 16]               0
      BasicBlock-122          [-1, 512, 16, 16]               0
          Conv2d-123          [-1, 512, 16, 16]       2,359,296
          Conv2d-124          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-125          [-1, 512, 16, 16]           1,024
     BatchNorm2d-126          [-1, 512, 16, 16]           1,024
            ReLU-127          [-1, 512, 16, 16]               0
            ReLU-128          [-1, 512, 16, 16]               0
          Conv2d-129          [-1, 512, 16, 16]       2,359,296
          Conv2d-130          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-131          [-1, 512, 16, 16]           1,024
     BatchNorm2d-132          [-1, 512, 16, 16]           1,024
            ReLU-133          [-1, 512, 16, 16]               0
            ReLU-134          [-1, 512, 16, 16]               0
      BasicBlock-135          [-1, 512, 16, 16]               0
      BasicBlock-136          [-1, 512, 16, 16]               0
          Conv2d-137          [-1, 512, 16, 16]         262,656
            ReLU-138          [-1, 512, 16, 16]               0
        Upsample-139          [-1, 512, 32, 32]               0
          Conv2d-140          [-1, 256, 32, 32]          65,792
            ReLU-141          [-1, 256, 32, 32]               0
          Conv2d-142          [-1, 512, 32, 32]       3,539,456
            ReLU-143          [-1, 512, 32, 32]               0
        Upsample-144          [-1, 512, 64, 64]               0
          Conv2d-145          [-1, 128, 64, 64]          16,512
            ReLU-146          [-1, 128, 64, 64]               0
          Conv2d-147          [-1, 256, 64, 64]       1,474,816
            ReLU-148          [-1, 256, 64, 64]               0
        Upsample-149        [-1, 256, 128, 128]               0
          Conv2d-150         [-1, 64, 128, 128]           4,160
            ReLU-151         [-1, 64, 128, 128]               0
          Conv2d-152        [-1, 256, 128, 128]         737,536
            ReLU-153        [-1, 256, 128, 128]               0
        Upsample-154        [-1, 256, 256, 256]               0
          Conv2d-155         [-1, 64, 256, 256]           4,160
            ReLU-156         [-1, 64, 256, 256]               0
          Conv2d-157        [-1, 128, 256, 256]         368,768
            ReLU-158        [-1, 128, 256, 256]               0
        Upsample-159        [-1, 128, 512, 512]               0
          Conv2d-160         [-1, 64, 512, 512]         110,656
            ReLU-161         [-1, 64, 512, 512]               0
          Conv2d-162          [-1, 1, 512, 512]              65
================================================================
Total params: 28,976,321
Trainable params: 28,976,321
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 2172.00
Params size (MB): 110.54
Estimated Total Size (MB): 2285.54
----------------------------------------------------------------
Start training now...
training: bce: 0.594424, dice: 0.942227, loss: 0.594424
training IoU in current batch 0 is 0.004363628077611056
training IoU uptillnow 0 is 0.0014545426925370187
testing: bce: 0.034966, dice: 0.055425, loss: 0.034966
IoU in current test batch is 0.0
training: bce: 0.392467, dice: 0.871581, loss: 0.392467
training IoU in current batch 100 is 0.0
training IoU uptillnow 100 is 1.4401412797396224e-05
testing: bce: 2.331718, dice: 5.178217, loss: 2.331718
IoU in current test batch is 0.0
Epoch   158: reducing learning rate of group 0 to 9.9900e-04.
training: bce: 0.314334, dice: 0.872617, loss: 0.314334
training IoU in current batch 200 is 0.0
training IoU uptillnow 200 is 7.236530808641884e-06
testing: bce: 3.716532, dice: 10.317410, loss: 3.716532
IoU in current test batch is 0.0
training: bce: 0.242038, dice: 0.773186, loss: 0.242038
training IoU in current batch 300 is 0.15533642691415314
training IoU uptillnow 300 is 0.0001768549889166824
testing: bce: 4.285493, dice: 13.689941, loss: 4.285493
IoU in current test batch is 0.36409186831843826
training: bce: 0.192076, dice: 0.650861, loss: 0.192076
training IoU in current batch 400 is 0.6659014964405056
training IoU uptillnow 400 is 0.0006862855789129425
testing: bce: 4.530726, dice: 15.352654, loss: 4.530726
IoU in current test batch is 0.7653073810173354
training: bce: 0.159536, dice: 0.561813, loss: 0.159536
training IoU in current batch 500 is 0.7556870543726051
training IoU uptillnow 500 is 0.0010520882274150864
testing: bce: 4.701616, dice: 16.556947, loss: 4.701616
IoU in current test batch is 0.7760947221071801
training: bce: 0.220257, dice: 0.560785, loss: 0.220257
training IoU in current batch 600 is 0.0
training IoU uptillnow 600 is 0.0008770319499749723
testing: bce: 7.786732, dice: 19.825410, loss: 7.786732
IoU in current test batch is 0.0
Epoch   611: reducing learning rate of group 0 to 9.9800e-04.
training: bce: 0.216300, dice: 0.594521, loss: 0.216300
training IoU in current batch 700 is 0.0
training IoU uptillnow 700 is 0.0007519204021896695
testing: bce: 8.919184, dice: 24.515268, loss: 8.919184
IoU in current test batch is 0.0
Epoch   712: reducing learning rate of group 0 to 9.9700e-04.
training: bce: 0.221608, dice: 0.610997, loss: 0.221608
training IoU in current batch 800 is 0.0
training IoU uptillnow 800 is 0.0006580476928026945
testing: bce: 10.441670, dice: 28.788720, loss: 10.441670
IoU in current test batch is 0.0
Epoch   813: reducing learning rate of group 0 to 9.9601e-04.
training: bce: 0.215110, dice: 0.625860, loss: 0.215110
training IoU in current batch 900 is 0.0
training IoU uptillnow 900 is 0.000585012432780198
testing: bce: 11.400839, dice: 33.170577, loss: 11.400839
IoU in current test batch is 0.0
Epoch   914: reducing learning rate of group 0 to 9.9501e-04.
training: bce: 0.206394, dice: 0.630975, loss: 0.206394
training IoU in current batch 1000 is 0.0
training IoU uptillnow 1000 is 0.0005265696323026557
testing: bce: 12.152947, dice: 37.153279, loss: 12.152947
IoU in current test batch is 0.0
Epoch  1015: reducing learning rate of group 0 to 9.9401e-04.
training: bce: 0.196521, dice: 0.629412, loss: 0.196521
training IoU in current batch 1100 is 0.0
training IoU uptillnow 1100 is 0.00047874314435509386
testing: bce: 12.727635, dice: 40.763708, loss: 12.727635
IoU in current test batch is 0.0
Epoch  1116: reducing learning rate of group 0 to 9.9302e-04.
training: bce: 0.191683, dice: 0.632254, loss: 0.191683
training IoU in current batch 1200 is 0.0
training IoU uptillnow 1200 is 0.00043888110069521927
testing: bce: 13.541835, dice: 44.666876, loss: 13.541835
IoU in current test batch is 0.0
Epoch  1217: reducing learning rate of group 0 to 9.9203e-04.
training: bce: 0.236936, dice: 0.648358, loss: 0.236936
training IoU in current batch 1300 is 0.0
training IoU uptillnow 1300 is 0.0004051469653612285
testing: bce: 18.132551, dice: 49.618437, loss: 18.132551
IoU in current test batch is 0.0
Epoch  1318: reducing learning rate of group 0 to 9.9104e-04.
training: bce: 0.233056, dice: 0.660462, loss: 0.233056
training IoU in current batch 1400 is 0.0
training IoU uptillnow 1400 is 0.00037622855241610157
testing: bce: 19.206585, dice: 54.429821, loss: 19.206585
IoU in current test batch is 0.0
Epoch  1419: reducing learning rate of group 0 to 9.9004e-04.
training: bce: 0.229609, dice: 0.670448, loss: 0.229609
training IoU in current batch 1500 is 0.0
training IoU uptillnow 1500 is 0.00035116335905060515
testing: bce: 20.273132, dice: 59.196652, loss: 20.273132
IoU in current test batch is 0.0
Epoch  1520: reducing learning rate of group 0 to 9.8905e-04.
training: bce: 0.225735, dice: 0.678253, loss: 0.225735
training IoU in current batch 1600 is 0.0
training IoU uptillnow 1600 is 0.000329229357860686
testing: bce: 21.258938, dice: 63.875494, loss: 21.258938
IoU in current test batch is 0.0
Epoch  1621: reducing learning rate of group 0 to 9.8807e-04.
training: bce: 0.219301, dice: 0.682318, loss: 0.219301
training IoU in current batch 1700 is 0.0
training IoU uptillnow 1700 is 0.00030987431036740644
testing: bce: 21.943045, dice: 68.271973, loss: 21.943045
IoU in current test batch is 0.0
Epoch  1722: reducing learning rate of group 0 to 9.8708e-04.
training: bce: 0.214932, dice: 0.686956, loss: 0.214932
training IoU in current batch 1800 is 0.0
training IoU uptillnow 1800 is 0.0002926686296140801
testing: bce: 22.770165, dice: 72.776894, loss: 22.770165
IoU in current test batch is 0.0
Epoch  1823: reducing learning rate of group 0 to 9.8609e-04.
training: bce: 0.207875, dice: 0.685824, loss: 0.207875
training IoU in current batch 1900 is 0.0
training IoU uptillnow 1900 is 0.0002772731204286998
testing: bce: 23.245353, dice: 76.691279, loss: 23.245353
IoU in current test batch is 0.0
Epoch  1924: reducing learning rate of group 0 to 9.8510e-04.
training: bce: 0.201269, dice: 0.683544, loss: 0.201269
training IoU in current batch 2000 is 0.0
training IoU uptillnow 2000 is 0.00026341639277109363
