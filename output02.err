Making network now
ResNetUNet(
  (base_model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (layer0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer0_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer1_1x1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2_1x1): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3_1x1): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4_1x1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (upsample): Upsample(scale_factor=2.0, mode=bilinear)
  (conv_up3): Sequential(
    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up2): Sequential(
    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up1): Sequential(
    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_up0): Sequential(
    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_original_size2): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 512, 512]           1,792
              ReLU-2         [-1, 64, 512, 512]               0
            Conv2d-3         [-1, 64, 512, 512]          36,928
              ReLU-4         [-1, 64, 512, 512]               0
            Conv2d-5         [-1, 64, 256, 256]           9,408
            Conv2d-6         [-1, 64, 256, 256]           9,408
       BatchNorm2d-7         [-1, 64, 256, 256]             128
       BatchNorm2d-8         [-1, 64, 256, 256]             128
              ReLU-9         [-1, 64, 256, 256]               0
             ReLU-10         [-1, 64, 256, 256]               0
        MaxPool2d-11         [-1, 64, 128, 128]               0
        MaxPool2d-12         [-1, 64, 128, 128]               0
           Conv2d-13         [-1, 64, 128, 128]          36,864
           Conv2d-14         [-1, 64, 128, 128]          36,864
      BatchNorm2d-15         [-1, 64, 128, 128]             128
      BatchNorm2d-16         [-1, 64, 128, 128]             128
             ReLU-17         [-1, 64, 128, 128]               0
             ReLU-18         [-1, 64, 128, 128]               0
           Conv2d-19         [-1, 64, 128, 128]          36,864
           Conv2d-20         [-1, 64, 128, 128]          36,864
      BatchNorm2d-21         [-1, 64, 128, 128]             128
      BatchNorm2d-22         [-1, 64, 128, 128]             128
             ReLU-23         [-1, 64, 128, 128]               0
             ReLU-24         [-1, 64, 128, 128]               0
       BasicBlock-25         [-1, 64, 128, 128]               0
       BasicBlock-26         [-1, 64, 128, 128]               0
           Conv2d-27         [-1, 64, 128, 128]          36,864
           Conv2d-28         [-1, 64, 128, 128]          36,864
      BatchNorm2d-29         [-1, 64, 128, 128]             128
      BatchNorm2d-30         [-1, 64, 128, 128]             128
             ReLU-31         [-1, 64, 128, 128]               0
             ReLU-32         [-1, 64, 128, 128]               0
           Conv2d-33         [-1, 64, 128, 128]          36,864
           Conv2d-34         [-1, 64, 128, 128]          36,864
      BatchNorm2d-35         [-1, 64, 128, 128]             128
      BatchNorm2d-36         [-1, 64, 128, 128]             128
             ReLU-37         [-1, 64, 128, 128]               0
             ReLU-38         [-1, 64, 128, 128]               0
       BasicBlock-39         [-1, 64, 128, 128]               0
       BasicBlock-40         [-1, 64, 128, 128]               0
           Conv2d-41          [-1, 128, 64, 64]          73,728
           Conv2d-42          [-1, 128, 64, 64]          73,728
      BatchNorm2d-43          [-1, 128, 64, 64]             256
      BatchNorm2d-44          [-1, 128, 64, 64]             256
             ReLU-45          [-1, 128, 64, 64]               0
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]         147,456
           Conv2d-48          [-1, 128, 64, 64]         147,456
      BatchNorm2d-49          [-1, 128, 64, 64]             256
      BatchNorm2d-50          [-1, 128, 64, 64]             256
           Conv2d-51          [-1, 128, 64, 64]           8,192
           Conv2d-52          [-1, 128, 64, 64]           8,192
      BatchNorm2d-53          [-1, 128, 64, 64]             256
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
             ReLU-56          [-1, 128, 64, 64]               0
       BasicBlock-57          [-1, 128, 64, 64]               0
       BasicBlock-58          [-1, 128, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]         147,456
           Conv2d-60          [-1, 128, 64, 64]         147,456
      BatchNorm2d-61          [-1, 128, 64, 64]             256
      BatchNorm2d-62          [-1, 128, 64, 64]             256
             ReLU-63          [-1, 128, 64, 64]               0
             ReLU-64          [-1, 128, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]         147,456
           Conv2d-66          [-1, 128, 64, 64]         147,456
      BatchNorm2d-67          [-1, 128, 64, 64]             256
      BatchNorm2d-68          [-1, 128, 64, 64]             256
             ReLU-69          [-1, 128, 64, 64]               0
             ReLU-70          [-1, 128, 64, 64]               0
       BasicBlock-71          [-1, 128, 64, 64]               0
       BasicBlock-72          [-1, 128, 64, 64]               0
           Conv2d-73          [-1, 256, 32, 32]         294,912
           Conv2d-74          [-1, 256, 32, 32]         294,912
      BatchNorm2d-75          [-1, 256, 32, 32]             512
      BatchNorm2d-76          [-1, 256, 32, 32]             512
             ReLU-77          [-1, 256, 32, 32]               0
             ReLU-78          [-1, 256, 32, 32]               0
           Conv2d-79          [-1, 256, 32, 32]         589,824
           Conv2d-80          [-1, 256, 32, 32]         589,824
      BatchNorm2d-81          [-1, 256, 32, 32]             512
      BatchNorm2d-82          [-1, 256, 32, 32]             512
           Conv2d-83          [-1, 256, 32, 32]          32,768
           Conv2d-84          [-1, 256, 32, 32]          32,768
      BatchNorm2d-85          [-1, 256, 32, 32]             512
      BatchNorm2d-86          [-1, 256, 32, 32]             512
             ReLU-87          [-1, 256, 32, 32]               0
             ReLU-88          [-1, 256, 32, 32]               0
       BasicBlock-89          [-1, 256, 32, 32]               0
       BasicBlock-90          [-1, 256, 32, 32]               0
           Conv2d-91          [-1, 256, 32, 32]         589,824
           Conv2d-92          [-1, 256, 32, 32]         589,824
      BatchNorm2d-93          [-1, 256, 32, 32]             512
      BatchNorm2d-94          [-1, 256, 32, 32]             512
             ReLU-95          [-1, 256, 32, 32]               0
             ReLU-96          [-1, 256, 32, 32]               0
           Conv2d-97          [-1, 256, 32, 32]         589,824
           Conv2d-98          [-1, 256, 32, 32]         589,824
      BatchNorm2d-99          [-1, 256, 32, 32]             512
     BatchNorm2d-100          [-1, 256, 32, 32]             512
            ReLU-101          [-1, 256, 32, 32]               0
            ReLU-102          [-1, 256, 32, 32]               0
      BasicBlock-103          [-1, 256, 32, 32]               0
      BasicBlock-104          [-1, 256, 32, 32]               0
          Conv2d-105          [-1, 512, 16, 16]       1,179,648
          Conv2d-106          [-1, 512, 16, 16]       1,179,648
     BatchNorm2d-107          [-1, 512, 16, 16]           1,024
     BatchNorm2d-108          [-1, 512, 16, 16]           1,024
            ReLU-109          [-1, 512, 16, 16]               0
            ReLU-110          [-1, 512, 16, 16]               0
          Conv2d-111          [-1, 512, 16, 16]       2,359,296
          Conv2d-112          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-113          [-1, 512, 16, 16]           1,024
     BatchNorm2d-114          [-1, 512, 16, 16]           1,024
          Conv2d-115          [-1, 512, 16, 16]         131,072
          Conv2d-116          [-1, 512, 16, 16]         131,072
     BatchNorm2d-117          [-1, 512, 16, 16]           1,024
     BatchNorm2d-118          [-1, 512, 16, 16]           1,024
            ReLU-119          [-1, 512, 16, 16]               0
            ReLU-120          [-1, 512, 16, 16]               0
      BasicBlock-121          [-1, 512, 16, 16]               0
      BasicBlock-122          [-1, 512, 16, 16]               0
          Conv2d-123          [-1, 512, 16, 16]       2,359,296
          Conv2d-124          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-125          [-1, 512, 16, 16]           1,024
     BatchNorm2d-126          [-1, 512, 16, 16]           1,024
            ReLU-127          [-1, 512, 16, 16]               0
            ReLU-128          [-1, 512, 16, 16]               0
          Conv2d-129          [-1, 512, 16, 16]       2,359,296
          Conv2d-130          [-1, 512, 16, 16]       2,359,296
     BatchNorm2d-131          [-1, 512, 16, 16]           1,024
     BatchNorm2d-132          [-1, 512, 16, 16]           1,024
            ReLU-133          [-1, 512, 16, 16]               0
            ReLU-134          [-1, 512, 16, 16]               0
      BasicBlock-135          [-1, 512, 16, 16]               0
      BasicBlock-136          [-1, 512, 16, 16]               0
          Conv2d-137          [-1, 512, 16, 16]         262,656
            ReLU-138          [-1, 512, 16, 16]               0
        Upsample-139          [-1, 512, 32, 32]               0
          Conv2d-140          [-1, 256, 32, 32]          65,792
            ReLU-141          [-1, 256, 32, 32]               0
          Conv2d-142          [-1, 512, 32, 32]       3,539,456
            ReLU-143          [-1, 512, 32, 32]               0
        Upsample-144          [-1, 512, 64, 64]               0
          Conv2d-145          [-1, 128, 64, 64]          16,512
            ReLU-146          [-1, 128, 64, 64]               0
          Conv2d-147          [-1, 256, 64, 64]       1,474,816
            ReLU-148          [-1, 256, 64, 64]               0
        Upsample-149        [-1, 256, 128, 128]               0
          Conv2d-150         [-1, 64, 128, 128]           4,160
            ReLU-151         [-1, 64, 128, 128]               0
          Conv2d-152        [-1, 256, 128, 128]         737,536
            ReLU-153        [-1, 256, 128, 128]               0
        Upsample-154        [-1, 256, 256, 256]               0
          Conv2d-155         [-1, 64, 256, 256]           4,160
            ReLU-156         [-1, 64, 256, 256]               0
          Conv2d-157        [-1, 128, 256, 256]         368,768
            ReLU-158        [-1, 128, 256, 256]               0
        Upsample-159        [-1, 128, 512, 512]               0
          Conv2d-160         [-1, 64, 512, 512]         110,656
            ReLU-161         [-1, 64, 512, 512]               0
          Conv2d-162          [-1, 1, 512, 512]              65
================================================================
Total params: 28,976,321
Trainable params: 28,976,321
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 2172.00
Params size (MB): 110.54
Estimated Total Size (MB): 2285.54
----------------------------------------------------------------
Start training now...
training: bce: 0.813008, dice: 0.893776, loss: 0.853392
training IoU in current batch 0 is 0.05762017793400859
training IoU uptillnow 0 is 0.019206725978002864
testing: bce: 0.023912, dice: 0.026288, loss: 0.025100
IoU in current test batch is 0.0
Epoch   376: reducing learning rate of group 0 to 9.9900e-04.
Epoch   556: reducing learning rate of group 0 to 9.9800e-04.
Epoch   657: reducing learning rate of group 0 to 9.9700e-04.
Epoch   758: reducing learning rate of group 0 to 9.9601e-04.
Epoch   960: reducing learning rate of group 0 to 9.9501e-04.
training: bce: 0.074773, dice: 0.205622, loss: 0.140198
training IoU in current batch 1000 is 0.8805881231531789
training IoU uptillnow 1000 is 0.00031242367668571017
testing: bce: 2.201406, dice: 6.053765, loss: 4.127585
IoU in current test batch is 0.8615007869458343
Epoch  1061: reducing learning rate of group 0 to 9.9401e-04.
Epoch  1272: reducing learning rate of group 0 to 9.9302e-04.
Epoch  1373: reducing learning rate of group 0 to 9.9203e-04.
Epoch  1474: reducing learning rate of group 0 to 9.9104e-04.
Epoch  1597: reducing learning rate of group 0 to 9.9004e-04.
Epoch  1736: reducing learning rate of group 0 to 9.8905e-04.
Epoch  1837: reducing learning rate of group 0 to 9.8807e-04.
Epoch  1938: reducing learning rate of group 0 to 9.8708e-04.
training: bce: 0.050685, dice: 0.144213, loss: 0.097449
training IoU in current batch 2000 is 0.8946153602934201
training IoU uptillnow 2000 is 0.00030531795125447405
testing: bce: 2.982954, dice: 8.487375, loss: 5.735165
IoU in current test batch is 0.8893505479852446
Epoch  2039: reducing learning rate of group 0 to 9.8609e-04.
Epoch  2140: reducing learning rate of group 0 to 9.8510e-04.
Epoch  2241: reducing learning rate of group 0 to 9.8412e-04.
Epoch  2342: reducing learning rate of group 0 to 9.8314e-04.
Epoch  2443: reducing learning rate of group 0 to 9.8215e-04.
Epoch  2544: reducing learning rate of group 0 to 9.8117e-04.
Epoch  2645: reducing learning rate of group 0 to 9.8019e-04.
Epoch  2746: reducing learning rate of group 0 to 9.7921e-04.
Epoch  2847: reducing learning rate of group 0 to 9.7823e-04.
Epoch  2948: reducing learning rate of group 0 to 9.7725e-04.
training: bce: 0.043826, dice: 0.123316, loss: 0.083571
training IoU in current batch 3000 is 0.7747231989770713
training IoU uptillnow 3000 is 0.0002896308853001976
testing: bce: 3.868297, dice: 10.884407, loss: 7.376352
IoU in current test batch is 0.8677923023519833
Epoch  3049: reducing learning rate of group 0 to 9.7627e-04.
Epoch  3150: reducing learning rate of group 0 to 9.7530e-04.
Epoch  3251: reducing learning rate of group 0 to 9.7432e-04.
Epoch  3352: reducing learning rate of group 0 to 9.7335e-04.
Epoch  3453: reducing learning rate of group 0 to 9.7237e-04.
Epoch  3554: reducing learning rate of group 0 to 9.7140e-04.
Epoch  3655: reducing learning rate of group 0 to 9.7043e-04.
Epoch  3756: reducing learning rate of group 0 to 9.6946e-04.
Epoch  3857: reducing learning rate of group 0 to 9.6849e-04.
Epoch  3996: reducing learning rate of group 0 to 9.6752e-04.
training: bce: 0.038446, dice: 0.111173, loss: 0.074810
training IoU in current batch 4000 is 0.9405260011954573
training IoU uptillnow 4000 is 0.00029559883875307307
testing: bce: 4.524232, dice: 13.082469, loss: 8.803351
IoU in current test batch is 0.8638991990330233
Epoch  4097: reducing learning rate of group 0 to 9.6656e-04.
Epoch  4198: reducing learning rate of group 0 to 9.6559e-04.
Epoch  4299: reducing learning rate of group 0 to 9.6462e-04.
Epoch  4400: reducing learning rate of group 0 to 9.6366e-04.
Epoch  4501: reducing learning rate of group 0 to 9.6269e-04.
Epoch  4602: reducing learning rate of group 0 to 9.6173e-04.
Epoch  4703: reducing learning rate of group 0 to 9.6077e-04.
Epoch  4804: reducing learning rate of group 0 to 9.5981e-04.
Epoch  4905: reducing learning rate of group 0 to 9.5885e-04.
training: bce: 0.035451, dice: 0.103003, loss: 0.069227
training IoU in current batch 5000 is 0.9133223728539471
training IoU uptillnow 5000 is 0.00029736687558535517
testing: bce: 5.214373, dice: 15.150576, loss: 10.182474
IoU in current test batch is 0.8993249323179691
Epoch  5006: reducing learning rate of group 0 to 9.5789e-04.
Epoch  5107: reducing learning rate of group 0 to 9.5693e-04.
Epoch  5208: reducing learning rate of group 0 to 9.5598e-04.
Epoch  5309: reducing learning rate of group 0 to 9.5502e-04.
Epoch  5410: reducing learning rate of group 0 to 9.5406e-04.
Epoch  5511: reducing learning rate of group 0 to 9.5311e-04.
Epoch  5612: reducing learning rate of group 0 to 9.5216e-04.
Epoch  5713: reducing learning rate of group 0 to 9.5121e-04.
Epoch  5814: reducing learning rate of group 0 to 9.5025e-04.
Epoch  5915: reducing learning rate of group 0 to 9.4930e-04.
training: bce: 0.032603, dice: 0.097009, loss: 0.064806
training IoU in current batch 6000 is 0.8416714042069358
training IoU uptillnow 6000 is 0.00029456571896983944
testing: bce: 5.754452, dice: 17.122032, loss: 11.438242
IoU in current test batch is 0.8882871022565745
Epoch  6016: reducing learning rate of group 0 to 9.4835e-04.
Epoch  6117: reducing learning rate of group 0 to 9.4741e-04.
Epoch  6218: reducing learning rate of group 0 to 9.4646e-04.
Epoch  6362: reducing learning rate of group 0 to 9.4551e-04.
Epoch  6463: reducing learning rate of group 0 to 9.4457e-04.
Epoch  6564: reducing learning rate of group 0 to 9.4362e-04.
Epoch  6665: reducing learning rate of group 0 to 9.4268e-04.
Epoch  6766: reducing learning rate of group 0 to 9.4174e-04.
Epoch  6867: reducing learning rate of group 0 to 9.4079e-04.
Epoch  6968: reducing learning rate of group 0 to 9.3985e-04.
training: bce: 0.031132, dice: 0.092923, loss: 0.062027
training IoU in current batch 7000 is 0.916628268632646
training IoU uptillnow 7000 is 0.00029613364315796153
